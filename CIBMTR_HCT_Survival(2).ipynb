{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YueY1pvqB29B"
      },
      "source": [
        "# Class req\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxLp7qZkCbCL"
      },
      "source": [
        "\n",
        "## Mid-term submission\n",
        "1. Documentation\n",
        "2. Cover letter\n",
        "3. Abstract\n",
        "4. Problem statement\n",
        "5. Tools and Technologies to be used\n",
        "6. Dataset\n",
        "7. Team Name and responsibilities of each team member.\n",
        "8. Presentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWP4IMsXCdq8"
      },
      "source": [
        "## Final submission\n",
        "1. Documentation\n",
        "2. Briefly describe each topic mentioned in the documentation of mid-term\n",
        "submission.\n",
        "3. Methods (how you trained data, why you used particular tools and\n",
        "technologies rather than other tools and technologies, etc.)\n",
        "4. Result (Screenshots)\n",
        "5. Conclusion\n",
        "6. Presentation with Demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUZIgDz_83xY"
      },
      "source": [
        "# CIBMTR - Equity in post - HCT Survival Predictions\n",
        "This notebook is intended as our main source for working on the kaggle competition [CIBMTR - Equity in post - HCT Survival Predictions](https://www.kaggle.com/competitions/equity-post-HCT-survival-predictions/team )\n",
        "\n",
        "This is for UofMDearbornTeam1's submission."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEKwK4Xy9ips"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzMr7fo4Ll_P"
      },
      "source": [
        "This is where we can all collectively work on different portions of the code to be broken off into separate files, notebooks or python scripts if needed. Otherwise we will contain it in a single python notebook hosted here on google colab.\n",
        "\n",
        "\n",
        "Make your code as self describing as possible. Where needed, add a text description like the one here or in the code itself using comments. Using '#', '##', '###', etc to notate different sections is much appreciated.\n",
        "\n",
        "If you've never used collab/notebooks/jupyter before or you just forgot how to add text or code blocks just float your mouse to the top or bottom center of an existing block of text or code. You'll then be able to add a new text box or a code block with just a click. You can also do it using the insert menu in the toolbar or just below it, by default it is a hotkey in the toolbar.\n",
        "\n",
        "A great place to start finding what we need to do is to look here:\n",
        "https://www.kaggle.com/competitions/equity-post-HCT-survival-predictions/discussion/549968\n",
        "\n",
        "You can see other people's submission here:\n",
        "https://www.kaggle.com/competitions/equity-post-HCT-survival-predictions/leaderboard\n",
        "\n",
        "A guide on getting started with kaggle API's can be found here:\n",
        "\n",
        "https://www.kaggle.com/competitions/rsna-2024-lumbar-spine-degenerative-classification/code\n",
        "\n",
        "The github page for kaggle api is here:\n",
        "\n",
        "https://github.com/Kaggle/kagglehub?tab=readme-ov-file#authenticate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvD2rGBJ-aYt"
      },
      "source": [
        "## Previous Works\n",
        "This is where you can list out previous work similar to our own that can be applied towards this project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "by0bA1vkLqC_"
      },
      "source": [
        "### Example 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZt9qxxO-jwz"
      },
      "source": [
        "## Prequisite Installs and Imports\n",
        "A set of scripts that might need to be ran before your environment will run properly. This has been written with collab in mind and might not be needed. If you regularly have to install certain libraries please initiate a pull request with CIBMTR/Kaggle to modify the competition as they do not include internet access for the analysis section of the competition - this begins after the competition is over. Any additional libraries that you need installed have to be added by them in order to properly run.\n",
        "\n",
        "Start by adding your library to the import page, if it natively runs you may not need an install. If it does not or if you get outdated errors, consider adding it to the install section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X71jqb_k-ydd"
      },
      "source": [
        "### Installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "saT7WOmq8t89",
        "outputId": "3af41967-0588-4119-bb40-2e5ba025e37f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (0.3.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kagglehub) (24.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from kagglehub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2025.1.31)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.6.17)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.3.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.11/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kaggle) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kaggle) (3.10)\n"
          ]
        }
      ],
      "source": [
        "%pip install kagglehub\n",
        "%pip install kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gl1TE8Un-5FS"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6NDiNPh-7Yn"
      },
      "outputs": [],
      "source": [
        "import kagglehub, shutil, numpy as np, os, pandas as pd\n",
        "import shap, torch, torch.nn as nn\n",
        "from google.colab import userdata, drive, files\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aae1_TMU_Wpy"
      },
      "source": [
        "## Kaggle API Access Codeblock\n",
        "Many different options are included here because...both google and kaggle keep changing what they will and wont allow. Forcing us to change the method in which we login. Option 5 works for me right now. This page has people with other methods:\n",
        "https://www.kaggle.com/discussions/general/51898\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwSuKIH9_o7b"
      },
      "source": [
        "### Option 1\n",
        "login using the credentials generated from kaggle. This is different from the username and login that you use in kaggle.\n",
        "\n",
        "Instead you will...\n",
        "1. Go to kaggle.com\n",
        "2. Select your profile\n",
        "3. Go to settings\n",
        "4. Generate new api keys. (alternatively if you already have them generated you can use them.)\n",
        "5. It will generate and download a 'kaggle.json' file that contains your username and password for the API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgFwvbyG_s4B"
      },
      "outputs": [],
      "source": [
        "#kagglehub.login()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOhffD0I_tUA"
      },
      "source": [
        "### Option 2\n",
        "You will hardcode your username and password (see option 1 for more details on how to get these) and export them to kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrojyKH__uOo"
      },
      "outputs": [],
      "source": [
        "#export KAGGLE_USERNAME = your_un\n",
        "#export KAGGLE_KEY = your_generated_api_key/token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqACleT6_unR"
      },
      "source": [
        "### Option 3\n",
        "upload your kaggle.json file to the file structure at this path\n",
        "\n",
        "~/.kaggle/kaggle.json\n",
        "\n",
        "By default this is where !kaggle commands look for your credentials\n",
        "\n",
        "If your going to use this option make sure you upload your kaggle.json file to your google drive and named 'kaggle.json' - this code will take care of the rest.\n",
        "\n",
        "In order to generate this private key all you have to do is:\n",
        "1. Login to kaggle\n",
        "2. Click your profile in the upper right corner\n",
        "3. Click Settings\n",
        "4. Scroll down to API and create new token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4hG5jTO_yy8"
      },
      "outputs": [],
      "source": [
        "#drive.mount('/content/drive')\n",
        "#SourceFile = '/content/drive/MyDrive/kaggle.json'\n",
        "#!mkdir -p /root/.kaggle\n",
        "#DestinationFile = '/root/.kaggle'\n",
        "#shutil.copy(SourceFile,DestinationFile)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JG9mdVn_0jX"
      },
      "source": [
        "### Option 4\n",
        "update your Secretes page (the key symbol on the left) to include kaggle credentials:\n",
        " 1. Click the little key on left tab (default locations and icons for google collab)\n",
        " 2. Add new secret (yes use all capitals for the Name)\n",
        " 3. Add KAGGLE_USERNAME\n",
        " 4. Add KAGGLE_KEY\n",
        " 5. Toggle Notebook access to on\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXDslN8MGfAr"
      },
      "outputs": [],
      "source": [
        "#os.environ[\"KAGGLE_KEY\"] = userdata.get('KAGGLE_KEY')\n",
        "#os.environ[\"KAGGLE_USERNAME\"] = userdata.get('KAGGLE_USERNAME')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzatHwLIJkj2"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4ULFsk8W1nJ"
      },
      "source": [
        "### Option 5\n",
        "Change the Kaggle CLI working directory\n",
        "1. Login to kaggle\n",
        "2. Click your profile in the upper right corner\n",
        "3. Click Settings\n",
        "4. Scroll down to API and create new token\n",
        "5. Move that token to your google drive under the folder .kaggle\n",
        "\n",
        "(if you don't have the folder on your drive create it or run this script once and it will create it for you even though you'll get an error)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53h48GdhKl8M",
        "outputId": "1d7f86c9-f9e7-41c1-8c8f-f9f83c38e983"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')\n",
        "!mkdir -p /content/drive/MyDrive/.kaggle\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/drive/MyDrive/.kaggle/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zN1SP13RJ4cu"
      },
      "source": [
        "## Data\n",
        "The following code blocks are dedicated to data loading, cleaning and normalization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTiZgPY0KAOd"
      },
      "source": [
        "### Data Loading\n",
        "Use this code to load the data using the API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdgabvL8Ymd0"
      },
      "source": [
        "#### Download the Data from Kaggle\n",
        "You only need to run this once but can be ran everytime as well(details below).\n",
        "\n",
        "When you download, by default, it will put it on your google drive. It will be under the following file structure:\n",
        "\n",
        "- zip file ---- MyDrive/CIBMTR_Equity/dataset/equity-post-HCT-survival-predictions.zip\n",
        "- unzipped ---- MyDrive/CIBMTR_Equity\n",
        "\n",
        "With the following files in ../CIBMTR_Equity/:\n",
        "- data_dictionary.csv\n",
        "- sample_submission.csv\n",
        "- test.csv\n",
        "- train.csv\n",
        "\n",
        "\n",
        "The script is designed to check for updates when needed and skip if you have the most up to date zip file. Likewise it checks the files unzipped and compares them to files already in the directory. If there's a difference it will replace them - you may want to save your work in a different subdirectory or use a different file to manipulate your data as you go if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jorH9aVmKTj-",
        "outputId": "5a254d13-1524-4aec-aa31-ddcce101931a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive\n",
            "/content/drive/MyDrive/CIBMTR_Equity/dataset\n",
            "/content/drive/MyDrive\n",
            "equity-post-HCT-survival-predictions.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Archive:  equity-post-HCT-survival-predictions.zip\n",
            "/content/drive/MyDrive/CIBMTR_Equity\n"
          ]
        }
      ],
      "source": [
        "#Data Load\n",
        "#Use the drive folders and create RSNA24/dataset if it doesn't exist\n",
        "%cd /content/drive/MyDrive\n",
        "%mkdir -p CIBMTR_Equity/dataset\n",
        "%cd /content/drive/MyDrive/CIBMTR_Equity/dataset\n",
        "\n",
        "#Download and unzip the data from kaggle\n",
        "#ETA Instant -\n",
        "%cd /content/drive/MyDrive\n",
        "!kaggle competitions download -c equity-post-HCT-survival-predictions\n",
        "!unzip -u equity-post-HCT-survival-predictions.zip -d /content/drive/MyDrive/CIBMTR_Equity\n",
        "\n",
        "%cd CIBMTR_Equity/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kf3xNWkbSJO"
      },
      "source": [
        "#### Load the data into a pandas dataframe\n",
        "Now that we have the data let's get it loaded into panda so we can look at it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QE6cGnXnbj9u"
      },
      "outputs": [],
      "source": [
        "dfDataDict = pd.read_csv('data_dictionary.csv')\n",
        "dfSample = pd.read_csv('sample_submission.csv')\n",
        "dfTest = pd.read_csv('test.csv')\n",
        "dfTrain = pd.read_csv('train.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozaULplfKYwd"
      },
      "source": [
        "### Data Cleaning\n",
        "This is the code block where we will clean the data. Here we may have to remove bad data, change formats, check for errors, separate different forms of data etc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBn4wtZ1kixM"
      },
      "source": [
        "#### Data Dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ajd_FqtWn-ua"
      },
      "source": [
        "tbi = traumatic brain injury"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Pba7Jzh6KlAW",
        "outputId": "dac139af-35bd-440f-b907-faa0544731f4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"dfDataDict\",\n  \"rows\": 59,\n  \"fields\": [\n    {\n      \"column\": \"variable\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 59,\n        \"samples\": [\n          \"dri_score\",\n          \"hla_high_res_8\",\n          \"hepatic_severe\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"description\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 57,\n        \"samples\": [\n          \"Refined disease risk index\",\n          \"Recipient / 1st donor allele-level (high resolution) matching at HLA-A,-B,-C,-DRB1\",\n          \"MRD at time of HCT (AML/ALL)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Numerical\",\n          \"Categorical\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"values\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 26,\n        \"samples\": [\n          \"['ALL' 'MPN' 'IPA' 'AML' 'MDS' 'Other acute leukemia' 'AI' 'SAA' 'IEA'\\n 'NHL' 'PCD' 'IIS' 'HIS' 'Other leukemia' 'Solid tumor' 'IMD' 'HD' 'CML']\",\n          \"['Yes' 'No' nan]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "dfDataDict"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-c884a9ff-496d-4057-bb0c-ace775d194e4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>variable</th>\n",
              "      <th>description</th>\n",
              "      <th>type</th>\n",
              "      <th>values</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>dri_score</td>\n",
              "      <td>Refined disease risk index</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['Intermediate' 'High' 'N/A - non-malignant in...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>psych_disturb</td>\n",
              "      <td>Psychiatric disturbance</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['Yes' 'No' nan 'Not done']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>cyto_score</td>\n",
              "      <td>Cytogenetic score</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['Intermediate' 'Favorable' 'Poor' 'TBD' nan '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>diabetes</td>\n",
              "      <td>Diabetes</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['No' 'Yes' nan 'Not done']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>hla_match_c_high</td>\n",
              "      <td>Recipient / 1st donor allele level (high resol...</td>\n",
              "      <td>Numerical</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>hla_high_res_8</td>\n",
              "      <td>Recipient / 1st donor allele-level (high resol...</td>\n",
              "      <td>Numerical</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>tbi_status</td>\n",
              "      <td>TBI</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['No TBI' 'TBI + Cy +- Other' 'TBI +- Other, &lt;...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>arrhythmia</td>\n",
              "      <td>Arrhythmia</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['No' nan 'Yes' 'Not done']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>hla_low_res_6</td>\n",
              "      <td>Recipient / 1st donor antigen-level (low resol...</td>\n",
              "      <td>Numerical</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>graft_type</td>\n",
              "      <td>Graft type</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['Peripheral blood' 'Bone marrow']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>vent_hist</td>\n",
              "      <td>History of mechanical ventilation</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['No' 'Yes' nan]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>renal_issue</td>\n",
              "      <td>Renal, moderate / severe</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['No' nan 'Yes' 'Not done']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>pulm_severe</td>\n",
              "      <td>Pulmonary, severe</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['No' 'Yes' nan 'Not done']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>prim_disease_hct</td>\n",
              "      <td>Primary disease for HCT</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['ALL' 'MPN' 'IPA' 'AML' 'MDS' 'Other acute le...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>hla_high_res_6</td>\n",
              "      <td>Recipient / 1st donor allele-level (high resol...</td>\n",
              "      <td>Numerical</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>cmv_status</td>\n",
              "      <td>Donor/recipient CMV serostatus</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['+/-' '+/+' '-/-' '-/+' nan]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>hla_high_res_10</td>\n",
              "      <td>Recipient / 1st donor allele-level (high resol...</td>\n",
              "      <td>Numerical</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>hla_match_dqb1_high</td>\n",
              "      <td>Recipient / 1st donor allele level (high resol...</td>\n",
              "      <td>Numerical</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>tce_imm_match</td>\n",
              "      <td>T-cell epitope immunogenicity/diversity match</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['P/P' nan 'G/G' 'H/H' 'G/B' 'H/B' 'P/H' 'P/G'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>hla_nmdp_6</td>\n",
              "      <td>Recipient / 1st donor matching at HLA-A(lo),-B...</td>\n",
              "      <td>Numerical</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>hla_match_c_low</td>\n",
              "      <td>Recipient / 1st donor antigen level (low resol...</td>\n",
              "      <td>Numerical</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>rituximab</td>\n",
              "      <td>Rituximab given in conditioning</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['No' 'Yes' nan]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>hla_match_drb1_low</td>\n",
              "      <td>Recipient / 1st donor antigen level (low resol...</td>\n",
              "      <td>Numerical</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>hla_match_dqb1_low</td>\n",
              "      <td>Recipient / 1st donor antigen level (low resol...</td>\n",
              "      <td>Numerical</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>prod_type</td>\n",
              "      <td>Product type</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['PB' 'BM']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>cyto_score_detail</td>\n",
              "      <td>Cytogenetics for DRI (AML/MDS)</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['Intermediate' nan 'TBD' 'Favorable' 'Poor' '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>conditioning_intensity</td>\n",
              "      <td>Computed planned conditioning intensity</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['RIC' nan 'NMA' 'MAC' 'TBD' 'No drugs reporte...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>ethnicity</td>\n",
              "      <td>Ethnicity</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['Not Hispanic or Latino' 'Hispanic or Latino'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>year_hct</td>\n",
              "      <td>Year of HCT</td>\n",
              "      <td>Numerical</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>obesity</td>\n",
              "      <td>Obesity</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['No' 'Yes' nan 'Not done']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>mrd_hct</td>\n",
              "      <td>MRD at time of HCT (AML/ALL)</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>[nan 'Negative' 'Positive']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>in_vivo_tcd</td>\n",
              "      <td>In-vivo T-cell depletion (ATG/alemtuzumab)</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['Yes' 'No' nan]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>tce_match</td>\n",
              "      <td>T-cell epitope matching</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['Permissive' 'Fully matched' nan 'GvH non-per...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>hla_match_a_high</td>\n",
              "      <td>Recipient / 1st donor allele level (high resol...</td>\n",
              "      <td>Numerical</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>hepatic_severe</td>\n",
              "      <td>Hepatic, moderate / severe</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['No' nan 'Yes' 'Not done']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>donor_age</td>\n",
              "      <td>Donor age</td>\n",
              "      <td>Numerical</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>prior_tumor</td>\n",
              "      <td>Solid tumor, prior</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['Yes' 'No' nan 'Not done']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>hla_match_b_low</td>\n",
              "      <td>Recipient / 1st donor antigen level (low resol...</td>\n",
              "      <td>Numerical</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>peptic_ulcer</td>\n",
              "      <td>Peptic ulcer</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['No' nan 'Yes' 'Not done']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>age_at_hct</td>\n",
              "      <td>Age at HCT</td>\n",
              "      <td>Numerical</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>hla_match_a_low</td>\n",
              "      <td>Recipient / 1st donor antigen level (low resol...</td>\n",
              "      <td>Numerical</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>gvhd_proph</td>\n",
              "      <td>Planned GVHD prophylaxis</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['FK+ MMF +- others' 'Parent Q = yes, but no a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>rheum_issue</td>\n",
              "      <td>Rheumatologic</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['No' nan 'Yes' 'Not done']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>sex_match</td>\n",
              "      <td>Donor/recipient sex match</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['M-M' 'F-F' 'F-M' 'M-F' nan]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>hla_match_b_high</td>\n",
              "      <td>Recipient / 1st donor allele level (high resol...</td>\n",
              "      <td>Numerical</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>race_group</td>\n",
              "      <td>Race</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['White' 'Black or African-American'\\n 'Native...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>comorbidity_score</td>\n",
              "      <td>Sorror comorbidity score</td>\n",
              "      <td>Numerical</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>karnofsky_score</td>\n",
              "      <td>KPS at HCT</td>\n",
              "      <td>Numerical</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>hepatic_mild</td>\n",
              "      <td>Hepatic, mild</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['No' 'Yes' nan 'Not done']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>tce_div_match</td>\n",
              "      <td>T-cell epitope matching</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['Permissive mismatched' 'Bi-directional non-p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>donor_related</td>\n",
              "      <td>Related vs. unrelated donor</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['Unrelated' 'Related' 'Multiple donor (non-UC...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>melphalan_dose</td>\n",
              "      <td>Melphalan dose (mg/m^2)</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['N/A, Mel not given' 'MEL' nan]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>hla_low_res_8</td>\n",
              "      <td>Recipient / 1st donor antigen-level (low resol...</td>\n",
              "      <td>Numerical</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>cardiac</td>\n",
              "      <td>Cardiac</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['No' 'Yes' nan 'Not done']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>hla_match_drb1_high</td>\n",
              "      <td>Recipient / 1st donor allele level (high resol...</td>\n",
              "      <td>Numerical</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>pulm_moderate</td>\n",
              "      <td>Pulmonary, moderate</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['Yes' 'Not done' 'No' nan]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>hla_low_res_10</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Numerical</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>efs</td>\n",
              "      <td>Event-free survival</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['Event' 'Censoring']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>efs_time</td>\n",
              "      <td>Time to event-free survival, months</td>\n",
              "      <td>Numerical</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c884a9ff-496d-4057-bb0c-ace775d194e4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c884a9ff-496d-4057-bb0c-ace775d194e4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c884a9ff-496d-4057-bb0c-ace775d194e4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a684b720-c517-4ed4-a254-eaf0624b69cb\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a684b720-c517-4ed4-a254-eaf0624b69cb')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a684b720-c517-4ed4-a254-eaf0624b69cb button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_f30a353c-0ff6-4300-ad41-1b3986ad4ce6\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('dfDataDict')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_f30a353c-0ff6-4300-ad41-1b3986ad4ce6 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('dfDataDict');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                  variable                                        description  \\\n",
              "0                dri_score                         Refined disease risk index   \n",
              "1            psych_disturb                            Psychiatric disturbance   \n",
              "2               cyto_score                                  Cytogenetic score   \n",
              "3                 diabetes                                           Diabetes   \n",
              "4         hla_match_c_high  Recipient / 1st donor allele level (high resol...   \n",
              "5           hla_high_res_8  Recipient / 1st donor allele-level (high resol...   \n",
              "6               tbi_status                                                TBI   \n",
              "7               arrhythmia                                         Arrhythmia   \n",
              "8            hla_low_res_6  Recipient / 1st donor antigen-level (low resol...   \n",
              "9               graft_type                                         Graft type   \n",
              "10               vent_hist                  History of mechanical ventilation   \n",
              "11             renal_issue                           Renal, moderate / severe   \n",
              "12             pulm_severe                                  Pulmonary, severe   \n",
              "13        prim_disease_hct                            Primary disease for HCT   \n",
              "14          hla_high_res_6  Recipient / 1st donor allele-level (high resol...   \n",
              "15              cmv_status                     Donor/recipient CMV serostatus   \n",
              "16         hla_high_res_10  Recipient / 1st donor allele-level (high resol...   \n",
              "17     hla_match_dqb1_high  Recipient / 1st donor allele level (high resol...   \n",
              "18           tce_imm_match      T-cell epitope immunogenicity/diversity match   \n",
              "19              hla_nmdp_6  Recipient / 1st donor matching at HLA-A(lo),-B...   \n",
              "20         hla_match_c_low  Recipient / 1st donor antigen level (low resol...   \n",
              "21               rituximab                    Rituximab given in conditioning   \n",
              "22      hla_match_drb1_low  Recipient / 1st donor antigen level (low resol...   \n",
              "23      hla_match_dqb1_low  Recipient / 1st donor antigen level (low resol...   \n",
              "24               prod_type                                       Product type   \n",
              "25       cyto_score_detail                     Cytogenetics for DRI (AML/MDS)   \n",
              "26  conditioning_intensity            Computed planned conditioning intensity   \n",
              "27               ethnicity                                          Ethnicity   \n",
              "28                year_hct                                        Year of HCT   \n",
              "29                 obesity                                            Obesity   \n",
              "30                 mrd_hct                       MRD at time of HCT (AML/ALL)   \n",
              "31             in_vivo_tcd         In-vivo T-cell depletion (ATG/alemtuzumab)   \n",
              "32               tce_match                            T-cell epitope matching   \n",
              "33        hla_match_a_high  Recipient / 1st donor allele level (high resol...   \n",
              "34          hepatic_severe                         Hepatic, moderate / severe   \n",
              "35               donor_age                                          Donor age   \n",
              "36             prior_tumor                                 Solid tumor, prior   \n",
              "37         hla_match_b_low  Recipient / 1st donor antigen level (low resol...   \n",
              "38            peptic_ulcer                                       Peptic ulcer   \n",
              "39              age_at_hct                                         Age at HCT   \n",
              "40         hla_match_a_low  Recipient / 1st donor antigen level (low resol...   \n",
              "41              gvhd_proph                           Planned GVHD prophylaxis   \n",
              "42             rheum_issue                                      Rheumatologic   \n",
              "43               sex_match                          Donor/recipient sex match   \n",
              "44        hla_match_b_high  Recipient / 1st donor allele level (high resol...   \n",
              "45              race_group                                               Race   \n",
              "46       comorbidity_score                           Sorror comorbidity score   \n",
              "47         karnofsky_score                                         KPS at HCT   \n",
              "48            hepatic_mild                                      Hepatic, mild   \n",
              "49           tce_div_match                            T-cell epitope matching   \n",
              "50           donor_related                        Related vs. unrelated donor   \n",
              "51          melphalan_dose                            Melphalan dose (mg/m^2)   \n",
              "52           hla_low_res_8  Recipient / 1st donor antigen-level (low resol...   \n",
              "53                 cardiac                                            Cardiac   \n",
              "54     hla_match_drb1_high  Recipient / 1st donor allele level (high resol...   \n",
              "55           pulm_moderate                                Pulmonary, moderate   \n",
              "56          hla_low_res_10                                                NaN   \n",
              "57                     efs                                Event-free survival   \n",
              "58                efs_time                Time to event-free survival, months   \n",
              "\n",
              "           type                                             values  \n",
              "0   Categorical  ['Intermediate' 'High' 'N/A - non-malignant in...  \n",
              "1   Categorical                        ['Yes' 'No' nan 'Not done']  \n",
              "2   Categorical  ['Intermediate' 'Favorable' 'Poor' 'TBD' nan '...  \n",
              "3   Categorical                        ['No' 'Yes' nan 'Not done']  \n",
              "4     Numerical                                                NaN  \n",
              "5     Numerical                                                NaN  \n",
              "6   Categorical  ['No TBI' 'TBI + Cy +- Other' 'TBI +- Other, <...  \n",
              "7   Categorical                        ['No' nan 'Yes' 'Not done']  \n",
              "8     Numerical                                                NaN  \n",
              "9   Categorical                 ['Peripheral blood' 'Bone marrow']  \n",
              "10  Categorical                                   ['No' 'Yes' nan]  \n",
              "11  Categorical                        ['No' nan 'Yes' 'Not done']  \n",
              "12  Categorical                        ['No' 'Yes' nan 'Not done']  \n",
              "13  Categorical  ['ALL' 'MPN' 'IPA' 'AML' 'MDS' 'Other acute le...  \n",
              "14    Numerical                                                NaN  \n",
              "15  Categorical                      ['+/-' '+/+' '-/-' '-/+' nan]  \n",
              "16    Numerical                                                NaN  \n",
              "17    Numerical                                                NaN  \n",
              "18  Categorical  ['P/P' nan 'G/G' 'H/H' 'G/B' 'H/B' 'P/H' 'P/G'...  \n",
              "19    Numerical                                                NaN  \n",
              "20    Numerical                                                NaN  \n",
              "21  Categorical                                   ['No' 'Yes' nan]  \n",
              "22    Numerical                                                NaN  \n",
              "23    Numerical                                                NaN  \n",
              "24  Categorical                                        ['PB' 'BM']  \n",
              "25  Categorical  ['Intermediate' nan 'TBD' 'Favorable' 'Poor' '...  \n",
              "26  Categorical  ['RIC' nan 'NMA' 'MAC' 'TBD' 'No drugs reporte...  \n",
              "27  Categorical  ['Not Hispanic or Latino' 'Hispanic or Latino'...  \n",
              "28    Numerical                                                NaN  \n",
              "29  Categorical                        ['No' 'Yes' nan 'Not done']  \n",
              "30  Categorical                        [nan 'Negative' 'Positive']  \n",
              "31  Categorical                                   ['Yes' 'No' nan]  \n",
              "32  Categorical  ['Permissive' 'Fully matched' nan 'GvH non-per...  \n",
              "33    Numerical                                                NaN  \n",
              "34  Categorical                        ['No' nan 'Yes' 'Not done']  \n",
              "35    Numerical                                                NaN  \n",
              "36  Categorical                        ['Yes' 'No' nan 'Not done']  \n",
              "37    Numerical                                                NaN  \n",
              "38  Categorical                        ['No' nan 'Yes' 'Not done']  \n",
              "39    Numerical                                                NaN  \n",
              "40    Numerical                                                NaN  \n",
              "41  Categorical  ['FK+ MMF +- others' 'Parent Q = yes, but no a...  \n",
              "42  Categorical                        ['No' nan 'Yes' 'Not done']  \n",
              "43  Categorical                      ['M-M' 'F-F' 'F-M' 'M-F' nan]  \n",
              "44    Numerical                                                NaN  \n",
              "45  Categorical  ['White' 'Black or African-American'\\n 'Native...  \n",
              "46    Numerical                                                NaN  \n",
              "47    Numerical                                                NaN  \n",
              "48  Categorical                        ['No' 'Yes' nan 'Not done']  \n",
              "49  Categorical  ['Permissive mismatched' 'Bi-directional non-p...  \n",
              "50  Categorical  ['Unrelated' 'Related' 'Multiple donor (non-UC...  \n",
              "51  Categorical                   ['N/A, Mel not given' 'MEL' nan]  \n",
              "52    Numerical                                                NaN  \n",
              "53  Categorical                        ['No' 'Yes' nan 'Not done']  \n",
              "54    Numerical                                                NaN  \n",
              "55  Categorical                        ['Yes' 'Not done' 'No' nan]  \n",
              "56    Numerical                                                NaN  \n",
              "57  Categorical                              ['Event' 'Censoring']  \n",
              "58    Numerical                                                NaN  "
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "    #Data Cleaning\n",
        "## First let's show the samples of each file:\n",
        "dfDataDict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtgeEVnBqDJR"
      },
      "source": [
        "### Modify Data Dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hSQQrihCqNGO",
        "outputId": "9af6897a-9ac8-4a77-ce33-d33b3ebfd1d2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"dfDataDict\",\n  \"rows\": 59,\n  \"fields\": [\n    {\n      \"column\": \"variable\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 59,\n        \"samples\": [\n          \"dri_score\",\n          \"hla_high_res_8\",\n          \"hepatic_severe\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"description\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 59,\n        \"samples\": [\n          \"Refined Disease Risk Index (DRI), categorizing the risk level of a patient's disease.\",\n          \"HLA High-Resolution Matching at A, B, C, and DRB1.\",\n          \"Presence of moderate/severe liver disease.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Numerical\",\n          \"Categorical\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"values\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 26,\n        \"samples\": [\n          \"['ALL' 'MPN' 'IPA' 'AML' 'MDS' 'Other acute leukemia' 'AI' 'SAA' 'IEA'\\n 'NHL' 'PCD' 'IIS' 'HIS' 'Other leukemia' 'Solid tumor' 'IMD' 'HD' 'CML']\",\n          \"['Yes' 'No' nan]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "dfDataDict"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-c7be8fd9-4f6f-4e1c-bbc4-e5d9a0bd332f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>variable</th>\n",
              "      <th>description</th>\n",
              "      <th>type</th>\n",
              "      <th>values</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>dri_score</td>\n",
              "      <td>Refined Disease Risk Index (DRI), categorizing...</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['Intermediate' 'High' 'N/A - non-malignant in...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>psych_disturb</td>\n",
              "      <td>Indicates whether the patient has a psychiatri...</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['Yes' 'No' nan 'Not done']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>cyto_score</td>\n",
              "      <td>Cytogenetic score assessing chromosomal abnorm...</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['Intermediate' 'Favorable' 'Poor' 'TBD' nan '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>diabetes</td>\n",
              "      <td>Indicates if the patient has been diagnosed wi...</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['No' 'Yes' nan 'Not done']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>hla_match_c_high</td>\n",
              "      <td>High-resolution HLA-C match.</td>\n",
              "      <td>Numerical</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>hla_high_res_8</td>\n",
              "      <td>HLA High-Resolution Matching at A, B, C, and D...</td>\n",
              "      <td>Numerical</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>tbi_status</td>\n",
              "      <td>Whether Total Body Irradiation (TBI) was used ...</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['No TBI' 'TBI + Cy +- Other' 'TBI +- Other, &lt;...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>arrhythmia</td>\n",
              "      <td>Presence of cardiac arrhythmia (irregular hear...</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['No' nan 'Yes' 'Not done']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>hla_low_res_6</td>\n",
              "      <td>HLA Low-Resolution Matching at A, B, and DRB1.</td>\n",
              "      <td>Numerical</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>graft_type</td>\n",
              "      <td>Type of graft used in transplantation (e.g., b...</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['Peripheral blood' 'Bone marrow']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>vent_hist</td>\n",
              "      <td>History of mechanical ventilation support.</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['No' 'Yes' nan]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>renal_issue</td>\n",
              "      <td>Severity of renal (kidney) dysfunction.</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['No' nan 'Yes' 'Not done']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>pulm_severe</td>\n",
              "      <td>Indicates severe pulmonary (lung) disease.</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['No' 'Yes' nan 'Not done']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>prim_disease_hct</td>\n",
              "      <td>Primary disease leading to Hematopoietic Cell ...</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['ALL' 'MPN' 'IPA' 'AML' 'MDS' 'Other acute le...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>hla_high_res_6</td>\n",
              "      <td>HLA High-Resolution Matching at A, B, and DRB1.</td>\n",
              "      <td>Numerical</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>cmv_status</td>\n",
              "      <td>Cytomegalovirus (CMV) serostatus of donor and ...</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['+/-' '+/+' '-/-' '-/+' nan]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>hla_high_res_10</td>\n",
              "      <td>HLA High-Resolution Matching at A, B, C, DRB1,...</td>\n",
              "      <td>Numerical</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>hla_match_dqb1_high</td>\n",
              "      <td>High-resolution HLA-DQB1 match.</td>\n",
              "      <td>Numerical</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>tce_imm_match</td>\n",
              "      <td>TCE Immunogenicity match assessment.</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['P/P' nan 'G/G' 'H/H' 'G/B' 'H/B' 'P/H' 'P/G'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>hla_nmdp_6</td>\n",
              "      <td>HLA matching based on National Marrow Donor Pr...</td>\n",
              "      <td>Numerical</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>hla_match_c_low</td>\n",
              "      <td>Recipient / 1st donor antigen level (low resol...</td>\n",
              "      <td>Numerical</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>rituximab</td>\n",
              "      <td>Use of Rituximab, a monoclonal antibody in con...</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['No' 'Yes' nan]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>hla_match_drb1_low</td>\n",
              "      <td>Recipient / 1st donor antigen level (low resol...</td>\n",
              "      <td>Numerical</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>hla_match_dqb1_low</td>\n",
              "      <td>Recipient / 1st donor antigen level (low resol...</td>\n",
              "      <td>Numerical</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>prod_type</td>\n",
              "      <td>Type of cell product used (bone marrow, periph...</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['PB' 'BM']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>cyto_score_detail</td>\n",
              "      <td>Detailed cytogenetics classification for Disea...</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['Intermediate' nan 'TBD' 'Favorable' 'Poor' '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>conditioning_intensity</td>\n",
              "      <td>Intensity of chemotherapy/radiation regimen be...</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['RIC' nan 'NMA' 'MAC' 'TBD' 'No drugs reporte...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>ethnicity</td>\n",
              "      <td>Ethnicity classification of the patient.</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['Not Hispanic or Latino' 'Hispanic or Latino'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>year_hct</td>\n",
              "      <td>Year when the Hematopoietic Cell Transplantati...</td>\n",
              "      <td>Numerical</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>obesity</td>\n",
              "      <td>Indicates if the patient is obese.</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['No' 'Yes' nan 'Not done']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>mrd_hct</td>\n",
              "      <td>Minimal Residual Disease (MRD) status at HCT.</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>[nan 'Negative' 'Positive']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>in_vivo_tcd</td>\n",
              "      <td>In-Vivo T-Cell Depletion (TCD) method used, su...</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['Yes' 'No' nan]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>tce_match</td>\n",
              "      <td>T-Cell Epitope (TCE) matching between donor an...</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['Permissive' 'Fully matched' nan 'GvH non-per...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>hla_match_a_high</td>\n",
              "      <td>High-resolution HLA-A match between recipient ...</td>\n",
              "      <td>Numerical</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>hepatic_severe</td>\n",
              "      <td>Presence of moderate/severe liver disease.</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['No' nan 'Yes' 'Not done']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>donor_age</td>\n",
              "      <td>Age of the donor at the time of transplant.</td>\n",
              "      <td>Numerical</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>prior_tumor</td>\n",
              "      <td>History of prior solid tumors (non-blood cance...</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['Yes' 'No' nan 'Not done']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>hla_match_b_low</td>\n",
              "      <td>Recipient / 1st donor antigen level (low resol...</td>\n",
              "      <td>Numerical</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>peptic_ulcer</td>\n",
              "      <td>History of peptic ulcer disease (PUD).</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['No' nan 'Yes' 'Not done']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>age_at_hct</td>\n",
              "      <td>Patient’s age at the time of HCT.</td>\n",
              "      <td>Numerical</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>hla_match_a_low</td>\n",
              "      <td>Recipient / 1st donor antigen level (low resol...</td>\n",
              "      <td>Numerical</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>gvhd_proph</td>\n",
              "      <td>Graft-versus-Host Disease (GVHD) prophylaxis m...</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['FK+ MMF +- others' 'Parent Q = yes, but no a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>rheum_issue</td>\n",
              "      <td>Presence of rheumatologic conditions (e.g., lu...</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['No' nan 'Yes' 'Not done']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>sex_match</td>\n",
              "      <td>Sex matching between donor and recipient.</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['M-M' 'F-F' 'F-M' 'M-F' nan]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>hla_match_b_high</td>\n",
              "      <td>High-resolution HLA-B match.</td>\n",
              "      <td>Numerical</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>race_group</td>\n",
              "      <td>Race classification of the patient.</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['White' 'Black or African-American'\\n 'Native...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>comorbidity_score</td>\n",
              "      <td>Hematopoietic Cell Transplantation Comorbidity...</td>\n",
              "      <td>Numerical</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>karnofsky_score</td>\n",
              "      <td>Karnofsky Performance Status (KPS) at HCT, mea...</td>\n",
              "      <td>Numerical</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>hepatic_mild</td>\n",
              "      <td>Indicates mild liver disease.</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['No' 'Yes' nan 'Not done']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>tce_div_match</td>\n",
              "      <td>TCE Diversity match assessment.</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['Permissive mismatched' 'Bi-directional non-p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>donor_related</td>\n",
              "      <td>Indicates whether the donor was related or unr...</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['Unrelated' 'Related' 'Multiple donor (non-UC...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>melphalan_dose</td>\n",
              "      <td>Dose of Melphalan chemotherapy drug (mg/m²).</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['N/A, Mel not given' 'MEL' nan]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>hla_low_res_8</td>\n",
              "      <td>HLA Low-Resolution Matching at A, B, C, and DRB1.</td>\n",
              "      <td>Numerical</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>cardiac</td>\n",
              "      <td>Cardiac</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['No' 'Yes' nan 'Not done']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>hla_match_drb1_high</td>\n",
              "      <td>High-resolution HLA-DRB1 match.</td>\n",
              "      <td>Numerical</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>pulm_moderate</td>\n",
              "      <td>Indicates moderate pulmonary disease.</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['Yes' 'Not done' 'No' nan]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>hla_low_res_10</td>\n",
              "      <td>HLA Low-Resolution Matching at A, B, C, DRB1, ...</td>\n",
              "      <td>Numerical</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>efs</td>\n",
              "      <td>Event-Free Survival (EFS), indicating if the p...</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>['Event' 'Censoring']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>efs_time</td>\n",
              "      <td>Time to event-free survival in months.</td>\n",
              "      <td>Numerical</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c7be8fd9-4f6f-4e1c-bbc4-e5d9a0bd332f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c7be8fd9-4f6f-4e1c-bbc4-e5d9a0bd332f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c7be8fd9-4f6f-4e1c-bbc4-e5d9a0bd332f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6e8438b7-012b-4939-ab47-922b2dd0f8fb\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6e8438b7-012b-4939-ab47-922b2dd0f8fb')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6e8438b7-012b-4939-ab47-922b2dd0f8fb button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_a678cb28-49f0-48a6-8d47-ec90761e4534\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('dfDataDict')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_a678cb28-49f0-48a6-8d47-ec90761e4534 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('dfDataDict');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                  variable                                        description  \\\n",
              "0                dri_score  Refined Disease Risk Index (DRI), categorizing...   \n",
              "1            psych_disturb  Indicates whether the patient has a psychiatri...   \n",
              "2               cyto_score  Cytogenetic score assessing chromosomal abnorm...   \n",
              "3                 diabetes  Indicates if the patient has been diagnosed wi...   \n",
              "4         hla_match_c_high                       High-resolution HLA-C match.   \n",
              "5           hla_high_res_8  HLA High-Resolution Matching at A, B, C, and D...   \n",
              "6               tbi_status  Whether Total Body Irradiation (TBI) was used ...   \n",
              "7               arrhythmia  Presence of cardiac arrhythmia (irregular hear...   \n",
              "8            hla_low_res_6     HLA Low-Resolution Matching at A, B, and DRB1.   \n",
              "9               graft_type  Type of graft used in transplantation (e.g., b...   \n",
              "10               vent_hist         History of mechanical ventilation support.   \n",
              "11             renal_issue            Severity of renal (kidney) dysfunction.   \n",
              "12             pulm_severe         Indicates severe pulmonary (lung) disease.   \n",
              "13        prim_disease_hct  Primary disease leading to Hematopoietic Cell ...   \n",
              "14          hla_high_res_6    HLA High-Resolution Matching at A, B, and DRB1.   \n",
              "15              cmv_status  Cytomegalovirus (CMV) serostatus of donor and ...   \n",
              "16         hla_high_res_10  HLA High-Resolution Matching at A, B, C, DRB1,...   \n",
              "17     hla_match_dqb1_high                    High-resolution HLA-DQB1 match.   \n",
              "18           tce_imm_match               TCE Immunogenicity match assessment.   \n",
              "19              hla_nmdp_6  HLA matching based on National Marrow Donor Pr...   \n",
              "20         hla_match_c_low  Recipient / 1st donor antigen level (low resol...   \n",
              "21               rituximab  Use of Rituximab, a monoclonal antibody in con...   \n",
              "22      hla_match_drb1_low  Recipient / 1st donor antigen level (low resol...   \n",
              "23      hla_match_dqb1_low  Recipient / 1st donor antigen level (low resol...   \n",
              "24               prod_type  Type of cell product used (bone marrow, periph...   \n",
              "25       cyto_score_detail  Detailed cytogenetics classification for Disea...   \n",
              "26  conditioning_intensity  Intensity of chemotherapy/radiation regimen be...   \n",
              "27               ethnicity           Ethnicity classification of the patient.   \n",
              "28                year_hct  Year when the Hematopoietic Cell Transplantati...   \n",
              "29                 obesity                 Indicates if the patient is obese.   \n",
              "30                 mrd_hct      Minimal Residual Disease (MRD) status at HCT.   \n",
              "31             in_vivo_tcd  In-Vivo T-Cell Depletion (TCD) method used, su...   \n",
              "32               tce_match  T-Cell Epitope (TCE) matching between donor an...   \n",
              "33        hla_match_a_high  High-resolution HLA-A match between recipient ...   \n",
              "34          hepatic_severe         Presence of moderate/severe liver disease.   \n",
              "35               donor_age        Age of the donor at the time of transplant.   \n",
              "36             prior_tumor  History of prior solid tumors (non-blood cance...   \n",
              "37         hla_match_b_low  Recipient / 1st donor antigen level (low resol...   \n",
              "38            peptic_ulcer             History of peptic ulcer disease (PUD).   \n",
              "39              age_at_hct                  Patient’s age at the time of HCT.   \n",
              "40         hla_match_a_low  Recipient / 1st donor antigen level (low resol...   \n",
              "41              gvhd_proph  Graft-versus-Host Disease (GVHD) prophylaxis m...   \n",
              "42             rheum_issue  Presence of rheumatologic conditions (e.g., lu...   \n",
              "43               sex_match          Sex matching between donor and recipient.   \n",
              "44        hla_match_b_high                       High-resolution HLA-B match.   \n",
              "45              race_group                Race classification of the patient.   \n",
              "46       comorbidity_score  Hematopoietic Cell Transplantation Comorbidity...   \n",
              "47         karnofsky_score  Karnofsky Performance Status (KPS) at HCT, mea...   \n",
              "48            hepatic_mild                      Indicates mild liver disease.   \n",
              "49           tce_div_match                    TCE Diversity match assessment.   \n",
              "50           donor_related  Indicates whether the donor was related or unr...   \n",
              "51          melphalan_dose       Dose of Melphalan chemotherapy drug (mg/m²).   \n",
              "52           hla_low_res_8  HLA Low-Resolution Matching at A, B, C, and DRB1.   \n",
              "53                 cardiac                                            Cardiac   \n",
              "54     hla_match_drb1_high                    High-resolution HLA-DRB1 match.   \n",
              "55           pulm_moderate              Indicates moderate pulmonary disease.   \n",
              "56          hla_low_res_10  HLA Low-Resolution Matching at A, B, C, DRB1, ...   \n",
              "57                     efs  Event-Free Survival (EFS), indicating if the p...   \n",
              "58                efs_time             Time to event-free survival in months.   \n",
              "\n",
              "           type                                             values  \n",
              "0   Categorical  ['Intermediate' 'High' 'N/A - non-malignant in...  \n",
              "1   Categorical                        ['Yes' 'No' nan 'Not done']  \n",
              "2   Categorical  ['Intermediate' 'Favorable' 'Poor' 'TBD' nan '...  \n",
              "3   Categorical                        ['No' 'Yes' nan 'Not done']  \n",
              "4     Numerical                                                NaN  \n",
              "5     Numerical                                                NaN  \n",
              "6   Categorical  ['No TBI' 'TBI + Cy +- Other' 'TBI +- Other, <...  \n",
              "7   Categorical                        ['No' nan 'Yes' 'Not done']  \n",
              "8     Numerical                                                NaN  \n",
              "9   Categorical                 ['Peripheral blood' 'Bone marrow']  \n",
              "10  Categorical                                   ['No' 'Yes' nan]  \n",
              "11  Categorical                        ['No' nan 'Yes' 'Not done']  \n",
              "12  Categorical                        ['No' 'Yes' nan 'Not done']  \n",
              "13  Categorical  ['ALL' 'MPN' 'IPA' 'AML' 'MDS' 'Other acute le...  \n",
              "14    Numerical                                                NaN  \n",
              "15  Categorical                      ['+/-' '+/+' '-/-' '-/+' nan]  \n",
              "16    Numerical                                                NaN  \n",
              "17    Numerical                                                NaN  \n",
              "18  Categorical  ['P/P' nan 'G/G' 'H/H' 'G/B' 'H/B' 'P/H' 'P/G'...  \n",
              "19    Numerical                                                NaN  \n",
              "20    Numerical                                                NaN  \n",
              "21  Categorical                                   ['No' 'Yes' nan]  \n",
              "22    Numerical                                                NaN  \n",
              "23    Numerical                                                NaN  \n",
              "24  Categorical                                        ['PB' 'BM']  \n",
              "25  Categorical  ['Intermediate' nan 'TBD' 'Favorable' 'Poor' '...  \n",
              "26  Categorical  ['RIC' nan 'NMA' 'MAC' 'TBD' 'No drugs reporte...  \n",
              "27  Categorical  ['Not Hispanic or Latino' 'Hispanic or Latino'...  \n",
              "28    Numerical                                                NaN  \n",
              "29  Categorical                        ['No' 'Yes' nan 'Not done']  \n",
              "30  Categorical                        [nan 'Negative' 'Positive']  \n",
              "31  Categorical                                   ['Yes' 'No' nan]  \n",
              "32  Categorical  ['Permissive' 'Fully matched' nan 'GvH non-per...  \n",
              "33    Numerical                                                NaN  \n",
              "34  Categorical                        ['No' nan 'Yes' 'Not done']  \n",
              "35    Numerical                                                NaN  \n",
              "36  Categorical                        ['Yes' 'No' nan 'Not done']  \n",
              "37    Numerical                                                NaN  \n",
              "38  Categorical                        ['No' nan 'Yes' 'Not done']  \n",
              "39    Numerical                                                NaN  \n",
              "40    Numerical                                                NaN  \n",
              "41  Categorical  ['FK+ MMF +- others' 'Parent Q = yes, but no a...  \n",
              "42  Categorical                        ['No' nan 'Yes' 'Not done']  \n",
              "43  Categorical                      ['M-M' 'F-F' 'F-M' 'M-F' nan]  \n",
              "44    Numerical                                                NaN  \n",
              "45  Categorical  ['White' 'Black or African-American'\\n 'Native...  \n",
              "46    Numerical                                                NaN  \n",
              "47    Numerical                                                NaN  \n",
              "48  Categorical                        ['No' 'Yes' nan 'Not done']  \n",
              "49  Categorical  ['Permissive mismatched' 'Bi-directional non-p...  \n",
              "50  Categorical  ['Unrelated' 'Related' 'Multiple donor (non-UC...  \n",
              "51  Categorical                   ['N/A, Mel not given' 'MEL' nan]  \n",
              "52    Numerical                                                NaN  \n",
              "53  Categorical                        ['No' 'Yes' nan 'Not done']  \n",
              "54    Numerical                                                NaN  \n",
              "55  Categorical                        ['Yes' 'Not done' 'No' nan]  \n",
              "56    Numerical                                                NaN  \n",
              "57  Categorical                              ['Event' 'Censoring']  \n",
              "58    Numerical                                                NaN  "
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Dictionary with additional details for each variable\n",
        "additional_details = {\n",
        "    \"dri_score\": \"Refined Disease Risk Index (DRI), categorizing the risk level of a patient's disease.\",\n",
        "    \"psych_disturb\": \"Indicates whether the patient has a psychiatric condition (Yes/No).\",\n",
        "    \"cyto_score\": \"Cytogenetic score assessing chromosomal abnormalities in diseases like leukemia.\",\n",
        "    \"diabetes\": \"Indicates if the patient has been diagnosed with diabetes (Yes/No).\",\n",
        "    \"arrhythmia\": \"Presence of cardiac arrhythmia (irregular heartbeats).\",\n",
        "    \"vent_hist\": \"History of mechanical ventilation support.\",\n",
        "    \"renal_issue\": \"Severity of renal (kidney) dysfunction.\",\n",
        "    \"pulm_severe\": \"Indicates severe pulmonary (lung) disease.\",\n",
        "    \"prim_disease_hct\": \"Primary disease leading to Hematopoietic Cell Transplantation (HCT).\",\n",
        "    \"obesity\": \"Indicates if the patient is obese.\",\n",
        "    \"hepatic_severe\": \"Presence of moderate/severe liver disease.\",\n",
        "    \"prior_tumor\": \"History of prior solid tumors (non-blood cancers).\",\n",
        "    \"peptic_ulcer\": \"History of peptic ulcer disease (PUD).\",\n",
        "    \"age_at_hct\": \"Patient’s age at the time of HCT.\",\n",
        "    \"rheum_issue\": \"Presence of rheumatologic conditions (e.g., lupus, rheumatoid arthritis).\",\n",
        "    \"efs\": \"Event-Free Survival (EFS), indicating if the patient remained free of relapse or complications.\",\n",
        "    \"efs_time\": \"Time to event-free survival in months.\",\n",
        "    \"graft_type\": \"Type of graft used in transplantation (e.g., bone marrow, peripheral blood stem cells).\",\n",
        "    \"conditioning_intensity\": \"Intensity of chemotherapy/radiation regimen before transplantation.\",\n",
        "    \"tbi_status\": \"Whether Total Body Irradiation (TBI) was used in conditioning.\",\n",
        "    \"rituximab\": \"Use of Rituximab, a monoclonal antibody in conditioning therapy.\",\n",
        "    \"prod_type\": \"Type of cell product used (bone marrow, peripheral blood, or cord blood).\",\n",
        "    \"melphalan_dose\": \"Dose of Melphalan chemotherapy drug (mg/m²).\",\n",
        "    \"gvhd_proph\": \"Graft-versus-Host Disease (GVHD) prophylaxis to prevent complications.\",\n",
        "    \"hla_nmdp_6\": \"HLA matching based on National Marrow Donor Program (NMDP) classification (A, B, DRB1).\",\n",
        "    \"hla_match_a_high\": \"High-resolution HLA-A match between recipient and donor.\",\n",
        "    \"hla_match_b_high\": \"High-resolution HLA-B match.\",\n",
        "    \"hla_match_c_high\": \"High-resolution HLA-C match.\",\n",
        "    \"hla_match_drb1_high\": \"High-resolution HLA-DRB1 match.\",\n",
        "    \"hla_match_dqb1_high\": \"High-resolution HLA-DQB1 match.\",\n",
        "    \"tce_match\": \"T-Cell Epitope (TCE) matching between donor and recipient.\",\n",
        "    \"comorbidity_score\": \"Hematopoietic Cell Transplantation Comorbidity Index (HCT-CI) by Sorror, assessing additional health risks before transplant.\",\n",
        "    \"karnofsky_score\": \"Karnofsky Performance Status (KPS) at HCT, measuring patient functional ability (scale 0-100).\",\n",
        "    \"hepatic_mild\": \"Indicates mild liver disease.\",\n",
        "    \"pulm_moderate\": \"Indicates moderate pulmonary disease.\",\n",
        "    \"year_hct\": \"Year when the Hematopoietic Cell Transplantation (HCT) was performed.\",\n",
        "    \"sex_match\": \"Sex matching between donor and recipient.\",\n",
        "    \"race_group\": \"Race classification of the patient.\",\n",
        "    \"ethnicity\": \"Ethnicity classification of the patient.\",\n",
        "    \"cmv_status\": \"Cytomegalovirus (CMV) serostatus of donor and recipient.\",\n",
        "    \"hla_low_res_6\": \"HLA Low-Resolution Matching at A, B, and DRB1.\",\n",
        "    \"hla_low_res_8\": \"HLA Low-Resolution Matching at A, B, C, and DRB1.\",\n",
        "    \"hla_low_res_10\": \"HLA Low-Resolution Matching at A, B, C, DRB1, and DQB1.\",\n",
        "    \"hla_high_res_6\": \"HLA High-Resolution Matching at A, B, and DRB1.\",\n",
        "    \"hla_high_res_8\": \"HLA High-Resolution Matching at A, B, C, and DRB1.\",\n",
        "    \"hla_high_res_10\": \"HLA High-Resolution Matching at A, B, C, DRB1, and DQB1.\",\n",
        "    \"tce_imm_match\": \"TCE Immunogenicity match assessment.\",\n",
        "    \"tce_div_match\": \"TCE Diversity match assessment.\",\n",
        "    \"cyto_score_detail\": \"Detailed cytogenetics classification for Disease Risk Index (DRI).\",\n",
        "    \"donor_age\": \"Age of the donor at the time of transplant.\",\n",
        "    \"donor_related\": \"Indicates whether the donor was related or unrelated to the patient.\",\n",
        "    \"mrd_hct\": \"Minimal Residual Disease (MRD) status at HCT.\",\n",
        "    \"gvhd_proph\": \"Graft-versus-Host Disease (GVHD) prophylaxis method used.\",\n",
        "    \"in_vivo_tcd\": \"In-Vivo T-Cell Depletion (TCD) method used, such as ATG or alemtuzumab.\"\n",
        "}\n",
        "\n",
        "# Update the description column with additional details\n",
        "dfDataDict[\"description\"] = dfDataDict[\"variable\"].map(additional_details).fillna(dfDataDict[\"description\"])\n",
        "dfDataDict.to_csv('DataDictionary.csv')\n",
        "# Display updated dataframe\n",
        "dfDataDict\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQQwxxipkp7b"
      },
      "source": [
        "#### Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "7QdV6tkDkuCO",
        "outputId": "4dc3449f-cc31-4e19-d7fa-3713811d4f07"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "dfTest"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-6f66e86f-2e78-4d8b-a81c-5b8b409cc0e7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>dri_score</th>\n",
              "      <th>psych_disturb</th>\n",
              "      <th>cyto_score</th>\n",
              "      <th>diabetes</th>\n",
              "      <th>hla_match_c_high</th>\n",
              "      <th>hla_high_res_8</th>\n",
              "      <th>tbi_status</th>\n",
              "      <th>arrhythmia</th>\n",
              "      <th>hla_low_res_6</th>\n",
              "      <th>...</th>\n",
              "      <th>karnofsky_score</th>\n",
              "      <th>hepatic_mild</th>\n",
              "      <th>tce_div_match</th>\n",
              "      <th>donor_related</th>\n",
              "      <th>melphalan_dose</th>\n",
              "      <th>hla_low_res_8</th>\n",
              "      <th>cardiac</th>\n",
              "      <th>hla_match_drb1_high</th>\n",
              "      <th>pulm_moderate</th>\n",
              "      <th>hla_low_res_10</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>28800</td>\n",
              "      <td>N/A - non-malignant indication</td>\n",
              "      <td>No</td>\n",
              "      <td>NaN</td>\n",
              "      <td>No</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>No TBI</td>\n",
              "      <td>No</td>\n",
              "      <td>6.0</td>\n",
              "      <td>...</td>\n",
              "      <td>90.0</td>\n",
              "      <td>No</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Unrelated</td>\n",
              "      <td>N/A, Mel not given</td>\n",
              "      <td>8.0</td>\n",
              "      <td>No</td>\n",
              "      <td>2.0</td>\n",
              "      <td>No</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>28801</td>\n",
              "      <td>Intermediate</td>\n",
              "      <td>No</td>\n",
              "      <td>Intermediate</td>\n",
              "      <td>No</td>\n",
              "      <td>2.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>TBI +- Other, &gt;cGy</td>\n",
              "      <td>No</td>\n",
              "      <td>6.0</td>\n",
              "      <td>...</td>\n",
              "      <td>90.0</td>\n",
              "      <td>No</td>\n",
              "      <td>Permissive mismatched</td>\n",
              "      <td>Related</td>\n",
              "      <td>N/A, Mel not given</td>\n",
              "      <td>8.0</td>\n",
              "      <td>No</td>\n",
              "      <td>2.0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>28802</td>\n",
              "      <td>N/A - non-malignant indication</td>\n",
              "      <td>No</td>\n",
              "      <td>NaN</td>\n",
              "      <td>No</td>\n",
              "      <td>2.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>No TBI</td>\n",
              "      <td>No</td>\n",
              "      <td>6.0</td>\n",
              "      <td>...</td>\n",
              "      <td>90.0</td>\n",
              "      <td>No</td>\n",
              "      <td>Permissive mismatched</td>\n",
              "      <td>Related</td>\n",
              "      <td>N/A, Mel not given</td>\n",
              "      <td>8.0</td>\n",
              "      <td>No</td>\n",
              "      <td>2.0</td>\n",
              "      <td>No</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 58 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6f66e86f-2e78-4d8b-a81c-5b8b409cc0e7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6f66e86f-2e78-4d8b-a81c-5b8b409cc0e7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6f66e86f-2e78-4d8b-a81c-5b8b409cc0e7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-54853726-bcee-47b1-92ff-605cbf426b33\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-54853726-bcee-47b1-92ff-605cbf426b33')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-54853726-bcee-47b1-92ff-605cbf426b33 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_3bad8cde-2ba0-47de-b577-96665db176a7\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('dfTest')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_3bad8cde-2ba0-47de-b577-96665db176a7 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('dfTest');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "      ID                       dri_score psych_disturb    cyto_score diabetes  \\\n",
              "0  28800  N/A - non-malignant indication            No           NaN       No   \n",
              "1  28801                    Intermediate            No  Intermediate       No   \n",
              "2  28802  N/A - non-malignant indication            No           NaN       No   \n",
              "\n",
              "   hla_match_c_high  hla_high_res_8          tbi_status arrhythmia  \\\n",
              "0               NaN             NaN              No TBI         No   \n",
              "1               2.0             8.0  TBI +- Other, >cGy         No   \n",
              "2               2.0             8.0              No TBI         No   \n",
              "\n",
              "   hla_low_res_6  ... karnofsky_score hepatic_mild          tce_div_match  \\\n",
              "0            6.0  ...            90.0           No                    NaN   \n",
              "1            6.0  ...            90.0           No  Permissive mismatched   \n",
              "2            6.0  ...            90.0           No  Permissive mismatched   \n",
              "\n",
              "  donor_related      melphalan_dose  hla_low_res_8 cardiac  \\\n",
              "0     Unrelated  N/A, Mel not given            8.0      No   \n",
              "1       Related  N/A, Mel not given            8.0      No   \n",
              "2       Related  N/A, Mel not given            8.0      No   \n",
              "\n",
              "   hla_match_drb1_high  pulm_moderate hla_low_res_10  \n",
              "0                  2.0             No           10.0  \n",
              "1                  2.0            Yes           10.0  \n",
              "2                  2.0             No           10.0  \n",
              "\n",
              "[3 rows x 58 columns]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dfTest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IOC1bfRkwsB"
      },
      "source": [
        "#### Train Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 721
        },
        "id": "Fa2oD1N7kyZE",
        "outputId": "687b45a4-58a1-4647-9431-ee46aafd1527"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "dfTrain"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-7e9d1b57-3e70-4886-a637-20e8acffb38b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>dri_score</th>\n",
              "      <th>psych_disturb</th>\n",
              "      <th>cyto_score</th>\n",
              "      <th>diabetes</th>\n",
              "      <th>hla_match_c_high</th>\n",
              "      <th>hla_high_res_8</th>\n",
              "      <th>tbi_status</th>\n",
              "      <th>arrhythmia</th>\n",
              "      <th>hla_low_res_6</th>\n",
              "      <th>...</th>\n",
              "      <th>tce_div_match</th>\n",
              "      <th>donor_related</th>\n",
              "      <th>melphalan_dose</th>\n",
              "      <th>hla_low_res_8</th>\n",
              "      <th>cardiac</th>\n",
              "      <th>hla_match_drb1_high</th>\n",
              "      <th>pulm_moderate</th>\n",
              "      <th>hla_low_res_10</th>\n",
              "      <th>efs</th>\n",
              "      <th>efs_time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>N/A - non-malignant indication</td>\n",
              "      <td>No</td>\n",
              "      <td>NaN</td>\n",
              "      <td>No</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>No TBI</td>\n",
              "      <td>No</td>\n",
              "      <td>6.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Unrelated</td>\n",
              "      <td>N/A, Mel not given</td>\n",
              "      <td>8.0</td>\n",
              "      <td>No</td>\n",
              "      <td>2.0</td>\n",
              "      <td>No</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>42.356</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Intermediate</td>\n",
              "      <td>No</td>\n",
              "      <td>Intermediate</td>\n",
              "      <td>No</td>\n",
              "      <td>2.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>TBI +- Other, &gt;cGy</td>\n",
              "      <td>No</td>\n",
              "      <td>6.0</td>\n",
              "      <td>...</td>\n",
              "      <td>Permissive mismatched</td>\n",
              "      <td>Related</td>\n",
              "      <td>N/A, Mel not given</td>\n",
              "      <td>8.0</td>\n",
              "      <td>No</td>\n",
              "      <td>2.0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>10.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>N/A - non-malignant indication</td>\n",
              "      <td>No</td>\n",
              "      <td>NaN</td>\n",
              "      <td>No</td>\n",
              "      <td>2.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>No TBI</td>\n",
              "      <td>No</td>\n",
              "      <td>6.0</td>\n",
              "      <td>...</td>\n",
              "      <td>Permissive mismatched</td>\n",
              "      <td>Related</td>\n",
              "      <td>N/A, Mel not given</td>\n",
              "      <td>8.0</td>\n",
              "      <td>No</td>\n",
              "      <td>2.0</td>\n",
              "      <td>No</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>19.793</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>High</td>\n",
              "      <td>No</td>\n",
              "      <td>Intermediate</td>\n",
              "      <td>No</td>\n",
              "      <td>2.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>No TBI</td>\n",
              "      <td>No</td>\n",
              "      <td>6.0</td>\n",
              "      <td>...</td>\n",
              "      <td>Permissive mismatched</td>\n",
              "      <td>Unrelated</td>\n",
              "      <td>N/A, Mel not given</td>\n",
              "      <td>8.0</td>\n",
              "      <td>No</td>\n",
              "      <td>2.0</td>\n",
              "      <td>No</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>102.349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>High</td>\n",
              "      <td>No</td>\n",
              "      <td>NaN</td>\n",
              "      <td>No</td>\n",
              "      <td>2.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>No TBI</td>\n",
              "      <td>No</td>\n",
              "      <td>6.0</td>\n",
              "      <td>...</td>\n",
              "      <td>Permissive mismatched</td>\n",
              "      <td>Related</td>\n",
              "      <td>MEL</td>\n",
              "      <td>8.0</td>\n",
              "      <td>No</td>\n",
              "      <td>2.0</td>\n",
              "      <td>No</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16.223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28795</th>\n",
              "      <td>28795</td>\n",
              "      <td>Intermediate - TED AML case &lt;missing cytogenetics</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Favorable</td>\n",
              "      <td>No</td>\n",
              "      <td>2.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>No TBI</td>\n",
              "      <td>No</td>\n",
              "      <td>6.0</td>\n",
              "      <td>...</td>\n",
              "      <td>Bi-directional non-permissive</td>\n",
              "      <td>NaN</td>\n",
              "      <td>N/A, Mel not given</td>\n",
              "      <td>8.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>No</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28796</th>\n",
              "      <td>28796</td>\n",
              "      <td>High</td>\n",
              "      <td>No</td>\n",
              "      <td>Poor</td>\n",
              "      <td>Yes</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>No TBI</td>\n",
              "      <td>No</td>\n",
              "      <td>5.0</td>\n",
              "      <td>...</td>\n",
              "      <td>GvH non-permissive</td>\n",
              "      <td>Related</td>\n",
              "      <td>N/A, Mel not given</td>\n",
              "      <td>6.0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>8.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.892</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28797</th>\n",
              "      <td>28797</td>\n",
              "      <td>TBD cytogenetics</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Poor</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>No TBI</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6.0</td>\n",
              "      <td>...</td>\n",
              "      <td>GvH non-permissive</td>\n",
              "      <td>Unrelated</td>\n",
              "      <td>N/A, Mel not given</td>\n",
              "      <td>8.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>No</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28798</th>\n",
              "      <td>28798</td>\n",
              "      <td>N/A - non-malignant indication</td>\n",
              "      <td>No</td>\n",
              "      <td>Poor</td>\n",
              "      <td>No</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>No TBI</td>\n",
              "      <td>No</td>\n",
              "      <td>3.0</td>\n",
              "      <td>...</td>\n",
              "      <td>Permissive mismatched</td>\n",
              "      <td>Related</td>\n",
              "      <td>MEL</td>\n",
              "      <td>4.0</td>\n",
              "      <td>No</td>\n",
              "      <td>1.0</td>\n",
              "      <td>No</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>52.351</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28799</th>\n",
              "      <td>28799</td>\n",
              "      <td>N/A - pediatric</td>\n",
              "      <td>No</td>\n",
              "      <td>NaN</td>\n",
              "      <td>No</td>\n",
              "      <td>2.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>No TBI</td>\n",
              "      <td>No</td>\n",
              "      <td>6.0</td>\n",
              "      <td>...</td>\n",
              "      <td>Permissive mismatched</td>\n",
              "      <td>Related</td>\n",
              "      <td>MEL</td>\n",
              "      <td>8.0</td>\n",
              "      <td>No</td>\n",
              "      <td>2.0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.158</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>28800 rows × 60 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7e9d1b57-3e70-4886-a637-20e8acffb38b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7e9d1b57-3e70-4886-a637-20e8acffb38b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7e9d1b57-3e70-4886-a637-20e8acffb38b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-8589b236-9a07-41c6-b8e8-8b0a0af8b66e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8589b236-9a07-41c6-b8e8-8b0a0af8b66e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-8589b236-9a07-41c6-b8e8-8b0a0af8b66e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_cc0de9de-5f16-4d41-bb11-ee65f8917fdb\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('dfTrain')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_cc0de9de-5f16-4d41-bb11-ee65f8917fdb button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('dfTrain');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "          ID                                          dri_score psych_disturb  \\\n",
              "0          0                     N/A - non-malignant indication            No   \n",
              "1          1                                       Intermediate            No   \n",
              "2          2                     N/A - non-malignant indication            No   \n",
              "3          3                                               High            No   \n",
              "4          4                                               High            No   \n",
              "...      ...                                                ...           ...   \n",
              "28795  28795  Intermediate - TED AML case <missing cytogenetics           NaN   \n",
              "28796  28796                                               High            No   \n",
              "28797  28797                                   TBD cytogenetics           NaN   \n",
              "28798  28798                     N/A - non-malignant indication            No   \n",
              "28799  28799                                    N/A - pediatric            No   \n",
              "\n",
              "         cyto_score diabetes  hla_match_c_high  hla_high_res_8  \\\n",
              "0               NaN       No               NaN             NaN   \n",
              "1      Intermediate       No               2.0             8.0   \n",
              "2               NaN       No               2.0             8.0   \n",
              "3      Intermediate       No               2.0             8.0   \n",
              "4               NaN       No               2.0             8.0   \n",
              "...             ...      ...               ...             ...   \n",
              "28795     Favorable       No               2.0             8.0   \n",
              "28796          Poor      Yes               1.0             4.0   \n",
              "28797          Poor      NaN               2.0             8.0   \n",
              "28798          Poor       No               1.0             4.0   \n",
              "28799           NaN       No               2.0             8.0   \n",
              "\n",
              "               tbi_status arrhythmia  hla_low_res_6  ...  \\\n",
              "0                  No TBI         No            6.0  ...   \n",
              "1      TBI +- Other, >cGy         No            6.0  ...   \n",
              "2                  No TBI         No            6.0  ...   \n",
              "3                  No TBI         No            6.0  ...   \n",
              "4                  No TBI         No            6.0  ...   \n",
              "...                   ...        ...            ...  ...   \n",
              "28795              No TBI         No            6.0  ...   \n",
              "28796              No TBI         No            5.0  ...   \n",
              "28797              No TBI        NaN            6.0  ...   \n",
              "28798              No TBI         No            3.0  ...   \n",
              "28799              No TBI         No            6.0  ...   \n",
              "\n",
              "                       tce_div_match donor_related      melphalan_dose  \\\n",
              "0                                NaN     Unrelated  N/A, Mel not given   \n",
              "1              Permissive mismatched       Related  N/A, Mel not given   \n",
              "2              Permissive mismatched       Related  N/A, Mel not given   \n",
              "3              Permissive mismatched     Unrelated  N/A, Mel not given   \n",
              "4              Permissive mismatched       Related                 MEL   \n",
              "...                              ...           ...                 ...   \n",
              "28795  Bi-directional non-permissive           NaN  N/A, Mel not given   \n",
              "28796             GvH non-permissive       Related  N/A, Mel not given   \n",
              "28797             GvH non-permissive     Unrelated  N/A, Mel not given   \n",
              "28798          Permissive mismatched       Related                 MEL   \n",
              "28799          Permissive mismatched       Related                 MEL   \n",
              "\n",
              "      hla_low_res_8 cardiac  hla_match_drb1_high pulm_moderate  \\\n",
              "0               8.0      No                  2.0            No   \n",
              "1               8.0      No                  2.0           Yes   \n",
              "2               8.0      No                  2.0            No   \n",
              "3               8.0      No                  2.0            No   \n",
              "4               8.0      No                  2.0            No   \n",
              "...             ...     ...                  ...           ...   \n",
              "28795           8.0     NaN                  2.0            No   \n",
              "28796           6.0     Yes                  1.0           Yes   \n",
              "28797           8.0     NaN                  2.0            No   \n",
              "28798           4.0      No                  1.0            No   \n",
              "28799           8.0      No                  2.0           Yes   \n",
              "\n",
              "       hla_low_res_10  efs efs_time  \n",
              "0                10.0  0.0   42.356  \n",
              "1                10.0  1.0    4.672  \n",
              "2                10.0  0.0   19.793  \n",
              "3                10.0  0.0  102.349  \n",
              "4                10.0  0.0   16.223  \n",
              "...               ...  ...      ...  \n",
              "28795            10.0  0.0   18.633  \n",
              "28796             8.0  1.0    4.892  \n",
              "28797            10.0  0.0   23.157  \n",
              "28798             5.0  0.0   52.351  \n",
              "28799            10.0  0.0   25.158  \n",
              "\n",
              "[28800 rows x 60 columns]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dfTrain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zI44Nc4k0SM"
      },
      "source": [
        "#### Sample Submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "L1Uq7pzFk4CQ",
        "outputId": "9b1d62a7-f0ec-4b85-f08a-1ae27fe2c14e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"dfSample\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"ID\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 28800,\n        \"max\": 28802,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          28800,\n          28801,\n          28802\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"prediction\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.5,\n        \"max\": 0.5,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "dfSample"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-43100cb4-aaca-4663-9ac5-95b9d6fd7d7b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>28800</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>28801</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>28802</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-43100cb4-aaca-4663-9ac5-95b9d6fd7d7b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-43100cb4-aaca-4663-9ac5-95b9d6fd7d7b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-43100cb4-aaca-4663-9ac5-95b9d6fd7d7b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ee369381-503b-4a78-b45e-a9bbf0cd4659\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ee369381-503b-4a78-b45e-a9bbf0cd4659')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ee369381-503b-4a78-b45e-a9bbf0cd4659 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_3ded63d2-3048-43e9-8f95-53495ea3f399\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('dfSample')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_3ded63d2-3048-43e9-8f95-53495ea3f399 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('dfSample');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "      ID  prediction\n",
              "0  28800         0.5\n",
              "1  28801         0.5\n",
              "2  28802         0.5"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dfSample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9vDRiiqKqHG"
      },
      "source": [
        "### Data Normalization\n",
        "Often when we load data into a NN we will have to normalize the data to some extent. In raw data this might mean changing the categorical values to numeric or setting the mean to 0 and applying a standard deviation to the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5g-kbza7K6Rm"
      },
      "outputs": [],
      "source": [
        "# Define columns\n",
        "categorical_columns = dfTrain.select_dtypes(include=['object']).columns.tolist()\n",
        "numerical_columns = dfTrain.select_dtypes(exclude=['object']).columns.tolist()\n",
        "\n",
        "# Remove ID and target columns from features\n",
        "columns_to_remove = ['ID', 'efs', 'efs_time']\n",
        "for col in columns_to_remove:\n",
        "    if col in numerical_columns:\n",
        "        numerical_columns.remove(col)\n",
        "\n",
        "# Define target column (focusing on efs_time prediction)\n",
        "target_column = 'efs_time'\n",
        "\n",
        "def convert_to_string(df, columns):\n",
        "    \"\"\"Convert specified columns to string type\"\"\"\n",
        "    df = df.copy()\n",
        "    for col in columns:\n",
        "        df[col] = df[col].astype(str)\n",
        "    return df\n",
        "\n",
        "def no_nulls(df, numerical, categorical):\n",
        "    \"\"\"Fill null values in the dataframe\"\"\"\n",
        "    df = df.copy()\n",
        "    for column in categorical:\n",
        "        df[column] = df[column].fillna('Unknown')\n",
        "    for column in numerical:\n",
        "        df[column] = df[column].fillna(df[numerical].mean()[column])\n",
        "    return df\n",
        "\n",
        "def preprocess_data(df, numerical_columns, categorical_columns, scaler=None, encoder=None, fit=False):\n",
        "    \"\"\"\n",
        "    Preprocess the data with option to fit or use existing transformers\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # Convert categorical columns to string type\n",
        "    df = convert_to_string(df, categorical_columns)\n",
        "\n",
        "    # Fill missing values\n",
        "    df = no_nulls(df, numerical_columns, categorical_columns)\n",
        "\n",
        "    # Scale numerical features\n",
        "    if numerical_columns:\n",
        "        if scaler is None and fit:\n",
        "            scaler = MinMaxScaler()\n",
        "            scaled_features = scaler.fit_transform(df[numerical_columns])\n",
        "        elif scaler is not None:\n",
        "            scaled_features = scaler.transform(df[numerical_columns])\n",
        "\n",
        "        df[numerical_columns] = scaled_features\n",
        "\n",
        "    # Encode categorical features\n",
        "    if categorical_columns:\n",
        "        if encoder is None and fit:\n",
        "            encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "            encoded_features = encoder.fit_transform(df[categorical_columns])\n",
        "        elif encoder is not None:\n",
        "            encoded_features = encoder.transform(df[categorical_columns])\n",
        "\n",
        "        encoded_df = pd.DataFrame(\n",
        "            encoded_features,\n",
        "            columns=encoder.get_feature_names_out(categorical_columns),\n",
        "            index=df.index\n",
        "        )\n",
        "\n",
        "        # Drop original categorical columns and join encoded features\n",
        "        df = pd.concat([df.drop(columns=categorical_columns), encoded_df], axis=1)\n",
        "\n",
        "    return df, scaler, encoder\n",
        "\n",
        "# Split the data\n",
        "y = dfTrain[target_column]\n",
        "X = dfTrain.drop(columns=[target_column, 'efs'])\n",
        "\n",
        "X_train_raw, X_val_raw, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Preprocess training data\n",
        "X_train, scaler, encoder = preprocess_data(\n",
        "    X_train_raw,\n",
        "    numerical_columns,\n",
        "    categorical_columns,\n",
        "    fit=True\n",
        ")\n",
        "\n",
        "# Preprocess validation data\n",
        "X_val, _, _ = preprocess_data(\n",
        "    X_val_raw,\n",
        "    numerical_columns,\n",
        "    categorical_columns,\n",
        "    scaler=scaler,\n",
        "    encoder=encoder\n",
        ")\n",
        "\n",
        "# Preprocess test data\n",
        "X_test, _, _ = preprocess_data(\n",
        "    dfTest,\n",
        "    numerical_columns,\n",
        "    categorical_columns,\n",
        "    scaler=scaler,\n",
        "    encoder=encoder,\n",
        "    fit = True\n",
        ")\n",
        "\n",
        "# Set series to dataframes\n",
        "y_val = pd.DataFrame(y_val)\n",
        "y_train = pd.DataFrame(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tT7uY1eB6NHF",
        "outputId": "2c59c653-e6ef-43e8-c5e7-4f466f20fbf0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index([], dtype='object')\n"
          ]
        }
      ],
      "source": [
        "#Check for mismatched columns\n",
        "namecompare = X_train.columns.difference(X_val.columns)\n",
        "print(namecompare)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0RArtGtPr1y",
        "outputId": "eb637df5-cc34-4ce6-c64b-334f746d9966"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NaN values in X_train: 0\n",
            "NaN values in y_train: 0\n",
            "ID\n",
            "hla_match_c_high\n",
            "hla_high_res_8\n",
            "hla_low_res_6\n",
            "hla_high_res_6\n",
            "hla_high_res_10\n",
            "hla_match_dqb1_high\n",
            "hla_nmdp_6\n",
            "hla_match_c_low\n",
            "hla_match_drb1_low\n",
            "hla_match_dqb1_low\n",
            "year_hct\n",
            "hla_match_a_high\n",
            "donor_age\n",
            "hla_match_b_low\n",
            "age_at_hct\n",
            "hla_match_a_low\n",
            "hla_match_b_high\n",
            "comorbidity_score\n",
            "karnofsky_score\n",
            "hla_low_res_8\n",
            "hla_match_drb1_high\n",
            "hla_low_res_10\n",
            "dri_score_High\n",
            "dri_score_High - TED AML case <missing cytogenetics\n",
            "dri_score_Intermediate\n",
            "dri_score_Intermediate - TED AML case <missing cytogenetics\n",
            "dri_score_Low\n",
            "dri_score_Missing disease status\n",
            "dri_score_N/A - disease not classifiable\n",
            "dri_score_N/A - non-malignant indication\n",
            "dri_score_N/A - pediatric\n",
            "dri_score_TBD cytogenetics\n",
            "dri_score_Very high\n",
            "dri_score_nan\n",
            "psych_disturb_No\n",
            "psych_disturb_Not done\n",
            "psych_disturb_Yes\n",
            "psych_disturb_nan\n",
            "cyto_score_Favorable\n",
            "cyto_score_Intermediate\n",
            "cyto_score_Normal\n",
            "cyto_score_Not tested\n",
            "cyto_score_Other\n",
            "cyto_score_Poor\n",
            "cyto_score_TBD\n",
            "cyto_score_nan\n",
            "diabetes_No\n",
            "diabetes_Not done\n",
            "diabetes_Yes\n",
            "diabetes_nan\n",
            "tbi_status_No TBI\n",
            "tbi_status_TBI + Cy +- Other\n",
            "tbi_status_TBI +- Other, -cGy, fractionated\n",
            "tbi_status_TBI +- Other, -cGy, single\n",
            "tbi_status_TBI +- Other, -cGy, unknown dose\n",
            "tbi_status_TBI +- Other, <=cGy\n",
            "tbi_status_TBI +- Other, >cGy\n",
            "tbi_status_TBI +- Other, unknown dose\n",
            "arrhythmia_No\n",
            "arrhythmia_Not done\n",
            "arrhythmia_Yes\n",
            "arrhythmia_nan\n",
            "graft_type_Bone marrow\n",
            "graft_type_Peripheral blood\n",
            "vent_hist_No\n",
            "vent_hist_Yes\n",
            "vent_hist_nan\n",
            "renal_issue_No\n",
            "renal_issue_Not done\n",
            "renal_issue_Yes\n",
            "renal_issue_nan\n",
            "pulm_severe_No\n",
            "pulm_severe_Not done\n",
            "pulm_severe_Yes\n",
            "pulm_severe_nan\n",
            "prim_disease_hct_AI\n",
            "prim_disease_hct_ALL\n",
            "prim_disease_hct_AML\n",
            "prim_disease_hct_CML\n",
            "prim_disease_hct_HD\n",
            "prim_disease_hct_HIS\n",
            "prim_disease_hct_IEA\n",
            "prim_disease_hct_IIS\n",
            "prim_disease_hct_IMD\n",
            "prim_disease_hct_IPA\n",
            "prim_disease_hct_MDS\n",
            "prim_disease_hct_MPN\n",
            "prim_disease_hct_NHL\n",
            "prim_disease_hct_Other acute leukemia\n",
            "prim_disease_hct_Other leukemia\n",
            "prim_disease_hct_PCD\n",
            "prim_disease_hct_SAA\n",
            "prim_disease_hct_Solid tumor\n",
            "cmv_status_+/+\n",
            "cmv_status_+/-\n",
            "cmv_status_-/+\n",
            "cmv_status_-/-\n",
            "cmv_status_nan\n",
            "tce_imm_match_G/B\n",
            "tce_imm_match_G/G\n",
            "tce_imm_match_H/B\n",
            "tce_imm_match_H/H\n",
            "tce_imm_match_P/B\n",
            "tce_imm_match_P/G\n",
            "tce_imm_match_P/H\n",
            "tce_imm_match_P/P\n",
            "tce_imm_match_nan\n",
            "rituximab_No\n",
            "rituximab_Yes\n",
            "rituximab_nan\n",
            "prod_type_BM\n",
            "prod_type_PB\n",
            "cyto_score_detail_Favorable\n",
            "cyto_score_detail_Intermediate\n",
            "cyto_score_detail_Not tested\n",
            "cyto_score_detail_Poor\n",
            "cyto_score_detail_TBD\n",
            "cyto_score_detail_nan\n",
            "conditioning_intensity_MAC\n",
            "conditioning_intensity_N/A, F(pre-TED) not submitted\n",
            "conditioning_intensity_NMA\n",
            "conditioning_intensity_No drugs reported\n",
            "conditioning_intensity_RIC\n",
            "conditioning_intensity_TBD\n",
            "conditioning_intensity_nan\n",
            "ethnicity_Hispanic or Latino\n",
            "ethnicity_Non-resident of the U.S.\n",
            "ethnicity_Not Hispanic or Latino\n",
            "ethnicity_nan\n",
            "obesity_No\n",
            "obesity_Not done\n",
            "obesity_Yes\n",
            "obesity_nan\n",
            "mrd_hct_Negative\n",
            "mrd_hct_Positive\n",
            "mrd_hct_nan\n",
            "in_vivo_tcd_No\n",
            "in_vivo_tcd_Yes\n",
            "in_vivo_tcd_nan\n",
            "tce_match_Fully matched\n",
            "tce_match_GvH non-permissive\n",
            "tce_match_HvG non-permissive\n",
            "tce_match_Permissive\n",
            "tce_match_nan\n",
            "hepatic_severe_No\n",
            "hepatic_severe_Not done\n",
            "hepatic_severe_Yes\n",
            "hepatic_severe_nan\n",
            "prior_tumor_No\n",
            "prior_tumor_Not done\n",
            "prior_tumor_Yes\n",
            "prior_tumor_nan\n",
            "peptic_ulcer_No\n",
            "peptic_ulcer_Not done\n",
            "peptic_ulcer_Yes\n",
            "peptic_ulcer_nan\n",
            "gvhd_proph_CDselect +- other\n",
            "gvhd_proph_CDselect alone\n",
            "gvhd_proph_CSA + MMF +- others(not FK)\n",
            "gvhd_proph_CSA + MTX +- others(not MMF,FK)\n",
            "gvhd_proph_CSA +- others(not FK,MMF,MTX)\n",
            "gvhd_proph_CSA alone\n",
            "gvhd_proph_Cyclophosphamide +- others\n",
            "gvhd_proph_Cyclophosphamide alone\n",
            "gvhd_proph_FK+ MMF +- others\n",
            "gvhd_proph_FK+ MTX +- others(not MMF)\n",
            "gvhd_proph_FK+- others(not MMF,MTX)\n",
            "gvhd_proph_FKalone\n",
            "gvhd_proph_No GvHD Prophylaxis\n",
            "gvhd_proph_Other GVHD Prophylaxis\n",
            "gvhd_proph_Parent Q = yes, but no agent\n",
            "gvhd_proph_TDEPLETION +- other\n",
            "gvhd_proph_TDEPLETION alone\n",
            "gvhd_proph_nan\n",
            "rheum_issue_No\n",
            "rheum_issue_Not done\n",
            "rheum_issue_Yes\n",
            "rheum_issue_nan\n",
            "sex_match_F-F\n",
            "sex_match_F-M\n",
            "sex_match_M-F\n",
            "sex_match_M-M\n",
            "sex_match_nan\n",
            "race_group_American Indian or Alaska Native\n",
            "race_group_Asian\n",
            "race_group_Black or African-American\n",
            "race_group_More than one race\n",
            "race_group_Native Hawaiian or other Pacific Islander\n",
            "race_group_White\n",
            "hepatic_mild_No\n",
            "hepatic_mild_Not done\n",
            "hepatic_mild_Yes\n",
            "hepatic_mild_nan\n",
            "tce_div_match_Bi-directional non-permissive\n",
            "tce_div_match_GvH non-permissive\n",
            "tce_div_match_HvG non-permissive\n",
            "tce_div_match_Permissive mismatched\n",
            "tce_div_match_nan\n",
            "donor_related_Multiple donor (non-UCB)\n",
            "donor_related_Related\n",
            "donor_related_Unrelated\n",
            "donor_related_nan\n",
            "melphalan_dose_MEL\n",
            "melphalan_dose_N/A, Mel not given\n",
            "melphalan_dose_nan\n",
            "cardiac_No\n",
            "cardiac_Not done\n",
            "cardiac_Yes\n",
            "cardiac_nan\n",
            "pulm_moderate_No\n",
            "pulm_moderate_Not done\n",
            "pulm_moderate_Yes\n",
            "pulm_moderate_nan\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 5760 entries, 18932 to 4564\n",
            "Columns: 214 entries, ID to pulm_moderate_nan\n",
            "dtypes: float64(213), int64(1)\n",
            "memory usage: 9.4 MB\n"
          ]
        }
      ],
      "source": [
        "# Ensure no NaN values remain in the data\n",
        "print(\"NaN values in X_train:\", X_train.isnull().sum().sum())\n",
        "print(\"NaN values in y_train:\", y_train.isnull().sum().sum())\n",
        "\n",
        "for x in X_train.isnull():\n",
        "  print(x)\n",
        "\n",
        "X_val.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7uOUmnKSEuW"
      },
      "source": [
        "#### Build Tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5yrr-JyVkBU",
        "outputId": "92a95f03-b232-4c31-99d8-6451f48c5a51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train shape: (23040, 214)\n",
            "X_val shape: (5760, 214)\n",
            "y_train shape: (23040, 1)\n",
            "y_val shape: (5760, 1)\n"
          ]
        }
      ],
      "source": [
        "#Check the shape of each dataset\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"X_val shape: {X_val.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"y_val shape: {y_val.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hiU7R1ASKxN"
      },
      "outputs": [],
      "source": [
        "# Convert to tensors\n",
        "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).reshape(-1, 1)\n",
        "X_val_tensor = torch.tensor(X_val.values, dtype=torch.float32)\n",
        "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).reshape(-1, 1)\n",
        "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNKZftq4R80p"
      },
      "source": [
        "## Model\n",
        "Here we train and test the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ql413fH6SNwK"
      },
      "source": [
        "### Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56qAtHGRSR28"
      },
      "outputs": [],
      "source": [
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.leaky_relu = nn.LeakyReLU()\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.leaky_relu(out)\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "# Define the model\n",
        "\n",
        "# Number of features\n",
        "input_size = X_train.shape[1]\n",
        "\n",
        "# Number of neurons in the hidden layer\n",
        "# Found using the original number training features (59) minus 3 (efs, efs_time & id)\n",
        "hidden_size = 56\n",
        "\n",
        "# Number of target variables (efs and efs_time)\n",
        "output_size = 1\n",
        "\n",
        "model = SimpleNN(input_size, hidden_size, output_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOFraQJLUJ2j"
      },
      "outputs": [],
      "source": [
        "# Define loss function and optimizer\n",
        "criterion = nn.MSELoss()  # Mean Squared Error for regression\n",
        "#We may want to try different types of loss\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adKFLbjsxh1V"
      },
      "outputs": [],
      "source": [
        "#CrossEntropyLoss with SGD\n",
        "#criterion = nn.CrossEntropyLoss()\n",
        "#optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGA7XDkiUO6R",
        "outputId": "3fb2bb67-c69f-4a96-e6ba-6c58e381b81c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Validation Loss: 547.9101\n",
            "Epoch [83340/100000]\n",
            "Training Loss: 439.9840\n",
            "Validation Loss: 550.4993\n",
            "Epoch [83350/100000]\n",
            "Training Loss: 445.3303\n",
            "Validation Loss: 543.8710\n",
            "Epoch [83360/100000]\n",
            "Training Loss: 439.1339\n",
            "Validation Loss: 549.9436\n",
            "Epoch [83370/100000]\n",
            "Training Loss: 444.2703\n",
            "Validation Loss: 550.3005\n",
            "Epoch [83380/100000]\n",
            "Training Loss: 434.7476\n",
            "Validation Loss: 550.7262\n",
            "Epoch [83390/100000]\n",
            "Training Loss: 440.2317\n",
            "Validation Loss: 549.9515\n",
            "Epoch [83400/100000]\n",
            "Training Loss: 446.0052\n",
            "Validation Loss: 547.3174\n",
            "Epoch [83410/100000]\n",
            "Training Loss: 438.1192\n",
            "Validation Loss: 549.8209\n",
            "Epoch [83420/100000]\n",
            "Training Loss: 438.2850\n",
            "Validation Loss: 549.8766\n",
            "Epoch [83430/100000]\n",
            "Training Loss: 442.4743\n",
            "Validation Loss: 551.2457\n",
            "Epoch [83440/100000]\n",
            "Training Loss: 437.7892\n",
            "Validation Loss: 548.7646\n",
            "Epoch [83450/100000]\n",
            "Training Loss: 442.7766\n",
            "Validation Loss: 545.4547\n",
            "Epoch [83460/100000]\n",
            "Training Loss: 440.8141\n",
            "Validation Loss: 547.1473\n",
            "Epoch [83470/100000]\n",
            "Training Loss: 439.4605\n",
            "Validation Loss: 546.6448\n",
            "Epoch [83480/100000]\n",
            "Training Loss: 449.0392\n",
            "Validation Loss: 545.9322\n",
            "Epoch [83490/100000]\n",
            "Training Loss: 446.1590\n",
            "Validation Loss: 553.1109\n",
            "Epoch [83500/100000]\n",
            "Training Loss: 449.9149\n",
            "Validation Loss: 544.5248\n",
            "Epoch [83510/100000]\n",
            "Training Loss: 447.4678\n",
            "Validation Loss: 549.0854\n",
            "Epoch [83520/100000]\n",
            "Training Loss: 446.1594\n",
            "Validation Loss: 551.1218\n",
            "Epoch [83530/100000]\n",
            "Training Loss: 444.0967\n",
            "Validation Loss: 549.9552\n",
            "Epoch [83540/100000]\n",
            "Training Loss: 443.4109\n",
            "Validation Loss: 548.3893\n",
            "Epoch [83550/100000]\n",
            "Training Loss: 439.4749\n",
            "Validation Loss: 548.4273\n",
            "Epoch [83560/100000]\n",
            "Training Loss: 436.7329\n",
            "Validation Loss: 548.4578\n",
            "Epoch [83570/100000]\n",
            "Training Loss: 441.4833\n",
            "Validation Loss: 550.0309\n",
            "Epoch [83580/100000]\n",
            "Training Loss: 444.0252\n",
            "Validation Loss: 547.5378\n",
            "Epoch [83590/100000]\n",
            "Training Loss: 442.0757\n",
            "Validation Loss: 547.9036\n",
            "Epoch [83600/100000]\n",
            "Training Loss: 444.5080\n",
            "Validation Loss: 547.8298\n",
            "Epoch [83610/100000]\n",
            "Training Loss: 444.1965\n",
            "Validation Loss: 548.6891\n",
            "Epoch [83620/100000]\n",
            "Training Loss: 445.6578\n",
            "Validation Loss: 547.3149\n",
            "Epoch [83630/100000]\n",
            "Training Loss: 444.3127\n",
            "Validation Loss: 545.0527\n",
            "Epoch [83640/100000]\n",
            "Training Loss: 450.5541\n",
            "Validation Loss: 547.7825\n",
            "Epoch [83650/100000]\n",
            "Training Loss: 448.6514\n",
            "Validation Loss: 542.7279\n",
            "Epoch [83660/100000]\n",
            "Training Loss: 442.7695\n",
            "Validation Loss: 551.5718\n",
            "Epoch [83670/100000]\n",
            "Training Loss: 443.9186\n",
            "Validation Loss: 548.2687\n",
            "Epoch [83680/100000]\n",
            "Training Loss: 439.0423\n",
            "Validation Loss: 548.3148\n",
            "Epoch [83690/100000]\n",
            "Training Loss: 438.0675\n",
            "Validation Loss: 549.2736\n",
            "Epoch [83700/100000]\n",
            "Training Loss: 441.2027\n",
            "Validation Loss: 549.1168\n",
            "Epoch [83710/100000]\n",
            "Training Loss: 450.4427\n",
            "Validation Loss: 549.2838\n",
            "Epoch [83720/100000]\n",
            "Training Loss: 449.9568\n",
            "Validation Loss: 543.6097\n",
            "Epoch [83730/100000]\n",
            "Training Loss: 441.4724\n",
            "Validation Loss: 549.1478\n",
            "Epoch [83740/100000]\n",
            "Training Loss: 440.1431\n",
            "Validation Loss: 549.2068\n",
            "Epoch [83750/100000]\n",
            "Training Loss: 439.1688\n",
            "Validation Loss: 549.3519\n",
            "Epoch [83760/100000]\n",
            "Training Loss: 440.6626\n",
            "Validation Loss: 546.0120\n",
            "Epoch [83770/100000]\n",
            "Training Loss: 439.7495\n",
            "Validation Loss: 548.0187\n",
            "Epoch [83780/100000]\n",
            "Training Loss: 446.8546\n",
            "Validation Loss: 548.4590\n",
            "Epoch [83790/100000]\n",
            "Training Loss: 442.9033\n",
            "Validation Loss: 548.5110\n",
            "Epoch [83800/100000]\n",
            "Training Loss: 438.6401\n",
            "Validation Loss: 552.2507\n",
            "Epoch [83810/100000]\n",
            "Training Loss: 445.4651\n",
            "Validation Loss: 546.8888\n",
            "Epoch [83820/100000]\n",
            "Training Loss: 442.8675\n",
            "Validation Loss: 548.3506\n",
            "Epoch [83830/100000]\n",
            "Training Loss: 437.5592\n",
            "Validation Loss: 550.5634\n",
            "Epoch [83840/100000]\n",
            "Training Loss: 441.1126\n",
            "Validation Loss: 550.2255\n",
            "Epoch [83850/100000]\n",
            "Training Loss: 446.6797\n",
            "Validation Loss: 548.8678\n",
            "Epoch [83860/100000]\n",
            "Training Loss: 438.9288\n",
            "Validation Loss: 548.8054\n",
            "Epoch [83870/100000]\n",
            "Training Loss: 442.0204\n",
            "Validation Loss: 548.9789\n",
            "Epoch [83880/100000]\n",
            "Training Loss: 436.8951\n",
            "Validation Loss: 544.8851\n",
            "Epoch [83890/100000]\n",
            "Training Loss: 442.4090\n",
            "Validation Loss: 551.3441\n",
            "Epoch [83900/100000]\n",
            "Training Loss: 444.6464\n",
            "Validation Loss: 545.6223\n",
            "Epoch [83910/100000]\n",
            "Training Loss: 441.8695\n",
            "Validation Loss: 548.4006\n",
            "Epoch [83920/100000]\n",
            "Training Loss: 437.4390\n",
            "Validation Loss: 547.5234\n",
            "Epoch [83930/100000]\n",
            "Training Loss: 438.3817\n",
            "Validation Loss: 549.0896\n",
            "Epoch [83940/100000]\n",
            "Training Loss: 441.3793\n",
            "Validation Loss: 545.3635\n",
            "Epoch [83950/100000]\n",
            "Training Loss: 440.2728\n",
            "Validation Loss: 548.5743\n",
            "Epoch [83960/100000]\n",
            "Training Loss: 440.3249\n",
            "Validation Loss: 550.4596\n",
            "Epoch [83970/100000]\n",
            "Training Loss: 443.2046\n",
            "Validation Loss: 547.9659\n",
            "Epoch [83980/100000]\n",
            "Training Loss: 442.2681\n",
            "Validation Loss: 546.4615\n",
            "Epoch [83990/100000]\n",
            "Training Loss: 443.7487\n",
            "Validation Loss: 549.8704\n",
            "Epoch [84000/100000]\n",
            "Training Loss: 440.8410\n",
            "Validation Loss: 550.3882\n",
            "Epoch [84010/100000]\n",
            "Training Loss: 440.9076\n",
            "Validation Loss: 546.8547\n",
            "Epoch [84020/100000]\n",
            "Training Loss: 439.6964\n",
            "Validation Loss: 547.8229\n",
            "Epoch [84030/100000]\n",
            "Training Loss: 438.9263\n",
            "Validation Loss: 546.7223\n",
            "Epoch [84040/100000]\n",
            "Training Loss: 442.1769\n",
            "Validation Loss: 547.8917\n",
            "Epoch [84050/100000]\n",
            "Training Loss: 445.7990\n",
            "Validation Loss: 548.2041\n",
            "Epoch [84060/100000]\n",
            "Training Loss: 441.0809\n",
            "Validation Loss: 546.6196\n",
            "Epoch [84070/100000]\n",
            "Training Loss: 447.5154\n",
            "Validation Loss: 550.6309\n",
            "Epoch [84080/100000]\n",
            "Training Loss: 446.4693\n",
            "Validation Loss: 546.0443\n",
            "Epoch [84090/100000]\n",
            "Training Loss: 444.2028\n",
            "Validation Loss: 545.4896\n",
            "Epoch [84100/100000]\n",
            "Training Loss: 442.1800\n",
            "Validation Loss: 548.9527\n",
            "Epoch [84110/100000]\n",
            "Training Loss: 444.8977\n",
            "Validation Loss: 544.8662\n",
            "Epoch [84120/100000]\n",
            "Training Loss: 444.5822\n",
            "Validation Loss: 549.2301\n",
            "Epoch [84130/100000]\n",
            "Training Loss: 446.3992\n",
            "Validation Loss: 544.9677\n",
            "Epoch [84140/100000]\n",
            "Training Loss: 442.6534\n",
            "Validation Loss: 546.5999\n",
            "Epoch [84150/100000]\n",
            "Training Loss: 444.6664\n",
            "Validation Loss: 546.0364\n",
            "Epoch [84160/100000]\n",
            "Training Loss: 444.4553\n",
            "Validation Loss: 547.7249\n",
            "Epoch [84170/100000]\n",
            "Training Loss: 438.7353\n",
            "Validation Loss: 548.7147\n",
            "Epoch [84180/100000]\n",
            "Training Loss: 442.8583\n",
            "Validation Loss: 546.0590\n",
            "Epoch [84190/100000]\n",
            "Training Loss: 442.2421\n",
            "Validation Loss: 547.1386\n",
            "Epoch [84200/100000]\n",
            "Training Loss: 439.3680\n",
            "Validation Loss: 547.8903\n",
            "Epoch [84210/100000]\n",
            "Training Loss: 442.0592\n",
            "Validation Loss: 550.6398\n",
            "Epoch [84220/100000]\n",
            "Training Loss: 439.3102\n",
            "Validation Loss: 548.4894\n",
            "Epoch [84230/100000]\n",
            "Training Loss: 447.1083\n",
            "Validation Loss: 547.4202\n",
            "Epoch [84240/100000]\n",
            "Training Loss: 435.5273\n",
            "Validation Loss: 550.3393\n",
            "Epoch [84250/100000]\n",
            "Training Loss: 444.2961\n",
            "Validation Loss: 547.1601\n",
            "Epoch [84260/100000]\n",
            "Training Loss: 444.2781\n",
            "Validation Loss: 546.6048\n",
            "Epoch [84270/100000]\n",
            "Training Loss: 439.4671\n",
            "Validation Loss: 549.8021\n",
            "Epoch [84280/100000]\n",
            "Training Loss: 439.8508\n",
            "Validation Loss: 546.9944\n",
            "Epoch [84290/100000]\n",
            "Training Loss: 447.1129\n",
            "Validation Loss: 546.3634\n",
            "Epoch [84300/100000]\n",
            "Training Loss: 438.0894\n",
            "Validation Loss: 549.3713\n",
            "Epoch [84310/100000]\n",
            "Training Loss: 441.8519\n",
            "Validation Loss: 549.2388\n",
            "Epoch [84320/100000]\n",
            "Training Loss: 439.6101\n",
            "Validation Loss: 549.1817\n",
            "Epoch [84330/100000]\n",
            "Training Loss: 443.1554\n",
            "Validation Loss: 547.9700\n",
            "Epoch [84340/100000]\n",
            "Training Loss: 443.0579\n",
            "Validation Loss: 547.7008\n",
            "Epoch [84350/100000]\n",
            "Training Loss: 439.8473\n",
            "Validation Loss: 551.1138\n",
            "Epoch [84360/100000]\n",
            "Training Loss: 443.5496\n",
            "Validation Loss: 545.3076\n",
            "Epoch [84370/100000]\n",
            "Training Loss: 443.7924\n",
            "Validation Loss: 546.3509\n",
            "Epoch [84380/100000]\n",
            "Training Loss: 443.1241\n",
            "Validation Loss: 548.2468\n",
            "Epoch [84390/100000]\n",
            "Training Loss: 438.4619\n",
            "Validation Loss: 550.5287\n",
            "Epoch [84400/100000]\n",
            "Training Loss: 437.4704\n",
            "Validation Loss: 549.4011\n",
            "Epoch [84410/100000]\n",
            "Training Loss: 441.5275\n",
            "Validation Loss: 549.4004\n",
            "Epoch [84420/100000]\n",
            "Training Loss: 444.0244\n",
            "Validation Loss: 544.1774\n",
            "Epoch [84430/100000]\n",
            "Training Loss: 443.8499\n",
            "Validation Loss: 546.0078\n",
            "Epoch [84440/100000]\n",
            "Training Loss: 439.6018\n",
            "Validation Loss: 546.7420\n",
            "Epoch [84450/100000]\n",
            "Training Loss: 442.7114\n",
            "Validation Loss: 551.3128\n",
            "Epoch [84460/100000]\n",
            "Training Loss: 446.1861\n",
            "Validation Loss: 544.0890\n",
            "Epoch [84470/100000]\n",
            "Training Loss: 446.4294\n",
            "Validation Loss: 548.1990\n",
            "Epoch [84480/100000]\n",
            "Training Loss: 443.5204\n",
            "Validation Loss: 549.2361\n",
            "Epoch [84490/100000]\n",
            "Training Loss: 440.8618\n",
            "Validation Loss: 549.4183\n",
            "Epoch [84500/100000]\n",
            "Training Loss: 441.4960\n",
            "Validation Loss: 549.8091\n",
            "Epoch [84510/100000]\n",
            "Training Loss: 441.7353\n",
            "Validation Loss: 544.2109\n",
            "Epoch [84520/100000]\n",
            "Training Loss: 444.3246\n",
            "Validation Loss: 547.7875\n",
            "Epoch [84530/100000]\n",
            "Training Loss: 441.3465\n",
            "Validation Loss: 549.7444\n",
            "Epoch [84540/100000]\n",
            "Training Loss: 449.2133\n",
            "Validation Loss: 541.7104\n",
            "Epoch [84550/100000]\n",
            "Training Loss: 440.1497\n",
            "Validation Loss: 547.0300\n",
            "Epoch [84560/100000]\n",
            "Training Loss: 445.1267\n",
            "Validation Loss: 548.7209\n",
            "Epoch [84570/100000]\n",
            "Training Loss: 437.7090\n",
            "Validation Loss: 545.8116\n",
            "Epoch [84580/100000]\n",
            "Training Loss: 441.9664\n",
            "Validation Loss: 548.1326\n",
            "Epoch [84590/100000]\n",
            "Training Loss: 440.8569\n",
            "Validation Loss: 546.0493\n",
            "Epoch [84600/100000]\n",
            "Training Loss: 437.0635\n",
            "Validation Loss: 547.4644\n",
            "Epoch [84610/100000]\n",
            "Training Loss: 440.5162\n",
            "Validation Loss: 547.6088\n",
            "Epoch [84620/100000]\n",
            "Training Loss: 439.7381\n",
            "Validation Loss: 546.4842\n",
            "Epoch [84630/100000]\n",
            "Training Loss: 438.5331\n",
            "Validation Loss: 552.2498\n",
            "Epoch [84640/100000]\n",
            "Training Loss: 439.0818\n",
            "Validation Loss: 551.0865\n",
            "Epoch [84650/100000]\n",
            "Training Loss: 442.7197\n",
            "Validation Loss: 548.8244\n",
            "Epoch [84660/100000]\n",
            "Training Loss: 438.4707\n",
            "Validation Loss: 547.6560\n",
            "Epoch [84670/100000]\n",
            "Training Loss: 438.4145\n",
            "Validation Loss: 547.7041\n",
            "Epoch [84680/100000]\n",
            "Training Loss: 445.2340\n",
            "Validation Loss: 549.8182\n",
            "Epoch [84690/100000]\n",
            "Training Loss: 442.6939\n",
            "Validation Loss: 548.8419\n",
            "Epoch [84700/100000]\n",
            "Training Loss: 438.2778\n",
            "Validation Loss: 548.7175\n",
            "Epoch [84710/100000]\n",
            "Training Loss: 439.1354\n",
            "Validation Loss: 552.3735\n",
            "Epoch [84720/100000]\n",
            "Training Loss: 440.1933\n",
            "Validation Loss: 546.6375\n",
            "Epoch [84730/100000]\n",
            "Training Loss: 433.8162\n",
            "Validation Loss: 552.9960\n",
            "Epoch [84740/100000]\n",
            "Training Loss: 438.8335\n",
            "Validation Loss: 548.5508\n",
            "Epoch [84750/100000]\n",
            "Training Loss: 442.7969\n",
            "Validation Loss: 547.8635\n",
            "Epoch [84760/100000]\n",
            "Training Loss: 446.5220\n",
            "Validation Loss: 549.8982\n",
            "Epoch [84770/100000]\n",
            "Training Loss: 445.8374\n",
            "Validation Loss: 551.2358\n",
            "Epoch [84780/100000]\n",
            "Training Loss: 443.9951\n",
            "Validation Loss: 549.5101\n",
            "Epoch [84790/100000]\n",
            "Training Loss: 440.2191\n",
            "Validation Loss: 548.1545\n",
            "Epoch [84800/100000]\n",
            "Training Loss: 442.4355\n",
            "Validation Loss: 553.3782\n",
            "Epoch [84810/100000]\n",
            "Training Loss: 437.5750\n",
            "Validation Loss: 547.2394\n",
            "Epoch [84820/100000]\n",
            "Training Loss: 443.2550\n",
            "Validation Loss: 551.8361\n",
            "Epoch [84830/100000]\n",
            "Training Loss: 442.4482\n",
            "Validation Loss: 546.7996\n",
            "Epoch [84840/100000]\n",
            "Training Loss: 441.1531\n",
            "Validation Loss: 546.9846\n",
            "Epoch [84850/100000]\n",
            "Training Loss: 437.1331\n",
            "Validation Loss: 548.2867\n",
            "Epoch [84860/100000]\n",
            "Training Loss: 446.1837\n",
            "Validation Loss: 543.3114\n",
            "Epoch [84870/100000]\n",
            "Training Loss: 438.5162\n",
            "Validation Loss: 550.4543\n",
            "Epoch [84880/100000]\n",
            "Training Loss: 448.2062\n",
            "Validation Loss: 546.7823\n",
            "Epoch [84890/100000]\n",
            "Training Loss: 437.6274\n",
            "Validation Loss: 550.6431\n",
            "Epoch [84900/100000]\n",
            "Training Loss: 442.1503\n",
            "Validation Loss: 547.1240\n",
            "Epoch [84910/100000]\n",
            "Training Loss: 441.2286\n",
            "Validation Loss: 548.8898\n",
            "Epoch [84920/100000]\n",
            "Training Loss: 438.5883\n",
            "Validation Loss: 548.7802\n",
            "Epoch [84930/100000]\n",
            "Training Loss: 440.7958\n",
            "Validation Loss: 548.0809\n",
            "Epoch [84940/100000]\n",
            "Training Loss: 441.5962\n",
            "Validation Loss: 546.6597\n",
            "Epoch [84950/100000]\n",
            "Training Loss: 439.3053\n",
            "Validation Loss: 547.7158\n",
            "Epoch [84960/100000]\n",
            "Training Loss: 440.3509\n",
            "Validation Loss: 547.4495\n",
            "Epoch [84970/100000]\n",
            "Training Loss: 441.5239\n",
            "Validation Loss: 549.0886\n",
            "Epoch [84980/100000]\n",
            "Training Loss: 443.2460\n",
            "Validation Loss: 550.7487\n",
            "Epoch [84990/100000]\n",
            "Training Loss: 442.4912\n",
            "Validation Loss: 547.5198\n",
            "Epoch [85000/100000]\n",
            "Training Loss: 437.8940\n",
            "Validation Loss: 549.5742\n",
            "Epoch [85010/100000]\n",
            "Training Loss: 442.6626\n",
            "Validation Loss: 547.0781\n",
            "Epoch [85020/100000]\n",
            "Training Loss: 443.3815\n",
            "Validation Loss: 550.5034\n",
            "Epoch [85030/100000]\n",
            "Training Loss: 440.0884\n",
            "Validation Loss: 549.0405\n",
            "Epoch [85040/100000]\n",
            "Training Loss: 437.4904\n",
            "Validation Loss: 548.8370\n",
            "Epoch [85050/100000]\n",
            "Training Loss: 443.1429\n",
            "Validation Loss: 548.0828\n",
            "Epoch [85060/100000]\n",
            "Training Loss: 433.4065\n",
            "Validation Loss: 553.1821\n",
            "Epoch [85070/100000]\n",
            "Training Loss: 441.9576\n",
            "Validation Loss: 549.9341\n",
            "Epoch [85080/100000]\n",
            "Training Loss: 442.4040\n",
            "Validation Loss: 549.2895\n",
            "Epoch [85090/100000]\n",
            "Training Loss: 435.7708\n",
            "Validation Loss: 549.6582\n",
            "Epoch [85100/100000]\n",
            "Training Loss: 438.8014\n",
            "Validation Loss: 550.9180\n",
            "Epoch [85110/100000]\n",
            "Training Loss: 435.4401\n",
            "Validation Loss: 549.2522\n",
            "Epoch [85120/100000]\n",
            "Training Loss: 434.0385\n",
            "Validation Loss: 551.4485\n",
            "Epoch [85130/100000]\n",
            "Training Loss: 437.5541\n",
            "Validation Loss: 551.9545\n",
            "Epoch [85140/100000]\n",
            "Training Loss: 443.6234\n",
            "Validation Loss: 547.9106\n",
            "Epoch [85150/100000]\n",
            "Training Loss: 441.6720\n",
            "Validation Loss: 545.6264\n",
            "Epoch [85160/100000]\n",
            "Training Loss: 438.0650\n",
            "Validation Loss: 548.1644\n",
            "Epoch [85170/100000]\n",
            "Training Loss: 437.0944\n",
            "Validation Loss: 549.6044\n",
            "Epoch [85180/100000]\n",
            "Training Loss: 438.4781\n",
            "Validation Loss: 553.4685\n",
            "Epoch [85190/100000]\n",
            "Training Loss: 440.6425\n",
            "Validation Loss: 551.5408\n",
            "Epoch [85200/100000]\n",
            "Training Loss: 437.3214\n",
            "Validation Loss: 548.0195\n",
            "Epoch [85210/100000]\n",
            "Training Loss: 440.0315\n",
            "Validation Loss: 547.9880\n",
            "Epoch [85220/100000]\n",
            "Training Loss: 439.3871\n",
            "Validation Loss: 548.5443\n",
            "Epoch [85230/100000]\n",
            "Training Loss: 445.8363\n",
            "Validation Loss: 548.3481\n",
            "Epoch [85240/100000]\n",
            "Training Loss: 441.0677\n",
            "Validation Loss: 550.3917\n",
            "Epoch [85250/100000]\n",
            "Training Loss: 440.6073\n",
            "Validation Loss: 547.5870\n",
            "Epoch [85260/100000]\n",
            "Training Loss: 440.6403\n",
            "Validation Loss: 544.1649\n",
            "Epoch [85270/100000]\n",
            "Training Loss: 440.5797\n",
            "Validation Loss: 547.2543\n",
            "Epoch [85280/100000]\n",
            "Training Loss: 446.4735\n",
            "Validation Loss: 549.2372\n",
            "Epoch [85290/100000]\n",
            "Training Loss: 436.5545\n",
            "Validation Loss: 548.7656\n",
            "Epoch [85300/100000]\n",
            "Training Loss: 437.5101\n",
            "Validation Loss: 552.0909\n",
            "Epoch [85310/100000]\n",
            "Training Loss: 444.1118\n",
            "Validation Loss: 548.8073\n",
            "Epoch [85320/100000]\n",
            "Training Loss: 440.4002\n",
            "Validation Loss: 551.6326\n",
            "Epoch [85330/100000]\n",
            "Training Loss: 438.7007\n",
            "Validation Loss: 548.8957\n",
            "Epoch [85340/100000]\n",
            "Training Loss: 441.7295\n",
            "Validation Loss: 552.5157\n",
            "Epoch [85350/100000]\n",
            "Training Loss: 437.6841\n",
            "Validation Loss: 551.3920\n",
            "Epoch [85360/100000]\n",
            "Training Loss: 443.3753\n",
            "Validation Loss: 550.1826\n",
            "Epoch [85370/100000]\n",
            "Training Loss: 439.3988\n",
            "Validation Loss: 547.5476\n",
            "Epoch [85380/100000]\n",
            "Training Loss: 439.9455\n",
            "Validation Loss: 548.3088\n",
            "Epoch [85390/100000]\n",
            "Training Loss: 438.8838\n",
            "Validation Loss: 549.5994\n",
            "Epoch [85400/100000]\n",
            "Training Loss: 437.2970\n",
            "Validation Loss: 551.0549\n",
            "Epoch [85410/100000]\n",
            "Training Loss: 441.7686\n",
            "Validation Loss: 549.6835\n",
            "Epoch [85420/100000]\n",
            "Training Loss: 440.7047\n",
            "Validation Loss: 549.9573\n",
            "Epoch [85430/100000]\n",
            "Training Loss: 436.5969\n",
            "Validation Loss: 546.8012\n",
            "Epoch [85440/100000]\n",
            "Training Loss: 440.8971\n",
            "Validation Loss: 554.1465\n",
            "Epoch [85450/100000]\n",
            "Training Loss: 445.5545\n",
            "Validation Loss: 548.5349\n",
            "Epoch [85460/100000]\n",
            "Training Loss: 441.5913\n",
            "Validation Loss: 550.6135\n",
            "Epoch [85470/100000]\n",
            "Training Loss: 439.2682\n",
            "Validation Loss: 547.8364\n",
            "Epoch [85480/100000]\n",
            "Training Loss: 443.5490\n",
            "Validation Loss: 544.8519\n",
            "Epoch [85490/100000]\n",
            "Training Loss: 439.0809\n",
            "Validation Loss: 548.7659\n",
            "Epoch [85500/100000]\n",
            "Training Loss: 440.3176\n",
            "Validation Loss: 549.6081\n",
            "Epoch [85510/100000]\n",
            "Training Loss: 436.1013\n",
            "Validation Loss: 547.1882\n",
            "Epoch [85520/100000]\n",
            "Training Loss: 436.6866\n",
            "Validation Loss: 547.6832\n",
            "Epoch [85530/100000]\n",
            "Training Loss: 440.2042\n",
            "Validation Loss: 549.7335\n",
            "Epoch [85540/100000]\n",
            "Training Loss: 436.9975\n",
            "Validation Loss: 550.4858\n",
            "Epoch [85550/100000]\n",
            "Training Loss: 440.3831\n",
            "Validation Loss: 548.1931\n",
            "Epoch [85560/100000]\n",
            "Training Loss: 437.6747\n",
            "Validation Loss: 550.0229\n",
            "Epoch [85570/100000]\n",
            "Training Loss: 443.3114\n",
            "Validation Loss: 546.8425\n",
            "Epoch [85580/100000]\n",
            "Training Loss: 439.0725\n",
            "Validation Loss: 546.7363\n",
            "Epoch [85590/100000]\n",
            "Training Loss: 444.9336\n",
            "Validation Loss: 548.6322\n",
            "Epoch [85600/100000]\n",
            "Training Loss: 434.9684\n",
            "Validation Loss: 551.7699\n",
            "Epoch [85610/100000]\n",
            "Training Loss: 439.7248\n",
            "Validation Loss: 551.5323\n",
            "Epoch [85620/100000]\n",
            "Training Loss: 439.4587\n",
            "Validation Loss: 546.8946\n",
            "Epoch [85630/100000]\n",
            "Training Loss: 435.8357\n",
            "Validation Loss: 551.3677\n",
            "Epoch [85640/100000]\n",
            "Training Loss: 439.8092\n",
            "Validation Loss: 548.6962\n",
            "Epoch [85650/100000]\n",
            "Training Loss: 437.7466\n",
            "Validation Loss: 547.9143\n",
            "Epoch [85660/100000]\n",
            "Training Loss: 435.6018\n",
            "Validation Loss: 549.2361\n",
            "Epoch [85670/100000]\n",
            "Training Loss: 435.1269\n",
            "Validation Loss: 552.5898\n",
            "Epoch [85680/100000]\n",
            "Training Loss: 434.7230\n",
            "Validation Loss: 549.9012\n",
            "Epoch [85690/100000]\n",
            "Training Loss: 437.2149\n",
            "Validation Loss: 550.4795\n",
            "Epoch [85700/100000]\n",
            "Training Loss: 441.6389\n",
            "Validation Loss: 551.5615\n",
            "Epoch [85710/100000]\n",
            "Training Loss: 438.8641\n",
            "Validation Loss: 551.2609\n",
            "Epoch [85720/100000]\n",
            "Training Loss: 440.0729\n",
            "Validation Loss: 549.2390\n",
            "Epoch [85730/100000]\n",
            "Training Loss: 437.5664\n",
            "Validation Loss: 547.6231\n",
            "Epoch [85740/100000]\n",
            "Training Loss: 431.3716\n",
            "Validation Loss: 552.4619\n",
            "Epoch [85750/100000]\n",
            "Training Loss: 436.2145\n",
            "Validation Loss: 549.5171\n",
            "Epoch [85760/100000]\n",
            "Training Loss: 440.3679\n",
            "Validation Loss: 547.3890\n",
            "Epoch [85770/100000]\n",
            "Training Loss: 439.1984\n",
            "Validation Loss: 545.8624\n",
            "Epoch [85780/100000]\n",
            "Training Loss: 447.2708\n",
            "Validation Loss: 550.0153\n",
            "Epoch [85790/100000]\n",
            "Training Loss: 434.9284\n",
            "Validation Loss: 550.6294\n",
            "Epoch [85800/100000]\n",
            "Training Loss: 440.4362\n",
            "Validation Loss: 548.3399\n",
            "Epoch [85810/100000]\n",
            "Training Loss: 440.1913\n",
            "Validation Loss: 547.2993\n",
            "Epoch [85820/100000]\n",
            "Training Loss: 440.8169\n",
            "Validation Loss: 547.2479\n",
            "Epoch [85830/100000]\n",
            "Training Loss: 442.2718\n",
            "Validation Loss: 549.0536\n",
            "Epoch [85840/100000]\n",
            "Training Loss: 437.6186\n",
            "Validation Loss: 548.5941\n",
            "Epoch [85850/100000]\n",
            "Training Loss: 435.8460\n",
            "Validation Loss: 549.0977\n",
            "Epoch [85860/100000]\n",
            "Training Loss: 438.0447\n",
            "Validation Loss: 545.9056\n",
            "Epoch [85870/100000]\n",
            "Training Loss: 433.1765\n",
            "Validation Loss: 548.6306\n",
            "Epoch [85880/100000]\n",
            "Training Loss: 438.2941\n",
            "Validation Loss: 550.6345\n",
            "Epoch [85890/100000]\n",
            "Training Loss: 438.1737\n",
            "Validation Loss: 550.5660\n",
            "Epoch [85900/100000]\n",
            "Training Loss: 438.7566\n",
            "Validation Loss: 547.0695\n",
            "Epoch [85910/100000]\n",
            "Training Loss: 440.5168\n",
            "Validation Loss: 548.7051\n",
            "Epoch [85920/100000]\n",
            "Training Loss: 444.8165\n",
            "Validation Loss: 549.5901\n",
            "Epoch [85930/100000]\n",
            "Training Loss: 443.2593\n",
            "Validation Loss: 545.4387\n",
            "Epoch [85940/100000]\n",
            "Training Loss: 439.9857\n",
            "Validation Loss: 547.4803\n",
            "Epoch [85950/100000]\n",
            "Training Loss: 439.9057\n",
            "Validation Loss: 547.1273\n",
            "Epoch [85960/100000]\n",
            "Training Loss: 441.3566\n",
            "Validation Loss: 550.0287\n",
            "Epoch [85970/100000]\n",
            "Training Loss: 445.2306\n",
            "Validation Loss: 550.2851\n",
            "Epoch [85980/100000]\n",
            "Training Loss: 445.9739\n",
            "Validation Loss: 549.7421\n",
            "Epoch [85990/100000]\n",
            "Training Loss: 443.8990\n",
            "Validation Loss: 550.9350\n",
            "Epoch [86000/100000]\n",
            "Training Loss: 439.1176\n",
            "Validation Loss: 547.5124\n",
            "Epoch [86010/100000]\n",
            "Training Loss: 438.9868\n",
            "Validation Loss: 548.8468\n",
            "Epoch [86020/100000]\n",
            "Training Loss: 438.1064\n",
            "Validation Loss: 549.5065\n",
            "Epoch [86030/100000]\n",
            "Training Loss: 444.1931\n",
            "Validation Loss: 547.4854\n",
            "Epoch [86040/100000]\n",
            "Training Loss: 437.9646\n",
            "Validation Loss: 550.8457\n",
            "Epoch [86050/100000]\n",
            "Training Loss: 434.7064\n",
            "Validation Loss: 549.8110\n",
            "Epoch [86060/100000]\n",
            "Training Loss: 442.9203\n",
            "Validation Loss: 548.8637\n",
            "Epoch [86070/100000]\n",
            "Training Loss: 442.3529\n",
            "Validation Loss: 548.8223\n",
            "Epoch [86080/100000]\n",
            "Training Loss: 439.6235\n",
            "Validation Loss: 547.3291\n",
            "Epoch [86090/100000]\n",
            "Training Loss: 438.3369\n",
            "Validation Loss: 547.5392\n",
            "Epoch [86100/100000]\n",
            "Training Loss: 439.2446\n",
            "Validation Loss: 547.7384\n",
            "Epoch [86110/100000]\n",
            "Training Loss: 442.0229\n",
            "Validation Loss: 549.7992\n",
            "Epoch [86120/100000]\n",
            "Training Loss: 436.9170\n",
            "Validation Loss: 551.2411\n",
            "Epoch [86130/100000]\n",
            "Training Loss: 433.2977\n",
            "Validation Loss: 549.9041\n",
            "Epoch [86140/100000]\n",
            "Training Loss: 438.0383\n",
            "Validation Loss: 552.4302\n",
            "Epoch [86150/100000]\n",
            "Training Loss: 434.1768\n",
            "Validation Loss: 550.5992\n",
            "Epoch [86160/100000]\n",
            "Training Loss: 443.9716\n",
            "Validation Loss: 551.4141\n",
            "Epoch [86170/100000]\n",
            "Training Loss: 434.3750\n",
            "Validation Loss: 551.1415\n",
            "Epoch [86180/100000]\n",
            "Training Loss: 436.9424\n",
            "Validation Loss: 551.0070\n",
            "Epoch [86190/100000]\n",
            "Training Loss: 442.7078\n",
            "Validation Loss: 544.1396\n",
            "Epoch [86200/100000]\n",
            "Training Loss: 440.8133\n",
            "Validation Loss: 548.1716\n",
            "Epoch [86210/100000]\n",
            "Training Loss: 440.4936\n",
            "Validation Loss: 547.8928\n",
            "Epoch [86220/100000]\n",
            "Training Loss: 438.5706\n",
            "Validation Loss: 547.3105\n",
            "Epoch [86230/100000]\n",
            "Training Loss: 442.1939\n",
            "Validation Loss: 549.8254\n",
            "Epoch [86240/100000]\n",
            "Training Loss: 440.4364\n",
            "Validation Loss: 550.3657\n",
            "Epoch [86250/100000]\n",
            "Training Loss: 438.8712\n",
            "Validation Loss: 548.4703\n",
            "Epoch [86260/100000]\n",
            "Training Loss: 444.6397\n",
            "Validation Loss: 548.1907\n",
            "Epoch [86270/100000]\n",
            "Training Loss: 436.4979\n",
            "Validation Loss: 547.8605\n",
            "Epoch [86280/100000]\n",
            "Training Loss: 442.2731\n",
            "Validation Loss: 549.1716\n",
            "Epoch [86290/100000]\n",
            "Training Loss: 432.7485\n",
            "Validation Loss: 548.0500\n",
            "Epoch [86300/100000]\n",
            "Training Loss: 440.1722\n",
            "Validation Loss: 550.9828\n",
            "Epoch [86310/100000]\n",
            "Training Loss: 435.7411\n",
            "Validation Loss: 551.3568\n",
            "Epoch [86320/100000]\n",
            "Training Loss: 438.1211\n",
            "Validation Loss: 543.5366\n",
            "Epoch [86330/100000]\n",
            "Training Loss: 443.2576\n",
            "Validation Loss: 553.6802\n",
            "Epoch [86340/100000]\n",
            "Training Loss: 444.5868\n",
            "Validation Loss: 547.1985\n",
            "Epoch [86350/100000]\n",
            "Training Loss: 438.7101\n",
            "Validation Loss: 545.4423\n",
            "Epoch [86360/100000]\n",
            "Training Loss: 440.0321\n",
            "Validation Loss: 545.6993\n",
            "Epoch [86370/100000]\n",
            "Training Loss: 439.5092\n",
            "Validation Loss: 549.4155\n",
            "Epoch [86380/100000]\n",
            "Training Loss: 436.1236\n",
            "Validation Loss: 549.4070\n",
            "Epoch [86390/100000]\n",
            "Training Loss: 436.1964\n",
            "Validation Loss: 550.3586\n",
            "Epoch [86400/100000]\n",
            "Training Loss: 440.3784\n",
            "Validation Loss: 548.0433\n",
            "Epoch [86410/100000]\n",
            "Training Loss: 438.0417\n",
            "Validation Loss: 550.1219\n",
            "Epoch [86420/100000]\n",
            "Training Loss: 439.1928\n",
            "Validation Loss: 550.6726\n",
            "Epoch [86430/100000]\n",
            "Training Loss: 435.0668\n",
            "Validation Loss: 550.7667\n",
            "Epoch [86440/100000]\n",
            "Training Loss: 439.0396\n",
            "Validation Loss: 550.7574\n",
            "Epoch [86450/100000]\n",
            "Training Loss: 436.8756\n",
            "Validation Loss: 551.8529\n",
            "Epoch [86460/100000]\n",
            "Training Loss: 434.4187\n",
            "Validation Loss: 549.7463\n",
            "Epoch [86470/100000]\n",
            "Training Loss: 436.3846\n",
            "Validation Loss: 550.8923\n",
            "Epoch [86480/100000]\n",
            "Training Loss: 434.7457\n",
            "Validation Loss: 551.3295\n",
            "Epoch [86490/100000]\n",
            "Training Loss: 439.1228\n",
            "Validation Loss: 549.5531\n",
            "Epoch [86500/100000]\n",
            "Training Loss: 440.0651\n",
            "Validation Loss: 552.7883\n",
            "Epoch [86510/100000]\n",
            "Training Loss: 440.6680\n",
            "Validation Loss: 552.0950\n",
            "Epoch [86520/100000]\n",
            "Training Loss: 435.8308\n",
            "Validation Loss: 552.9221\n",
            "Epoch [86530/100000]\n",
            "Training Loss: 439.3929\n",
            "Validation Loss: 549.1385\n",
            "Epoch [86540/100000]\n",
            "Training Loss: 438.8815\n",
            "Validation Loss: 546.7840\n",
            "Epoch [86550/100000]\n",
            "Training Loss: 439.0056\n",
            "Validation Loss: 548.3033\n",
            "Epoch [86560/100000]\n",
            "Training Loss: 437.2693\n",
            "Validation Loss: 553.4462\n",
            "Epoch [86570/100000]\n",
            "Training Loss: 438.5355\n",
            "Validation Loss: 550.1331\n",
            "Epoch [86580/100000]\n",
            "Training Loss: 444.3175\n",
            "Validation Loss: 548.7730\n",
            "Epoch [86590/100000]\n",
            "Training Loss: 441.9852\n",
            "Validation Loss: 549.7715\n",
            "Epoch [86600/100000]\n",
            "Training Loss: 438.3379\n",
            "Validation Loss: 550.2955\n",
            "Epoch [86610/100000]\n",
            "Training Loss: 440.7244\n",
            "Validation Loss: 548.5055\n",
            "Epoch [86620/100000]\n",
            "Training Loss: 448.1075\n",
            "Validation Loss: 549.0937\n",
            "Epoch [86630/100000]\n",
            "Training Loss: 448.9998\n",
            "Validation Loss: 543.3138\n",
            "Epoch [86640/100000]\n",
            "Training Loss: 446.8269\n",
            "Validation Loss: 544.0134\n",
            "Epoch [86650/100000]\n",
            "Training Loss: 441.0171\n",
            "Validation Loss: 547.3787\n",
            "Epoch [86660/100000]\n",
            "Training Loss: 443.9695\n",
            "Validation Loss: 549.1454\n",
            "Epoch [86670/100000]\n",
            "Training Loss: 440.1755\n",
            "Validation Loss: 547.4595\n",
            "Epoch [86680/100000]\n",
            "Training Loss: 443.0749\n",
            "Validation Loss: 548.9807\n",
            "Epoch [86690/100000]\n",
            "Training Loss: 439.0272\n",
            "Validation Loss: 552.6671\n",
            "Epoch [86700/100000]\n",
            "Training Loss: 439.2964\n",
            "Validation Loss: 551.4337\n",
            "Epoch [86710/100000]\n",
            "Training Loss: 432.5053\n",
            "Validation Loss: 552.2175\n",
            "Epoch [86720/100000]\n",
            "Training Loss: 436.8557\n",
            "Validation Loss: 553.8755\n",
            "Epoch [86730/100000]\n",
            "Training Loss: 441.2041\n",
            "Validation Loss: 552.7908\n",
            "Epoch [86740/100000]\n",
            "Training Loss: 442.8024\n",
            "Validation Loss: 550.8969\n",
            "Epoch [86750/100000]\n",
            "Training Loss: 440.4224\n",
            "Validation Loss: 550.8271\n",
            "Epoch [86760/100000]\n",
            "Training Loss: 438.7012\n",
            "Validation Loss: 550.1189\n",
            "Epoch [86770/100000]\n",
            "Training Loss: 438.1701\n",
            "Validation Loss: 550.4818\n",
            "Epoch [86780/100000]\n",
            "Training Loss: 438.1359\n",
            "Validation Loss: 550.8935\n",
            "Epoch [86790/100000]\n",
            "Training Loss: 437.4174\n",
            "Validation Loss: 552.5563\n",
            "Epoch [86800/100000]\n",
            "Training Loss: 432.9973\n",
            "Validation Loss: 547.4633\n",
            "Epoch [86810/100000]\n",
            "Training Loss: 436.1924\n",
            "Validation Loss: 548.1035\n",
            "Epoch [86820/100000]\n",
            "Training Loss: 441.7392\n",
            "Validation Loss: 549.6562\n",
            "Epoch [86830/100000]\n",
            "Training Loss: 440.7050\n",
            "Validation Loss: 547.4662\n",
            "Epoch [86840/100000]\n",
            "Training Loss: 439.9948\n",
            "Validation Loss: 550.5494\n",
            "Epoch [86850/100000]\n",
            "Training Loss: 441.4485\n",
            "Validation Loss: 551.2175\n",
            "Epoch [86860/100000]\n",
            "Training Loss: 436.5685\n",
            "Validation Loss: 547.4866\n",
            "Epoch [86870/100000]\n",
            "Training Loss: 439.7217\n",
            "Validation Loss: 548.8367\n",
            "Epoch [86880/100000]\n",
            "Training Loss: 447.7723\n",
            "Validation Loss: 545.2652\n",
            "Epoch [86890/100000]\n",
            "Training Loss: 448.4218\n",
            "Validation Loss: 547.1082\n",
            "Epoch [86900/100000]\n",
            "Training Loss: 495.7812\n",
            "Validation Loss: 552.7570\n",
            "Epoch [86910/100000]\n",
            "Training Loss: 715.2843\n",
            "Validation Loss: 1455.0332\n",
            "Epoch [86920/100000]\n",
            "Training Loss: 884.8129\n",
            "Validation Loss: 587.0755\n",
            "Epoch [86930/100000]\n",
            "Training Loss: 997.7764\n",
            "Validation Loss: 619.5172\n",
            "Epoch [86940/100000]\n",
            "Training Loss: 746.1256\n",
            "Validation Loss: 578.8463\n",
            "Epoch [86950/100000]\n",
            "Training Loss: 742.1929\n",
            "Validation Loss: 588.4341\n",
            "Epoch [86960/100000]\n",
            "Training Loss: 744.8859\n",
            "Validation Loss: 600.0668\n",
            "Epoch [86970/100000]\n",
            "Training Loss: 756.2524\n",
            "Validation Loss: 583.0963\n",
            "Epoch [86980/100000]\n",
            "Training Loss: 718.5450\n",
            "Validation Loss: 589.9195\n",
            "Epoch [86990/100000]\n",
            "Training Loss: 695.5012\n",
            "Validation Loss: 583.9732\n",
            "Epoch [87000/100000]\n",
            "Training Loss: 686.6816\n",
            "Validation Loss: 588.6927\n",
            "Epoch [87010/100000]\n",
            "Training Loss: 664.8365\n",
            "Validation Loss: 584.7403\n",
            "Epoch [87020/100000]\n",
            "Training Loss: 654.4918\n",
            "Validation Loss: 582.9379\n",
            "Epoch [87030/100000]\n",
            "Training Loss: 636.4562\n",
            "Validation Loss: 584.4991\n",
            "Epoch [87040/100000]\n",
            "Training Loss: 630.7505\n",
            "Validation Loss: 576.5352\n",
            "Epoch [87050/100000]\n",
            "Training Loss: 622.2519\n",
            "Validation Loss: 592.5411\n",
            "Epoch [87060/100000]\n",
            "Training Loss: 607.8307\n",
            "Validation Loss: 572.4084\n",
            "Epoch [87070/100000]\n",
            "Training Loss: 606.2299\n",
            "Validation Loss: 579.8080\n",
            "Epoch [87080/100000]\n",
            "Training Loss: 596.7260\n",
            "Validation Loss: 574.8358\n",
            "Epoch [87090/100000]\n",
            "Training Loss: 586.8606\n",
            "Validation Loss: 578.3386\n",
            "Epoch [87100/100000]\n",
            "Training Loss: 580.6075\n",
            "Validation Loss: 573.6802\n",
            "Epoch [87110/100000]\n",
            "Training Loss: 574.0193\n",
            "Validation Loss: 570.2544\n",
            "Epoch [87120/100000]\n",
            "Training Loss: 566.8491\n",
            "Validation Loss: 570.9997\n",
            "Epoch [87130/100000]\n",
            "Training Loss: 550.2993\n",
            "Validation Loss: 565.4846\n",
            "Epoch [87140/100000]\n",
            "Training Loss: 553.3262\n",
            "Validation Loss: 570.2842\n",
            "Epoch [87150/100000]\n",
            "Training Loss: 545.4029\n",
            "Validation Loss: 564.8448\n",
            "Epoch [87160/100000]\n",
            "Training Loss: 533.8722\n",
            "Validation Loss: 562.4930\n",
            "Epoch [87170/100000]\n",
            "Training Loss: 531.7705\n",
            "Validation Loss: 564.5410\n",
            "Epoch [87180/100000]\n",
            "Training Loss: 520.0257\n",
            "Validation Loss: 562.4630\n",
            "Epoch [87190/100000]\n",
            "Training Loss: 525.6708\n",
            "Validation Loss: 563.5507\n",
            "Epoch [87200/100000]\n",
            "Training Loss: 513.9750\n",
            "Validation Loss: 565.2253\n",
            "Epoch [87210/100000]\n",
            "Training Loss: 519.2885\n",
            "Validation Loss: 563.6993\n",
            "Epoch [87220/100000]\n",
            "Training Loss: 515.4364\n",
            "Validation Loss: 562.0483\n",
            "Epoch [87230/100000]\n",
            "Training Loss: 510.1032\n",
            "Validation Loss: 563.4805\n",
            "Epoch [87240/100000]\n",
            "Training Loss: 515.0061\n",
            "Validation Loss: 563.8682\n",
            "Epoch [87250/100000]\n",
            "Training Loss: 513.0783\n",
            "Validation Loss: 561.8540\n",
            "Epoch [87260/100000]\n",
            "Training Loss: 510.0764\n",
            "Validation Loss: 562.1650\n",
            "Epoch [87270/100000]\n",
            "Training Loss: 510.4774\n",
            "Validation Loss: 562.9350\n",
            "Epoch [87280/100000]\n",
            "Training Loss: 505.9173\n",
            "Validation Loss: 561.9119\n",
            "Epoch [87290/100000]\n",
            "Training Loss: 509.8265\n",
            "Validation Loss: 562.8049\n",
            "Epoch [87300/100000]\n",
            "Training Loss: 505.4995\n",
            "Validation Loss: 560.8697\n",
            "Epoch [87310/100000]\n",
            "Training Loss: 508.1479\n",
            "Validation Loss: 561.3621\n",
            "Epoch [87320/100000]\n",
            "Training Loss: 493.0628\n",
            "Validation Loss: 558.2886\n",
            "Epoch [87330/100000]\n",
            "Training Loss: 500.0305\n",
            "Validation Loss: 557.7517\n",
            "Epoch [87340/100000]\n",
            "Training Loss: 495.8623\n",
            "Validation Loss: 560.3928\n",
            "Epoch [87350/100000]\n",
            "Training Loss: 491.7038\n",
            "Validation Loss: 558.9636\n",
            "Epoch [87360/100000]\n",
            "Training Loss: 489.7162\n",
            "Validation Loss: 559.0267\n",
            "Epoch [87370/100000]\n",
            "Training Loss: 488.5691\n",
            "Validation Loss: 559.4139\n",
            "Epoch [87380/100000]\n",
            "Training Loss: 491.0038\n",
            "Validation Loss: 559.9382\n",
            "Epoch [87390/100000]\n",
            "Training Loss: 489.6706\n",
            "Validation Loss: 559.9322\n",
            "Epoch [87400/100000]\n",
            "Training Loss: 489.0914\n",
            "Validation Loss: 561.1561\n",
            "Epoch [87410/100000]\n",
            "Training Loss: 489.5540\n",
            "Validation Loss: 559.7426\n",
            "Epoch [87420/100000]\n",
            "Training Loss: 488.4878\n",
            "Validation Loss: 560.3992\n",
            "Epoch [87430/100000]\n",
            "Training Loss: 491.4707\n",
            "Validation Loss: 560.9073\n",
            "Epoch [87440/100000]\n",
            "Training Loss: 488.3823\n",
            "Validation Loss: 559.6617\n",
            "Epoch [87450/100000]\n",
            "Training Loss: 492.6301\n",
            "Validation Loss: 559.6371\n",
            "Epoch [87460/100000]\n",
            "Training Loss: 491.8736\n",
            "Validation Loss: 559.4042\n",
            "Epoch [87470/100000]\n",
            "Training Loss: 488.3578\n",
            "Validation Loss: 558.4753\n",
            "Epoch [87480/100000]\n",
            "Training Loss: 485.7043\n",
            "Validation Loss: 559.0410\n",
            "Epoch [87490/100000]\n",
            "Training Loss: 486.0892\n",
            "Validation Loss: 558.6565\n",
            "Epoch [87500/100000]\n",
            "Training Loss: 490.4276\n",
            "Validation Loss: 557.0724\n",
            "Epoch [87510/100000]\n",
            "Training Loss: 489.0219\n",
            "Validation Loss: 556.4459\n",
            "Epoch [87520/100000]\n",
            "Training Loss: 489.2910\n",
            "Validation Loss: 557.4531\n",
            "Epoch [87530/100000]\n",
            "Training Loss: 487.2808\n",
            "Validation Loss: 555.0501\n",
            "Epoch [87540/100000]\n",
            "Training Loss: 482.0117\n",
            "Validation Loss: 559.6180\n",
            "Epoch [87550/100000]\n",
            "Training Loss: 488.5182\n",
            "Validation Loss: 558.6903\n",
            "Epoch [87560/100000]\n",
            "Training Loss: 486.5118\n",
            "Validation Loss: 561.7882\n",
            "Epoch [87570/100000]\n",
            "Training Loss: 479.0000\n",
            "Validation Loss: 559.6219\n",
            "Epoch [87580/100000]\n",
            "Training Loss: 485.2554\n",
            "Validation Loss: 561.1135\n",
            "Epoch [87590/100000]\n",
            "Training Loss: 484.3263\n",
            "Validation Loss: 562.3942\n",
            "Epoch [87600/100000]\n",
            "Training Loss: 478.6698\n",
            "Validation Loss: 559.2833\n",
            "Epoch [87610/100000]\n",
            "Training Loss: 482.3143\n",
            "Validation Loss: 557.5670\n",
            "Epoch [87620/100000]\n",
            "Training Loss: 478.0547\n",
            "Validation Loss: 561.0038\n",
            "Epoch [87630/100000]\n",
            "Training Loss: 475.8848\n",
            "Validation Loss: 558.4523\n",
            "Epoch [87640/100000]\n",
            "Training Loss: 475.6716\n",
            "Validation Loss: 555.5980\n",
            "Epoch [87650/100000]\n",
            "Training Loss: 470.9331\n",
            "Validation Loss: 561.9758\n",
            "Epoch [87660/100000]\n",
            "Training Loss: 476.9802\n",
            "Validation Loss: 554.8814\n",
            "Epoch [87670/100000]\n",
            "Training Loss: 465.6849\n",
            "Validation Loss: 558.3181\n",
            "Epoch [87680/100000]\n",
            "Training Loss: 475.3997\n",
            "Validation Loss: 560.3332\n",
            "Epoch [87690/100000]\n",
            "Training Loss: 477.2392\n",
            "Validation Loss: 558.6578\n",
            "Epoch [87700/100000]\n",
            "Training Loss: 471.7516\n",
            "Validation Loss: 557.4887\n",
            "Epoch [87710/100000]\n",
            "Training Loss: 473.1232\n",
            "Validation Loss: 557.4930\n",
            "Epoch [87720/100000]\n",
            "Training Loss: 473.5920\n",
            "Validation Loss: 556.6382\n",
            "Epoch [87730/100000]\n",
            "Training Loss: 467.2573\n",
            "Validation Loss: 557.9507\n",
            "Epoch [87740/100000]\n",
            "Training Loss: 469.7266\n",
            "Validation Loss: 555.4709\n",
            "Epoch [87750/100000]\n",
            "Training Loss: 465.8772\n",
            "Validation Loss: 556.6292\n",
            "Epoch [87760/100000]\n",
            "Training Loss: 467.3414\n",
            "Validation Loss: 554.7385\n",
            "Epoch [87770/100000]\n",
            "Training Loss: 474.4334\n",
            "Validation Loss: 556.0134\n",
            "Epoch [87780/100000]\n",
            "Training Loss: 463.4658\n",
            "Validation Loss: 558.7853\n",
            "Epoch [87790/100000]\n",
            "Training Loss: 462.1933\n",
            "Validation Loss: 554.9847\n",
            "Epoch [87800/100000]\n",
            "Training Loss: 466.6401\n",
            "Validation Loss: 553.4654\n",
            "Epoch [87810/100000]\n",
            "Training Loss: 463.6187\n",
            "Validation Loss: 555.3807\n",
            "Epoch [87820/100000]\n",
            "Training Loss: 465.8189\n",
            "Validation Loss: 555.8288\n",
            "Epoch [87830/100000]\n",
            "Training Loss: 471.7513\n",
            "Validation Loss: 555.6279\n",
            "Epoch [87840/100000]\n",
            "Training Loss: 461.3784\n",
            "Validation Loss: 559.6590\n",
            "Epoch [87850/100000]\n",
            "Training Loss: 465.3587\n",
            "Validation Loss: 553.8477\n",
            "Epoch [87860/100000]\n",
            "Training Loss: 466.6789\n",
            "Validation Loss: 558.7383\n",
            "Epoch [87870/100000]\n",
            "Training Loss: 473.8167\n",
            "Validation Loss: 551.5902\n",
            "Epoch [87880/100000]\n",
            "Training Loss: 468.8016\n",
            "Validation Loss: 556.4232\n",
            "Epoch [87890/100000]\n",
            "Training Loss: 468.3864\n",
            "Validation Loss: 555.8841\n",
            "Epoch [87900/100000]\n",
            "Training Loss: 463.2123\n",
            "Validation Loss: 558.0696\n",
            "Epoch [87910/100000]\n",
            "Training Loss: 466.6936\n",
            "Validation Loss: 554.2897\n",
            "Epoch [87920/100000]\n",
            "Training Loss: 461.5141\n",
            "Validation Loss: 555.2479\n",
            "Epoch [87930/100000]\n",
            "Training Loss: 460.7197\n",
            "Validation Loss: 558.0472\n",
            "Epoch [87940/100000]\n",
            "Training Loss: 457.8625\n",
            "Validation Loss: 553.8567\n",
            "Epoch [87950/100000]\n",
            "Training Loss: 460.5132\n",
            "Validation Loss: 556.0311\n",
            "Epoch [87960/100000]\n",
            "Training Loss: 455.9434\n",
            "Validation Loss: 554.8620\n",
            "Epoch [87970/100000]\n",
            "Training Loss: 457.4835\n",
            "Validation Loss: 556.1299\n",
            "Epoch [87980/100000]\n",
            "Training Loss: 460.2492\n",
            "Validation Loss: 556.0054\n",
            "Epoch [87990/100000]\n",
            "Training Loss: 459.3080\n",
            "Validation Loss: 557.4248\n",
            "Epoch [88000/100000]\n",
            "Training Loss: 454.4999\n",
            "Validation Loss: 555.0378\n",
            "Epoch [88010/100000]\n",
            "Training Loss: 463.1077\n",
            "Validation Loss: 552.3903\n",
            "Epoch [88020/100000]\n",
            "Training Loss: 457.8321\n",
            "Validation Loss: 556.7878\n",
            "Epoch [88030/100000]\n",
            "Training Loss: 459.6386\n",
            "Validation Loss: 557.6353\n",
            "Epoch [88040/100000]\n",
            "Training Loss: 453.0335\n",
            "Validation Loss: 555.8929\n",
            "Epoch [88050/100000]\n",
            "Training Loss: 457.2615\n",
            "Validation Loss: 556.8671\n",
            "Epoch [88060/100000]\n",
            "Training Loss: 460.8224\n",
            "Validation Loss: 554.3505\n",
            "Epoch [88070/100000]\n",
            "Training Loss: 457.9605\n",
            "Validation Loss: 553.2393\n",
            "Epoch [88080/100000]\n",
            "Training Loss: 456.3126\n",
            "Validation Loss: 553.6268\n",
            "Epoch [88090/100000]\n",
            "Training Loss: 458.1136\n",
            "Validation Loss: 554.9086\n",
            "Epoch [88100/100000]\n",
            "Training Loss: 455.1030\n",
            "Validation Loss: 551.7198\n",
            "Epoch [88110/100000]\n",
            "Training Loss: 451.0912\n",
            "Validation Loss: 551.0387\n",
            "Epoch [88120/100000]\n",
            "Training Loss: 453.7115\n",
            "Validation Loss: 556.1920\n",
            "Epoch [88130/100000]\n",
            "Training Loss: 453.5541\n",
            "Validation Loss: 552.8763\n",
            "Epoch [88140/100000]\n",
            "Training Loss: 447.8994\n",
            "Validation Loss: 556.6777\n",
            "Epoch [88150/100000]\n",
            "Training Loss: 451.1673\n",
            "Validation Loss: 552.7555\n",
            "Epoch [88160/100000]\n",
            "Training Loss: 448.3595\n",
            "Validation Loss: 554.8669\n",
            "Epoch [88170/100000]\n",
            "Training Loss: 450.6496\n",
            "Validation Loss: 552.7944\n",
            "Epoch [88180/100000]\n",
            "Training Loss: 453.7924\n",
            "Validation Loss: 554.8042\n",
            "Epoch [88190/100000]\n",
            "Training Loss: 454.4468\n",
            "Validation Loss: 555.7128\n",
            "Epoch [88200/100000]\n",
            "Training Loss: 448.3833\n",
            "Validation Loss: 553.8541\n",
            "Epoch [88210/100000]\n",
            "Training Loss: 447.4568\n",
            "Validation Loss: 554.5590\n",
            "Epoch [88220/100000]\n",
            "Training Loss: 455.4498\n",
            "Validation Loss: 554.1026\n",
            "Epoch [88230/100000]\n",
            "Training Loss: 453.6657\n",
            "Validation Loss: 554.4857\n",
            "Epoch [88240/100000]\n",
            "Training Loss: 449.3693\n",
            "Validation Loss: 552.1708\n",
            "Epoch [88250/100000]\n",
            "Training Loss: 451.1180\n",
            "Validation Loss: 553.8796\n",
            "Epoch [88260/100000]\n",
            "Training Loss: 447.4180\n",
            "Validation Loss: 553.4954\n",
            "Epoch [88270/100000]\n",
            "Training Loss: 449.4891\n",
            "Validation Loss: 553.5641\n",
            "Epoch [88280/100000]\n",
            "Training Loss: 446.8537\n",
            "Validation Loss: 554.6804\n",
            "Epoch [88290/100000]\n",
            "Training Loss: 449.8091\n",
            "Validation Loss: 550.0081\n",
            "Epoch [88300/100000]\n",
            "Training Loss: 455.4460\n",
            "Validation Loss: 545.7862\n",
            "Epoch [88310/100000]\n",
            "Training Loss: 459.3991\n",
            "Validation Loss: 552.3906\n",
            "Epoch [88320/100000]\n",
            "Training Loss: 451.8098\n",
            "Validation Loss: 549.0023\n",
            "Epoch [88330/100000]\n",
            "Training Loss: 460.4860\n",
            "Validation Loss: 552.0491\n",
            "Epoch [88340/100000]\n",
            "Training Loss: 455.6019\n",
            "Validation Loss: 552.3929\n",
            "Epoch [88350/100000]\n",
            "Training Loss: 450.1459\n",
            "Validation Loss: 552.7699\n",
            "Epoch [88360/100000]\n",
            "Training Loss: 443.5708\n",
            "Validation Loss: 553.6448\n",
            "Epoch [88370/100000]\n",
            "Training Loss: 447.2978\n",
            "Validation Loss: 555.4369\n",
            "Epoch [88380/100000]\n",
            "Training Loss: 450.7101\n",
            "Validation Loss: 555.4021\n",
            "Epoch [88390/100000]\n",
            "Training Loss: 451.0899\n",
            "Validation Loss: 551.9437\n",
            "Epoch [88400/100000]\n",
            "Training Loss: 451.3880\n",
            "Validation Loss: 552.2869\n",
            "Epoch [88410/100000]\n",
            "Training Loss: 448.9398\n",
            "Validation Loss: 552.4083\n",
            "Epoch [88420/100000]\n",
            "Training Loss: 444.1468\n",
            "Validation Loss: 553.5264\n",
            "Epoch [88430/100000]\n",
            "Training Loss: 445.6214\n",
            "Validation Loss: 552.8214\n",
            "Epoch [88440/100000]\n",
            "Training Loss: 448.6227\n",
            "Validation Loss: 558.9443\n",
            "Epoch [88450/100000]\n",
            "Training Loss: 454.3985\n",
            "Validation Loss: 554.3553\n",
            "Epoch [88460/100000]\n",
            "Training Loss: 449.2606\n",
            "Validation Loss: 552.5812\n",
            "Epoch [88470/100000]\n",
            "Training Loss: 454.5785\n",
            "Validation Loss: 553.9739\n",
            "Epoch [88480/100000]\n",
            "Training Loss: 449.7153\n",
            "Validation Loss: 553.7064\n",
            "Epoch [88490/100000]\n",
            "Training Loss: 451.1609\n",
            "Validation Loss: 554.4841\n",
            "Epoch [88500/100000]\n",
            "Training Loss: 445.5440\n",
            "Validation Loss: 553.2573\n",
            "Epoch [88510/100000]\n",
            "Training Loss: 447.2287\n",
            "Validation Loss: 554.2208\n",
            "Epoch [88520/100000]\n",
            "Training Loss: 447.2397\n",
            "Validation Loss: 555.4578\n",
            "Epoch [88530/100000]\n",
            "Training Loss: 444.7047\n",
            "Validation Loss: 553.6752\n",
            "Epoch [88540/100000]\n",
            "Training Loss: 455.3349\n",
            "Validation Loss: 554.1028\n",
            "Epoch [88550/100000]\n",
            "Training Loss: 446.9352\n",
            "Validation Loss: 552.6947\n",
            "Epoch [88560/100000]\n",
            "Training Loss: 448.8881\n",
            "Validation Loss: 554.1184\n",
            "Epoch [88570/100000]\n",
            "Training Loss: 447.3944\n",
            "Validation Loss: 555.0029\n",
            "Epoch [88580/100000]\n",
            "Training Loss: 448.3947\n",
            "Validation Loss: 551.9686\n",
            "Epoch [88590/100000]\n",
            "Training Loss: 446.7055\n",
            "Validation Loss: 553.4476\n",
            "Epoch [88600/100000]\n",
            "Training Loss: 445.4026\n",
            "Validation Loss: 551.1614\n",
            "Epoch [88610/100000]\n",
            "Training Loss: 446.7735\n",
            "Validation Loss: 550.6697\n",
            "Epoch [88620/100000]\n",
            "Training Loss: 446.5364\n",
            "Validation Loss: 556.8749\n",
            "Epoch [88630/100000]\n",
            "Training Loss: 449.4133\n",
            "Validation Loss: 550.7930\n",
            "Epoch [88640/100000]\n",
            "Training Loss: 449.4529\n",
            "Validation Loss: 550.0701\n",
            "Epoch [88650/100000]\n",
            "Training Loss: 448.8160\n",
            "Validation Loss: 552.8237\n",
            "Epoch [88660/100000]\n",
            "Training Loss: 447.0947\n",
            "Validation Loss: 553.0308\n",
            "Epoch [88670/100000]\n",
            "Training Loss: 449.5827\n",
            "Validation Loss: 551.7469\n",
            "Epoch [88680/100000]\n",
            "Training Loss: 446.5230\n",
            "Validation Loss: 554.0197\n",
            "Epoch [88690/100000]\n",
            "Training Loss: 449.1574\n",
            "Validation Loss: 552.8924\n",
            "Epoch [88700/100000]\n",
            "Training Loss: 448.8073\n",
            "Validation Loss: 555.4396\n",
            "Epoch [88710/100000]\n",
            "Training Loss: 446.6706\n",
            "Validation Loss: 549.1590\n",
            "Epoch [88720/100000]\n",
            "Training Loss: 454.8374\n",
            "Validation Loss: 556.0938\n",
            "Epoch [88730/100000]\n",
            "Training Loss: 451.1111\n",
            "Validation Loss: 552.0863\n",
            "Epoch [88740/100000]\n",
            "Training Loss: 451.7619\n",
            "Validation Loss: 553.4096\n",
            "Epoch [88750/100000]\n",
            "Training Loss: 456.6704\n",
            "Validation Loss: 550.6755\n",
            "Epoch [88760/100000]\n",
            "Training Loss: 447.3766\n",
            "Validation Loss: 551.6166\n",
            "Epoch [88770/100000]\n",
            "Training Loss: 447.9932\n",
            "Validation Loss: 554.1030\n",
            "Epoch [88780/100000]\n",
            "Training Loss: 448.8631\n",
            "Validation Loss: 550.8062\n",
            "Epoch [88790/100000]\n",
            "Training Loss: 450.6325\n",
            "Validation Loss: 556.0247\n",
            "Epoch [88800/100000]\n",
            "Training Loss: 446.4690\n",
            "Validation Loss: 552.9829\n",
            "Epoch [88810/100000]\n",
            "Training Loss: 446.7440\n",
            "Validation Loss: 553.9631\n",
            "Epoch [88820/100000]\n",
            "Training Loss: 450.1403\n",
            "Validation Loss: 556.2919\n",
            "Epoch [88830/100000]\n",
            "Training Loss: 444.8118\n",
            "Validation Loss: 553.7738\n",
            "Epoch [88840/100000]\n",
            "Training Loss: 452.4832\n",
            "Validation Loss: 549.0189\n",
            "Epoch [88850/100000]\n",
            "Training Loss: 444.3262\n",
            "Validation Loss: 553.3317\n",
            "Epoch [88860/100000]\n",
            "Training Loss: 452.5274\n",
            "Validation Loss: 553.5402\n",
            "Epoch [88870/100000]\n",
            "Training Loss: 446.7055\n",
            "Validation Loss: 556.7433\n",
            "Epoch [88880/100000]\n",
            "Training Loss: 445.3931\n",
            "Validation Loss: 551.3825\n",
            "Epoch [88890/100000]\n",
            "Training Loss: 445.9133\n",
            "Validation Loss: 554.4272\n",
            "Epoch [88900/100000]\n",
            "Training Loss: 444.8829\n",
            "Validation Loss: 552.4206\n",
            "Epoch [88910/100000]\n",
            "Training Loss: 452.0108\n",
            "Validation Loss: 552.9749\n",
            "Epoch [88920/100000]\n",
            "Training Loss: 448.2546\n",
            "Validation Loss: 552.6938\n",
            "Epoch [88930/100000]\n",
            "Training Loss: 449.6372\n",
            "Validation Loss: 552.6529\n",
            "Epoch [88940/100000]\n",
            "Training Loss: 449.1738\n",
            "Validation Loss: 553.1901\n",
            "Epoch [88950/100000]\n",
            "Training Loss: 444.1694\n",
            "Validation Loss: 553.0159\n",
            "Epoch [88960/100000]\n",
            "Training Loss: 452.1987\n",
            "Validation Loss: 551.3695\n",
            "Epoch [88970/100000]\n",
            "Training Loss: 447.6588\n",
            "Validation Loss: 554.5396\n",
            "Epoch [88980/100000]\n",
            "Training Loss: 445.9457\n",
            "Validation Loss: 552.8648\n",
            "Epoch [88990/100000]\n",
            "Training Loss: 455.4824\n",
            "Validation Loss: 550.8774\n",
            "Epoch [89000/100000]\n",
            "Training Loss: 446.8443\n",
            "Validation Loss: 548.9658\n",
            "Epoch [89010/100000]\n",
            "Training Loss: 446.6284\n",
            "Validation Loss: 553.3315\n",
            "Epoch [89020/100000]\n",
            "Training Loss: 445.6824\n",
            "Validation Loss: 553.7668\n",
            "Epoch [89030/100000]\n",
            "Training Loss: 449.9707\n",
            "Validation Loss: 551.3956\n",
            "Epoch [89040/100000]\n",
            "Training Loss: 448.8778\n",
            "Validation Loss: 551.7211\n",
            "Epoch [89050/100000]\n",
            "Training Loss: 441.7115\n",
            "Validation Loss: 552.6844\n",
            "Epoch [89060/100000]\n",
            "Training Loss: 445.0570\n",
            "Validation Loss: 551.5475\n",
            "Epoch [89070/100000]\n",
            "Training Loss: 444.0884\n",
            "Validation Loss: 551.2750\n",
            "Epoch [89080/100000]\n",
            "Training Loss: 451.4922\n",
            "Validation Loss: 550.9858\n",
            "Epoch [89090/100000]\n",
            "Training Loss: 445.9760\n",
            "Validation Loss: 551.2567\n",
            "Epoch [89100/100000]\n",
            "Training Loss: 450.1158\n",
            "Validation Loss: 554.3525\n",
            "Epoch [89110/100000]\n",
            "Training Loss: 451.3445\n",
            "Validation Loss: 554.4975\n",
            "Epoch [89120/100000]\n",
            "Training Loss: 454.1772\n",
            "Validation Loss: 550.0175\n",
            "Epoch [89130/100000]\n",
            "Training Loss: 444.5819\n",
            "Validation Loss: 553.7992\n",
            "Epoch [89140/100000]\n",
            "Training Loss: 449.6022\n",
            "Validation Loss: 551.3588\n",
            "Epoch [89150/100000]\n",
            "Training Loss: 445.4964\n",
            "Validation Loss: 552.9972\n",
            "Epoch [89160/100000]\n",
            "Training Loss: 450.1223\n",
            "Validation Loss: 553.3150\n",
            "Epoch [89170/100000]\n",
            "Training Loss: 444.5625\n",
            "Validation Loss: 553.0760\n",
            "Epoch [89180/100000]\n",
            "Training Loss: 451.9639\n",
            "Validation Loss: 554.5906\n",
            "Epoch [89190/100000]\n",
            "Training Loss: 446.4058\n",
            "Validation Loss: 553.5730\n",
            "Epoch [89200/100000]\n",
            "Training Loss: 445.5514\n",
            "Validation Loss: 549.5350\n",
            "Epoch [89210/100000]\n",
            "Training Loss: 446.3869\n",
            "Validation Loss: 552.9232\n",
            "Epoch [89220/100000]\n",
            "Training Loss: 451.1701\n",
            "Validation Loss: 549.2781\n",
            "Epoch [89230/100000]\n",
            "Training Loss: 445.6273\n",
            "Validation Loss: 551.4083\n",
            "Epoch [89240/100000]\n",
            "Training Loss: 448.7729\n",
            "Validation Loss: 551.3635\n",
            "Epoch [89250/100000]\n",
            "Training Loss: 444.3507\n",
            "Validation Loss: 552.7766\n",
            "Epoch [89260/100000]\n",
            "Training Loss: 440.4288\n",
            "Validation Loss: 553.3068\n",
            "Epoch [89270/100000]\n",
            "Training Loss: 441.6989\n",
            "Validation Loss: 553.2469\n",
            "Epoch [89280/100000]\n",
            "Training Loss: 445.4214\n",
            "Validation Loss: 548.8040\n",
            "Epoch [89290/100000]\n",
            "Training Loss: 448.8214\n",
            "Validation Loss: 552.7843\n",
            "Epoch [89300/100000]\n",
            "Training Loss: 445.7793\n",
            "Validation Loss: 553.1862\n",
            "Epoch [89310/100000]\n",
            "Training Loss: 444.5233\n",
            "Validation Loss: 552.5904\n",
            "Epoch [89320/100000]\n",
            "Training Loss: 450.2262\n",
            "Validation Loss: 552.9412\n",
            "Epoch [89330/100000]\n",
            "Training Loss: 451.8970\n",
            "Validation Loss: 550.8630\n",
            "Epoch [89340/100000]\n",
            "Training Loss: 452.1429\n",
            "Validation Loss: 552.9335\n",
            "Epoch [89350/100000]\n",
            "Training Loss: 445.3132\n",
            "Validation Loss: 552.5110\n",
            "Epoch [89360/100000]\n",
            "Training Loss: 445.0974\n",
            "Validation Loss: 552.4285\n",
            "Epoch [89370/100000]\n",
            "Training Loss: 446.8291\n",
            "Validation Loss: 553.8163\n",
            "Epoch [89380/100000]\n",
            "Training Loss: 446.9074\n",
            "Validation Loss: 552.5079\n",
            "Epoch [89390/100000]\n",
            "Training Loss: 447.6966\n",
            "Validation Loss: 552.1026\n",
            "Epoch [89400/100000]\n",
            "Training Loss: 446.4635\n",
            "Validation Loss: 550.8508\n",
            "Epoch [89410/100000]\n",
            "Training Loss: 443.4972\n",
            "Validation Loss: 552.6965\n",
            "Epoch [89420/100000]\n",
            "Training Loss: 446.5323\n",
            "Validation Loss: 551.5249\n",
            "Epoch [89430/100000]\n",
            "Training Loss: 449.2225\n",
            "Validation Loss: 551.7478\n",
            "Epoch [89440/100000]\n",
            "Training Loss: 443.6356\n",
            "Validation Loss: 551.7443\n",
            "Epoch [89450/100000]\n",
            "Training Loss: 444.0658\n",
            "Validation Loss: 553.1729\n",
            "Epoch [89460/100000]\n",
            "Training Loss: 450.2284\n",
            "Validation Loss: 551.8041\n",
            "Epoch [89470/100000]\n",
            "Training Loss: 442.6824\n",
            "Validation Loss: 551.8369\n",
            "Epoch [89480/100000]\n",
            "Training Loss: 444.5761\n",
            "Validation Loss: 550.3219\n",
            "Epoch [89490/100000]\n",
            "Training Loss: 444.4884\n",
            "Validation Loss: 552.7620\n",
            "Epoch [89500/100000]\n",
            "Training Loss: 448.6935\n",
            "Validation Loss: 553.7661\n",
            "Epoch [89510/100000]\n",
            "Training Loss: 440.6957\n",
            "Validation Loss: 552.4417\n",
            "Epoch [89520/100000]\n",
            "Training Loss: 445.6438\n",
            "Validation Loss: 550.8317\n",
            "Epoch [89530/100000]\n",
            "Training Loss: 439.5376\n",
            "Validation Loss: 550.7458\n",
            "Epoch [89540/100000]\n",
            "Training Loss: 444.6695\n",
            "Validation Loss: 553.3674\n",
            "Epoch [89550/100000]\n",
            "Training Loss: 455.3217\n",
            "Validation Loss: 549.8525\n",
            "Epoch [89560/100000]\n",
            "Training Loss: 450.2209\n",
            "Validation Loss: 552.4382\n",
            "Epoch [89570/100000]\n",
            "Training Loss: 442.4299\n",
            "Validation Loss: 548.6158\n",
            "Epoch [89580/100000]\n",
            "Training Loss: 449.2118\n",
            "Validation Loss: 552.7484\n",
            "Epoch [89590/100000]\n",
            "Training Loss: 442.8348\n",
            "Validation Loss: 554.7219\n",
            "Epoch [89600/100000]\n",
            "Training Loss: 442.4284\n",
            "Validation Loss: 555.3710\n",
            "Epoch [89610/100000]\n",
            "Training Loss: 444.2877\n",
            "Validation Loss: 550.4126\n",
            "Epoch [89620/100000]\n",
            "Training Loss: 449.5571\n",
            "Validation Loss: 553.4836\n",
            "Epoch [89630/100000]\n",
            "Training Loss: 449.6437\n",
            "Validation Loss: 552.4987\n",
            "Epoch [89640/100000]\n",
            "Training Loss: 443.6112\n",
            "Validation Loss: 555.2592\n",
            "Epoch [89650/100000]\n",
            "Training Loss: 443.5634\n",
            "Validation Loss: 553.8275\n",
            "Epoch [89660/100000]\n",
            "Training Loss: 439.0413\n",
            "Validation Loss: 553.1804\n",
            "Epoch [89670/100000]\n",
            "Training Loss: 446.8962\n",
            "Validation Loss: 551.6061\n",
            "Epoch [89680/100000]\n",
            "Training Loss: 446.9092\n",
            "Validation Loss: 551.7762\n",
            "Epoch [89690/100000]\n",
            "Training Loss: 451.8889\n",
            "Validation Loss: 550.9473\n",
            "Epoch [89700/100000]\n",
            "Training Loss: 450.1165\n",
            "Validation Loss: 551.0005\n",
            "Epoch [89710/100000]\n",
            "Training Loss: 447.1576\n",
            "Validation Loss: 551.8366\n",
            "Epoch [89720/100000]\n",
            "Training Loss: 447.8052\n",
            "Validation Loss: 551.8778\n",
            "Epoch [89730/100000]\n",
            "Training Loss: 447.6819\n",
            "Validation Loss: 552.2966\n",
            "Epoch [89740/100000]\n",
            "Training Loss: 442.6339\n",
            "Validation Loss: 548.1418\n",
            "Epoch [89750/100000]\n",
            "Training Loss: 439.7171\n",
            "Validation Loss: 548.7164\n",
            "Epoch [89760/100000]\n",
            "Training Loss: 441.2172\n",
            "Validation Loss: 551.4075\n",
            "Epoch [89770/100000]\n",
            "Training Loss: 441.0997\n",
            "Validation Loss: 551.5687\n",
            "Epoch [89780/100000]\n",
            "Training Loss: 446.3331\n",
            "Validation Loss: 551.9532\n",
            "Epoch [89790/100000]\n",
            "Training Loss: 442.1823\n",
            "Validation Loss: 551.8608\n",
            "Epoch [89800/100000]\n",
            "Training Loss: 443.5000\n",
            "Validation Loss: 552.9524\n",
            "Epoch [89810/100000]\n",
            "Training Loss: 447.5523\n",
            "Validation Loss: 550.6596\n",
            "Epoch [89820/100000]\n",
            "Training Loss: 442.9682\n",
            "Validation Loss: 551.2579\n",
            "Epoch [89830/100000]\n",
            "Training Loss: 445.0091\n",
            "Validation Loss: 553.1334\n",
            "Epoch [89840/100000]\n",
            "Training Loss: 442.3510\n",
            "Validation Loss: 552.1361\n",
            "Epoch [89850/100000]\n",
            "Training Loss: 446.9974\n",
            "Validation Loss: 550.7024\n",
            "Epoch [89860/100000]\n",
            "Training Loss: 435.1125\n",
            "Validation Loss: 552.6811\n",
            "Epoch [89870/100000]\n",
            "Training Loss: 439.5423\n",
            "Validation Loss: 554.5121\n",
            "Epoch [89880/100000]\n",
            "Training Loss: 442.0800\n",
            "Validation Loss: 553.6152\n",
            "Epoch [89890/100000]\n",
            "Training Loss: 446.4640\n",
            "Validation Loss: 552.0131\n",
            "Epoch [89900/100000]\n",
            "Training Loss: 445.2227\n",
            "Validation Loss: 553.9505\n",
            "Epoch [89910/100000]\n",
            "Training Loss: 447.5192\n",
            "Validation Loss: 548.9571\n",
            "Epoch [89920/100000]\n",
            "Training Loss: 436.7151\n",
            "Validation Loss: 552.4184\n",
            "Epoch [89930/100000]\n",
            "Training Loss: 440.0582\n",
            "Validation Loss: 551.8403\n",
            "Epoch [89940/100000]\n",
            "Training Loss: 446.6013\n",
            "Validation Loss: 552.9165\n",
            "Epoch [89950/100000]\n",
            "Training Loss: 444.6928\n",
            "Validation Loss: 549.3818\n",
            "Epoch [89960/100000]\n",
            "Training Loss: 450.7202\n",
            "Validation Loss: 552.8409\n",
            "Epoch [89970/100000]\n",
            "Training Loss: 451.2031\n",
            "Validation Loss: 552.1399\n",
            "Epoch [89980/100000]\n",
            "Training Loss: 449.9569\n",
            "Validation Loss: 552.4827\n",
            "Epoch [89990/100000]\n",
            "Training Loss: 442.2725\n",
            "Validation Loss: 549.9687\n",
            "Epoch [90000/100000]\n",
            "Training Loss: 447.0312\n",
            "Validation Loss: 554.5101\n",
            "Epoch [90010/100000]\n",
            "Training Loss: 453.0926\n",
            "Validation Loss: 548.2490\n",
            "Epoch [90020/100000]\n",
            "Training Loss: 448.4341\n",
            "Validation Loss: 547.6364\n",
            "Epoch [90030/100000]\n",
            "Training Loss: 444.3289\n",
            "Validation Loss: 551.3007\n",
            "Epoch [90040/100000]\n",
            "Training Loss: 441.6375\n",
            "Validation Loss: 552.6100\n",
            "Epoch [90050/100000]\n",
            "Training Loss: 443.0457\n",
            "Validation Loss: 551.4431\n",
            "Epoch [90060/100000]\n",
            "Training Loss: 440.5103\n",
            "Validation Loss: 553.3896\n",
            "Epoch [90070/100000]\n",
            "Training Loss: 439.6209\n",
            "Validation Loss: 550.0698\n",
            "Epoch [90080/100000]\n",
            "Training Loss: 442.0957\n",
            "Validation Loss: 549.8610\n",
            "Epoch [90090/100000]\n",
            "Training Loss: 442.0094\n",
            "Validation Loss: 552.6342\n",
            "Epoch [90100/100000]\n",
            "Training Loss: 438.5140\n",
            "Validation Loss: 554.2809\n",
            "Epoch [90110/100000]\n",
            "Training Loss: 444.5070\n",
            "Validation Loss: 553.3665\n",
            "Epoch [90120/100000]\n",
            "Training Loss: 445.1760\n",
            "Validation Loss: 553.8369\n",
            "Epoch [90130/100000]\n",
            "Training Loss: 444.1075\n",
            "Validation Loss: 552.7620\n",
            "Epoch [90140/100000]\n",
            "Training Loss: 442.6156\n",
            "Validation Loss: 551.5349\n",
            "Epoch [90150/100000]\n",
            "Training Loss: 445.3304\n",
            "Validation Loss: 552.6641\n",
            "Epoch [90160/100000]\n",
            "Training Loss: 450.1874\n",
            "Validation Loss: 548.4998\n",
            "Epoch [90170/100000]\n",
            "Training Loss: 445.2458\n",
            "Validation Loss: 554.5929\n",
            "Epoch [90180/100000]\n",
            "Training Loss: 444.3777\n",
            "Validation Loss: 552.9875\n",
            "Epoch [90190/100000]\n",
            "Training Loss: 441.1890\n",
            "Validation Loss: 551.3431\n",
            "Epoch [90200/100000]\n",
            "Training Loss: 443.4247\n",
            "Validation Loss: 550.3139\n",
            "Epoch [90210/100000]\n",
            "Training Loss: 446.4600\n",
            "Validation Loss: 549.2952\n",
            "Epoch [90220/100000]\n",
            "Training Loss: 444.4203\n",
            "Validation Loss: 550.9113\n",
            "Epoch [90230/100000]\n",
            "Training Loss: 440.0438\n",
            "Validation Loss: 552.6601\n",
            "Epoch [90240/100000]\n",
            "Training Loss: 444.5215\n",
            "Validation Loss: 550.3357\n",
            "Epoch [90250/100000]\n",
            "Training Loss: 444.4098\n",
            "Validation Loss: 553.4656\n",
            "Epoch [90260/100000]\n",
            "Training Loss: 442.7714\n",
            "Validation Loss: 552.0720\n",
            "Epoch [90270/100000]\n",
            "Training Loss: 443.7807\n",
            "Validation Loss: 550.4603\n",
            "Epoch [90280/100000]\n",
            "Training Loss: 449.5455\n",
            "Validation Loss: 551.5090\n",
            "Epoch [90290/100000]\n",
            "Training Loss: 441.1221\n",
            "Validation Loss: 550.6616\n",
            "Epoch [90300/100000]\n",
            "Training Loss: 440.5168\n",
            "Validation Loss: 557.2025\n",
            "Epoch [90310/100000]\n",
            "Training Loss: 445.5419\n",
            "Validation Loss: 552.4731\n",
            "Epoch [90320/100000]\n",
            "Training Loss: 439.9330\n",
            "Validation Loss: 554.4750\n",
            "Epoch [90330/100000]\n",
            "Training Loss: 441.0156\n",
            "Validation Loss: 551.6885\n",
            "Epoch [90340/100000]\n",
            "Training Loss: 439.4777\n",
            "Validation Loss: 550.1903\n",
            "Epoch [90350/100000]\n",
            "Training Loss: 443.5381\n",
            "Validation Loss: 551.0779\n",
            "Epoch [90360/100000]\n",
            "Training Loss: 438.7475\n",
            "Validation Loss: 551.3412\n",
            "Epoch [90370/100000]\n",
            "Training Loss: 441.3190\n",
            "Validation Loss: 552.1370\n",
            "Epoch [90380/100000]\n",
            "Training Loss: 436.5626\n",
            "Validation Loss: 554.6897\n",
            "Epoch [90390/100000]\n",
            "Training Loss: 441.8686\n",
            "Validation Loss: 552.3004\n",
            "Epoch [90400/100000]\n",
            "Training Loss: 444.3005\n",
            "Validation Loss: 551.2163\n",
            "Epoch [90410/100000]\n",
            "Training Loss: 443.9468\n",
            "Validation Loss: 550.9166\n",
            "Epoch [90420/100000]\n",
            "Training Loss: 446.3129\n",
            "Validation Loss: 550.8411\n",
            "Epoch [90430/100000]\n",
            "Training Loss: 441.8101\n",
            "Validation Loss: 551.8876\n",
            "Epoch [90440/100000]\n",
            "Training Loss: 439.3115\n",
            "Validation Loss: 556.9825\n",
            "Epoch [90450/100000]\n",
            "Training Loss: 440.8567\n",
            "Validation Loss: 549.3171\n",
            "Epoch [90460/100000]\n",
            "Training Loss: 438.9186\n",
            "Validation Loss: 553.6182\n",
            "Epoch [90470/100000]\n",
            "Training Loss: 438.4040\n",
            "Validation Loss: 547.6405\n",
            "Epoch [90480/100000]\n",
            "Training Loss: 445.8516\n",
            "Validation Loss: 547.6536\n",
            "Epoch [90490/100000]\n",
            "Training Loss: 447.0037\n",
            "Validation Loss: 553.0620\n",
            "Epoch [90500/100000]\n",
            "Training Loss: 443.1516\n",
            "Validation Loss: 551.3390\n",
            "Epoch [90510/100000]\n",
            "Training Loss: 441.1282\n",
            "Validation Loss: 549.1440\n",
            "Epoch [90520/100000]\n",
            "Training Loss: 439.9193\n",
            "Validation Loss: 553.0467\n",
            "Epoch [90530/100000]\n",
            "Training Loss: 442.1375\n",
            "Validation Loss: 549.7517\n",
            "Epoch [90540/100000]\n",
            "Training Loss: 441.7601\n",
            "Validation Loss: 552.2219\n",
            "Epoch [90550/100000]\n",
            "Training Loss: 440.7866\n",
            "Validation Loss: 552.5255\n",
            "Epoch [90560/100000]\n",
            "Training Loss: 438.7983\n",
            "Validation Loss: 556.4080\n",
            "Epoch [90570/100000]\n",
            "Training Loss: 444.1295\n",
            "Validation Loss: 548.4774\n",
            "Epoch [90580/100000]\n",
            "Training Loss: 443.1153\n",
            "Validation Loss: 552.2531\n",
            "Epoch [90590/100000]\n",
            "Training Loss: 439.6595\n",
            "Validation Loss: 554.1503\n",
            "Epoch [90600/100000]\n",
            "Training Loss: 443.3626\n",
            "Validation Loss: 551.6300\n",
            "Epoch [90610/100000]\n",
            "Training Loss: 440.8033\n",
            "Validation Loss: 556.5365\n",
            "Epoch [90620/100000]\n",
            "Training Loss: 443.1794\n",
            "Validation Loss: 547.1183\n",
            "Epoch [90630/100000]\n",
            "Training Loss: 441.0956\n",
            "Validation Loss: 551.3861\n",
            "Epoch [90640/100000]\n",
            "Training Loss: 439.3751\n",
            "Validation Loss: 554.6526\n",
            "Epoch [90650/100000]\n",
            "Training Loss: 451.1383\n",
            "Validation Loss: 548.9515\n",
            "Epoch [90660/100000]\n",
            "Training Loss: 450.5230\n",
            "Validation Loss: 551.5919\n",
            "Epoch [90670/100000]\n",
            "Training Loss: 444.8103\n",
            "Validation Loss: 553.2128\n",
            "Epoch [90680/100000]\n",
            "Training Loss: 438.8921\n",
            "Validation Loss: 552.5675\n",
            "Epoch [90690/100000]\n",
            "Training Loss: 439.2979\n",
            "Validation Loss: 554.2081\n",
            "Epoch [90700/100000]\n",
            "Training Loss: 440.1587\n",
            "Validation Loss: 549.1663\n",
            "Epoch [90710/100000]\n",
            "Training Loss: 440.6234\n",
            "Validation Loss: 548.5045\n",
            "Epoch [90720/100000]\n",
            "Training Loss: 443.1460\n",
            "Validation Loss: 549.5341\n",
            "Epoch [90730/100000]\n",
            "Training Loss: 436.6247\n",
            "Validation Loss: 548.6105\n",
            "Epoch [90740/100000]\n",
            "Training Loss: 436.3876\n",
            "Validation Loss: 551.7304\n",
            "Epoch [90750/100000]\n",
            "Training Loss: 447.5601\n",
            "Validation Loss: 554.7010\n",
            "Epoch [90760/100000]\n",
            "Training Loss: 445.3747\n",
            "Validation Loss: 548.2912\n",
            "Epoch [90770/100000]\n",
            "Training Loss: 443.9920\n",
            "Validation Loss: 551.4144\n",
            "Epoch [90780/100000]\n",
            "Training Loss: 442.8020\n",
            "Validation Loss: 552.5759\n",
            "Epoch [90790/100000]\n",
            "Training Loss: 442.1734\n",
            "Validation Loss: 550.8384\n",
            "Epoch [90800/100000]\n",
            "Training Loss: 440.9679\n",
            "Validation Loss: 548.7722\n",
            "Epoch [90810/100000]\n",
            "Training Loss: 436.5562\n",
            "Validation Loss: 548.4152\n",
            "Epoch [90820/100000]\n",
            "Training Loss: 440.2216\n",
            "Validation Loss: 551.2478\n",
            "Epoch [90830/100000]\n",
            "Training Loss: 439.4286\n",
            "Validation Loss: 553.1024\n",
            "Epoch [90840/100000]\n",
            "Training Loss: 440.1573\n",
            "Validation Loss: 550.9669\n",
            "Epoch [90850/100000]\n",
            "Training Loss: 449.3603\n",
            "Validation Loss: 550.0803\n",
            "Epoch [90860/100000]\n",
            "Training Loss: 440.0641\n",
            "Validation Loss: 548.7537\n",
            "Epoch [90870/100000]\n",
            "Training Loss: 443.8657\n",
            "Validation Loss: 550.5458\n",
            "Epoch [90880/100000]\n",
            "Training Loss: 435.3444\n",
            "Validation Loss: 551.9449\n",
            "Epoch [90890/100000]\n",
            "Training Loss: 440.6441\n",
            "Validation Loss: 550.8935\n",
            "Epoch [90900/100000]\n",
            "Training Loss: 442.3073\n",
            "Validation Loss: 547.0510\n",
            "Epoch [90910/100000]\n",
            "Training Loss: 445.8165\n",
            "Validation Loss: 550.3940\n",
            "Epoch [90920/100000]\n",
            "Training Loss: 435.1717\n",
            "Validation Loss: 554.9001\n",
            "Epoch [90930/100000]\n",
            "Training Loss: 437.2980\n",
            "Validation Loss: 552.0182\n",
            "Epoch [90940/100000]\n",
            "Training Loss: 443.0279\n",
            "Validation Loss: 551.8540\n",
            "Epoch [90950/100000]\n",
            "Training Loss: 435.1486\n",
            "Validation Loss: 550.4720\n",
            "Epoch [90960/100000]\n",
            "Training Loss: 438.5493\n",
            "Validation Loss: 549.0250\n",
            "Epoch [90970/100000]\n",
            "Training Loss: 437.7196\n",
            "Validation Loss: 553.0516\n",
            "Epoch [90980/100000]\n",
            "Training Loss: 439.6256\n",
            "Validation Loss: 551.1588\n",
            "Epoch [90990/100000]\n",
            "Training Loss: 440.1937\n",
            "Validation Loss: 550.5831\n",
            "Epoch [91000/100000]\n",
            "Training Loss: 443.1342\n",
            "Validation Loss: 548.1957\n",
            "Epoch [91010/100000]\n",
            "Training Loss: 443.1945\n",
            "Validation Loss: 551.1324\n",
            "Epoch [91020/100000]\n",
            "Training Loss: 437.2874\n",
            "Validation Loss: 550.1683\n",
            "Epoch [91030/100000]\n",
            "Training Loss: 434.2777\n",
            "Validation Loss: 551.1287\n",
            "Epoch [91040/100000]\n",
            "Training Loss: 438.0287\n",
            "Validation Loss: 550.3194\n",
            "Epoch [91050/100000]\n",
            "Training Loss: 441.1366\n",
            "Validation Loss: 552.7497\n",
            "Epoch [91060/100000]\n",
            "Training Loss: 440.3965\n",
            "Validation Loss: 550.3113\n",
            "Epoch [91070/100000]\n",
            "Training Loss: 437.5812\n",
            "Validation Loss: 552.3170\n",
            "Epoch [91080/100000]\n",
            "Training Loss: 437.7858\n",
            "Validation Loss: 548.7011\n",
            "Epoch [91090/100000]\n",
            "Training Loss: 440.1279\n",
            "Validation Loss: 549.6746\n",
            "Epoch [91100/100000]\n",
            "Training Loss: 439.6351\n",
            "Validation Loss: 549.3668\n",
            "Epoch [91110/100000]\n",
            "Training Loss: 448.0683\n",
            "Validation Loss: 549.1108\n",
            "Epoch [91120/100000]\n",
            "Training Loss: 443.1283\n",
            "Validation Loss: 551.2714\n",
            "Epoch [91130/100000]\n",
            "Training Loss: 441.3293\n",
            "Validation Loss: 550.9780\n",
            "Epoch [91140/100000]\n",
            "Training Loss: 443.5145\n",
            "Validation Loss: 550.5615\n",
            "Epoch [91150/100000]\n",
            "Training Loss: 438.9079\n",
            "Validation Loss: 549.2180\n",
            "Epoch [91160/100000]\n",
            "Training Loss: 436.4679\n",
            "Validation Loss: 549.4373\n",
            "Epoch [91170/100000]\n",
            "Training Loss: 438.2688\n",
            "Validation Loss: 552.3998\n",
            "Epoch [91180/100000]\n",
            "Training Loss: 441.9149\n",
            "Validation Loss: 550.9850\n",
            "Epoch [91190/100000]\n",
            "Training Loss: 441.4936\n",
            "Validation Loss: 552.4463\n",
            "Epoch [91200/100000]\n",
            "Training Loss: 436.1361\n",
            "Validation Loss: 552.5190\n",
            "Epoch [91210/100000]\n",
            "Training Loss: 443.9457\n",
            "Validation Loss: 550.5331\n",
            "Epoch [91220/100000]\n",
            "Training Loss: 440.2260\n",
            "Validation Loss: 549.9255\n",
            "Epoch [91230/100000]\n",
            "Training Loss: 440.5446\n",
            "Validation Loss: 549.1651\n",
            "Epoch [91240/100000]\n",
            "Training Loss: 442.2475\n",
            "Validation Loss: 547.2560\n",
            "Epoch [91250/100000]\n",
            "Training Loss: 442.0340\n",
            "Validation Loss: 551.3255\n",
            "Epoch [91260/100000]\n",
            "Training Loss: 443.1946\n",
            "Validation Loss: 551.5124\n",
            "Epoch [91270/100000]\n",
            "Training Loss: 445.2686\n",
            "Validation Loss: 551.7146\n",
            "Epoch [91280/100000]\n",
            "Training Loss: 439.6238\n",
            "Validation Loss: 549.5228\n",
            "Epoch [91290/100000]\n",
            "Training Loss: 443.8548\n",
            "Validation Loss: 547.1006\n",
            "Epoch [91300/100000]\n",
            "Training Loss: 442.2470\n",
            "Validation Loss: 548.3218\n",
            "Epoch [91310/100000]\n",
            "Training Loss: 443.5402\n",
            "Validation Loss: 546.5892\n",
            "Epoch [91320/100000]\n",
            "Training Loss: 441.5193\n",
            "Validation Loss: 548.1302\n",
            "Epoch [91330/100000]\n",
            "Training Loss: 436.6269\n",
            "Validation Loss: 553.6887\n",
            "Epoch [91340/100000]\n",
            "Training Loss: 439.2287\n",
            "Validation Loss: 549.8164\n",
            "Epoch [91350/100000]\n",
            "Training Loss: 439.0127\n",
            "Validation Loss: 552.7887\n",
            "Epoch [91360/100000]\n",
            "Training Loss: 441.1573\n",
            "Validation Loss: 550.1986\n",
            "Epoch [91370/100000]\n",
            "Training Loss: 444.7314\n",
            "Validation Loss: 548.5426\n",
            "Epoch [91380/100000]\n",
            "Training Loss: 439.7930\n",
            "Validation Loss: 548.0551\n",
            "Epoch [91390/100000]\n",
            "Training Loss: 438.7859\n",
            "Validation Loss: 548.6633\n",
            "Epoch [91400/100000]\n",
            "Training Loss: 437.6419\n",
            "Validation Loss: 551.8799\n",
            "Epoch [91410/100000]\n",
            "Training Loss: 437.9013\n",
            "Validation Loss: 549.8076\n",
            "Epoch [91420/100000]\n",
            "Training Loss: 440.1762\n",
            "Validation Loss: 549.8647\n",
            "Epoch [91430/100000]\n",
            "Training Loss: 442.6747\n",
            "Validation Loss: 549.0800\n",
            "Epoch [91440/100000]\n",
            "Training Loss: 439.0426\n",
            "Validation Loss: 546.8839\n",
            "Epoch [91450/100000]\n",
            "Training Loss: 438.7856\n",
            "Validation Loss: 550.4095\n",
            "Epoch [91460/100000]\n",
            "Training Loss: 440.0964\n",
            "Validation Loss: 550.8695\n",
            "Epoch [91470/100000]\n",
            "Training Loss: 438.6918\n",
            "Validation Loss: 549.3008\n",
            "Epoch [91480/100000]\n",
            "Training Loss: 436.0016\n",
            "Validation Loss: 549.4782\n",
            "Epoch [91490/100000]\n",
            "Training Loss: 439.2132\n",
            "Validation Loss: 550.9719\n",
            "Epoch [91500/100000]\n",
            "Training Loss: 441.3302\n",
            "Validation Loss: 550.2384\n",
            "Epoch [91510/100000]\n",
            "Training Loss: 435.5583\n",
            "Validation Loss: 550.7029\n",
            "Epoch [91520/100000]\n",
            "Training Loss: 437.4592\n",
            "Validation Loss: 552.7480\n",
            "Epoch [91530/100000]\n",
            "Training Loss: 444.7009\n",
            "Validation Loss: 548.0326\n",
            "Epoch [91540/100000]\n",
            "Training Loss: 444.2014\n",
            "Validation Loss: 549.1778\n",
            "Epoch [91550/100000]\n",
            "Training Loss: 450.7100\n",
            "Validation Loss: 545.5550\n",
            "Epoch [91560/100000]\n",
            "Training Loss: 445.6964\n",
            "Validation Loss: 545.6885\n",
            "Epoch [91570/100000]\n",
            "Training Loss: 447.7215\n",
            "Validation Loss: 542.6998\n",
            "Epoch [91580/100000]\n",
            "Training Loss: 441.1985\n",
            "Validation Loss: 553.5931\n",
            "Epoch [91590/100000]\n",
            "Training Loss: 442.4049\n",
            "Validation Loss: 550.8719\n",
            "Epoch [91600/100000]\n",
            "Training Loss: 437.8774\n",
            "Validation Loss: 552.7457\n",
            "Epoch [91610/100000]\n",
            "Training Loss: 438.5551\n",
            "Validation Loss: 550.6291\n",
            "Epoch [91620/100000]\n",
            "Training Loss: 443.8375\n",
            "Validation Loss: 549.5418\n",
            "Epoch [91630/100000]\n",
            "Training Loss: 441.0857\n",
            "Validation Loss: 551.5104\n",
            "Epoch [91640/100000]\n",
            "Training Loss: 435.2151\n",
            "Validation Loss: 557.7526\n",
            "Epoch [91650/100000]\n",
            "Training Loss: 442.8379\n",
            "Validation Loss: 548.4695\n",
            "Epoch [91660/100000]\n",
            "Training Loss: 437.4720\n",
            "Validation Loss: 552.5856\n",
            "Epoch [91670/100000]\n",
            "Training Loss: 441.1467\n",
            "Validation Loss: 550.6771\n",
            "Epoch [91680/100000]\n",
            "Training Loss: 435.6733\n",
            "Validation Loss: 550.0181\n",
            "Epoch [91690/100000]\n",
            "Training Loss: 439.8213\n",
            "Validation Loss: 551.1188\n",
            "Epoch [91700/100000]\n",
            "Training Loss: 440.8501\n",
            "Validation Loss: 551.6879\n",
            "Epoch [91710/100000]\n",
            "Training Loss: 440.2224\n",
            "Validation Loss: 549.1732\n",
            "Epoch [91720/100000]\n",
            "Training Loss: 443.2223\n",
            "Validation Loss: 548.1473\n",
            "Epoch [91730/100000]\n",
            "Training Loss: 440.5004\n",
            "Validation Loss: 549.8240\n",
            "Epoch [91740/100000]\n",
            "Training Loss: 442.1306\n",
            "Validation Loss: 546.3329\n",
            "Epoch [91750/100000]\n",
            "Training Loss: 440.6841\n",
            "Validation Loss: 551.8637\n",
            "Epoch [91760/100000]\n",
            "Training Loss: 440.8704\n",
            "Validation Loss: 548.4689\n",
            "Epoch [91770/100000]\n",
            "Training Loss: 440.9701\n",
            "Validation Loss: 547.6254\n",
            "Epoch [91780/100000]\n",
            "Training Loss: 437.5309\n",
            "Validation Loss: 549.1202\n",
            "Epoch [91790/100000]\n",
            "Training Loss: 439.8126\n",
            "Validation Loss: 549.6799\n",
            "Epoch [91800/100000]\n",
            "Training Loss: 443.3626\n",
            "Validation Loss: 548.4861\n",
            "Epoch [91810/100000]\n",
            "Training Loss: 437.8069\n",
            "Validation Loss: 550.7391\n",
            "Epoch [91820/100000]\n",
            "Training Loss: 438.7880\n",
            "Validation Loss: 550.9760\n",
            "Epoch [91830/100000]\n",
            "Training Loss: 432.9752\n",
            "Validation Loss: 551.7982\n",
            "Epoch [91840/100000]\n",
            "Training Loss: 435.2026\n",
            "Validation Loss: 552.7232\n",
            "Epoch [91850/100000]\n",
            "Training Loss: 436.5645\n",
            "Validation Loss: 550.0854\n",
            "Epoch [91860/100000]\n",
            "Training Loss: 436.1836\n",
            "Validation Loss: 551.9085\n",
            "Epoch [91870/100000]\n",
            "Training Loss: 443.7484\n",
            "Validation Loss: 550.1044\n",
            "Epoch [91880/100000]\n",
            "Training Loss: 433.9579\n",
            "Validation Loss: 549.9176\n",
            "Epoch [91890/100000]\n",
            "Training Loss: 435.7694\n",
            "Validation Loss: 552.9673\n",
            "Epoch [91900/100000]\n",
            "Training Loss: 442.8510\n",
            "Validation Loss: 549.4819\n",
            "Epoch [91910/100000]\n",
            "Training Loss: 444.5637\n",
            "Validation Loss: 552.8296\n",
            "Epoch [91920/100000]\n",
            "Training Loss: 441.1553\n",
            "Validation Loss: 549.5067\n",
            "Epoch [91930/100000]\n",
            "Training Loss: 442.8343\n",
            "Validation Loss: 551.3345\n",
            "Epoch [91940/100000]\n",
            "Training Loss: 438.3185\n",
            "Validation Loss: 552.0540\n",
            "Epoch [91950/100000]\n",
            "Training Loss: 440.1000\n",
            "Validation Loss: 547.6021\n",
            "Epoch [91960/100000]\n",
            "Training Loss: 437.8890\n",
            "Validation Loss: 547.5452\n",
            "Epoch [91970/100000]\n",
            "Training Loss: 442.1082\n",
            "Validation Loss: 553.6653\n",
            "Epoch [91980/100000]\n",
            "Training Loss: 438.5320\n",
            "Validation Loss: 550.3093\n",
            "Epoch [91990/100000]\n",
            "Training Loss: 443.2050\n",
            "Validation Loss: 550.1010\n",
            "Epoch [92000/100000]\n",
            "Training Loss: 437.3885\n",
            "Validation Loss: 551.6083\n",
            "Epoch [92010/100000]\n",
            "Training Loss: 440.5694\n",
            "Validation Loss: 546.1224\n",
            "Epoch [92020/100000]\n",
            "Training Loss: 437.8748\n",
            "Validation Loss: 555.3204\n",
            "Epoch [92030/100000]\n",
            "Training Loss: 434.8828\n",
            "Validation Loss: 554.3931\n",
            "Epoch [92040/100000]\n",
            "Training Loss: 444.7045\n",
            "Validation Loss: 550.2454\n",
            "Epoch [92050/100000]\n",
            "Training Loss: 441.9522\n",
            "Validation Loss: 547.8685\n",
            "Epoch [92060/100000]\n",
            "Training Loss: 436.0073\n",
            "Validation Loss: 550.8404\n",
            "Epoch [92070/100000]\n",
            "Training Loss: 442.5207\n",
            "Validation Loss: 549.2561\n",
            "Epoch [92080/100000]\n",
            "Training Loss: 438.8821\n",
            "Validation Loss: 552.9715\n",
            "Epoch [92090/100000]\n",
            "Training Loss: 441.0082\n",
            "Validation Loss: 548.1340\n",
            "Epoch [92100/100000]\n",
            "Training Loss: 438.6575\n",
            "Validation Loss: 551.3599\n",
            "Epoch [92110/100000]\n",
            "Training Loss: 438.2019\n",
            "Validation Loss: 550.7880\n",
            "Epoch [92120/100000]\n",
            "Training Loss: 439.1301\n",
            "Validation Loss: 550.0960\n",
            "Epoch [92130/100000]\n",
            "Training Loss: 434.1688\n",
            "Validation Loss: 550.0289\n",
            "Epoch [92140/100000]\n",
            "Training Loss: 444.4952\n",
            "Validation Loss: 549.7523\n",
            "Epoch [92150/100000]\n",
            "Training Loss: 443.8134\n",
            "Validation Loss: 549.5135\n",
            "Epoch [92160/100000]\n",
            "Training Loss: 437.5197\n",
            "Validation Loss: 550.1826\n",
            "Epoch [92170/100000]\n",
            "Training Loss: 442.2769\n",
            "Validation Loss: 548.5500\n",
            "Epoch [92180/100000]\n",
            "Training Loss: 438.0015\n",
            "Validation Loss: 551.5030\n",
            "Epoch [92190/100000]\n",
            "Training Loss: 437.2411\n",
            "Validation Loss: 553.0405\n",
            "Epoch [92200/100000]\n",
            "Training Loss: 436.5210\n",
            "Validation Loss: 548.8120\n",
            "Epoch [92210/100000]\n",
            "Training Loss: 439.9269\n",
            "Validation Loss: 552.1090\n",
            "Epoch [92220/100000]\n",
            "Training Loss: 436.7657\n",
            "Validation Loss: 549.1152\n",
            "Epoch [92230/100000]\n",
            "Training Loss: 438.9174\n",
            "Validation Loss: 553.2569\n",
            "Epoch [92240/100000]\n",
            "Training Loss: 443.0559\n",
            "Validation Loss: 551.1216\n",
            "Epoch [92250/100000]\n",
            "Training Loss: 434.6986\n",
            "Validation Loss: 552.5463\n",
            "Epoch [92260/100000]\n",
            "Training Loss: 435.9859\n",
            "Validation Loss: 552.7935\n",
            "Epoch [92270/100000]\n",
            "Training Loss: 441.1774\n",
            "Validation Loss: 549.0281\n",
            "Epoch [92280/100000]\n",
            "Training Loss: 436.3050\n",
            "Validation Loss: 550.8159\n",
            "Epoch [92290/100000]\n",
            "Training Loss: 438.3337\n",
            "Validation Loss: 549.7824\n",
            "Epoch [92300/100000]\n",
            "Training Loss: 439.1002\n",
            "Validation Loss: 553.4768\n",
            "Epoch [92310/100000]\n",
            "Training Loss: 447.2053\n",
            "Validation Loss: 547.5406\n",
            "Epoch [92320/100000]\n",
            "Training Loss: 438.0530\n",
            "Validation Loss: 550.8949\n",
            "Epoch [92330/100000]\n",
            "Training Loss: 436.2036\n",
            "Validation Loss: 550.3783\n",
            "Epoch [92340/100000]\n",
            "Training Loss: 440.7705\n",
            "Validation Loss: 550.3411\n",
            "Epoch [92350/100000]\n",
            "Training Loss: 437.5438\n",
            "Validation Loss: 550.5865\n",
            "Epoch [92360/100000]\n",
            "Training Loss: 438.6939\n",
            "Validation Loss: 550.3803\n",
            "Epoch [92370/100000]\n",
            "Training Loss: 438.5637\n",
            "Validation Loss: 550.8344\n",
            "Epoch [92380/100000]\n",
            "Training Loss: 439.8056\n",
            "Validation Loss: 548.9130\n",
            "Epoch [92390/100000]\n",
            "Training Loss: 438.9632\n",
            "Validation Loss: 551.6578\n",
            "Epoch [92400/100000]\n",
            "Training Loss: 443.8572\n",
            "Validation Loss: 548.9852\n",
            "Epoch [92410/100000]\n",
            "Training Loss: 439.7837\n",
            "Validation Loss: 549.8263\n",
            "Epoch [92420/100000]\n",
            "Training Loss: 438.0220\n",
            "Validation Loss: 548.4652\n",
            "Epoch [92430/100000]\n",
            "Training Loss: 438.5131\n",
            "Validation Loss: 551.4583\n",
            "Epoch [92440/100000]\n",
            "Training Loss: 438.9075\n",
            "Validation Loss: 550.4041\n",
            "Epoch [92450/100000]\n",
            "Training Loss: 436.8118\n",
            "Validation Loss: 550.3414\n",
            "Epoch [92460/100000]\n",
            "Training Loss: 434.6534\n",
            "Validation Loss: 549.5477\n",
            "Epoch [92470/100000]\n",
            "Training Loss: 441.4506\n",
            "Validation Loss: 549.2831\n",
            "Epoch [92480/100000]\n",
            "Training Loss: 435.4799\n",
            "Validation Loss: 550.0344\n",
            "Epoch [92490/100000]\n",
            "Training Loss: 446.3583\n",
            "Validation Loss: 548.7289\n",
            "Epoch [92500/100000]\n",
            "Training Loss: 438.8056\n",
            "Validation Loss: 552.1671\n",
            "Epoch [92510/100000]\n",
            "Training Loss: 438.4398\n",
            "Validation Loss: 550.9984\n",
            "Epoch [92520/100000]\n",
            "Training Loss: 437.2466\n",
            "Validation Loss: 549.1530\n",
            "Epoch [92530/100000]\n",
            "Training Loss: 438.1559\n",
            "Validation Loss: 551.7173\n",
            "Epoch [92540/100000]\n",
            "Training Loss: 439.1206\n",
            "Validation Loss: 553.1330\n",
            "Epoch [92550/100000]\n",
            "Training Loss: 437.9235\n",
            "Validation Loss: 551.7255\n",
            "Epoch [92560/100000]\n",
            "Training Loss: 435.8143\n",
            "Validation Loss: 550.4584\n",
            "Epoch [92570/100000]\n",
            "Training Loss: 442.7557\n",
            "Validation Loss: 552.3198\n",
            "Epoch [92580/100000]\n",
            "Training Loss: 436.2166\n",
            "Validation Loss: 552.0041\n",
            "Epoch [92590/100000]\n",
            "Training Loss: 439.0400\n",
            "Validation Loss: 547.6856\n",
            "Epoch [92600/100000]\n",
            "Training Loss: 438.0327\n",
            "Validation Loss: 544.5714\n",
            "Epoch [92610/100000]\n",
            "Training Loss: 438.7516\n",
            "Validation Loss: 548.5060\n",
            "Epoch [92620/100000]\n",
            "Training Loss: 437.0215\n",
            "Validation Loss: 547.8597\n",
            "Epoch [92630/100000]\n",
            "Training Loss: 440.8300\n",
            "Validation Loss: 549.8951\n",
            "Epoch [92640/100000]\n",
            "Training Loss: 438.6740\n",
            "Validation Loss: 553.5234\n",
            "Epoch [92650/100000]\n",
            "Training Loss: 438.6643\n",
            "Validation Loss: 553.4708\n",
            "Epoch [92660/100000]\n",
            "Training Loss: 443.3438\n",
            "Validation Loss: 553.5956\n",
            "Epoch [92670/100000]\n",
            "Training Loss: 436.1100\n",
            "Validation Loss: 550.3037\n",
            "Epoch [92680/100000]\n",
            "Training Loss: 442.6178\n",
            "Validation Loss: 548.3715\n",
            "Epoch [92690/100000]\n",
            "Training Loss: 438.8548\n",
            "Validation Loss: 552.9281\n",
            "Epoch [92700/100000]\n",
            "Training Loss: 440.5396\n",
            "Validation Loss: 546.7093\n",
            "Epoch [92710/100000]\n",
            "Training Loss: 444.6652\n",
            "Validation Loss: 546.0883\n",
            "Epoch [92720/100000]\n",
            "Training Loss: 443.3351\n",
            "Validation Loss: 553.4826\n",
            "Epoch [92730/100000]\n",
            "Training Loss: 432.5285\n",
            "Validation Loss: 556.3058\n",
            "Epoch [92740/100000]\n",
            "Training Loss: 441.9273\n",
            "Validation Loss: 556.7922\n",
            "Epoch [92750/100000]\n",
            "Training Loss: 447.5977\n",
            "Validation Loss: 550.0684\n",
            "Epoch [92760/100000]\n",
            "Training Loss: 438.0573\n",
            "Validation Loss: 550.5320\n",
            "Epoch [92770/100000]\n",
            "Training Loss: 439.8758\n",
            "Validation Loss: 550.0850\n",
            "Epoch [92780/100000]\n",
            "Training Loss: 439.3381\n",
            "Validation Loss: 551.9147\n",
            "Epoch [92790/100000]\n",
            "Training Loss: 441.4683\n",
            "Validation Loss: 550.6155\n",
            "Epoch [92800/100000]\n",
            "Training Loss: 434.2735\n",
            "Validation Loss: 550.3854\n",
            "Epoch [92810/100000]\n",
            "Training Loss: 439.4425\n",
            "Validation Loss: 548.4939\n",
            "Epoch [92820/100000]\n",
            "Training Loss: 434.2861\n",
            "Validation Loss: 553.7870\n",
            "Epoch [92830/100000]\n",
            "Training Loss: 442.5647\n",
            "Validation Loss: 552.3097\n",
            "Epoch [92840/100000]\n",
            "Training Loss: 437.2402\n",
            "Validation Loss: 550.5732\n",
            "Epoch [92850/100000]\n",
            "Training Loss: 435.6631\n",
            "Validation Loss: 550.5053\n",
            "Epoch [92860/100000]\n",
            "Training Loss: 436.2094\n",
            "Validation Loss: 553.5249\n",
            "Epoch [92870/100000]\n",
            "Training Loss: 442.9781\n",
            "Validation Loss: 550.1617\n",
            "Epoch [92880/100000]\n",
            "Training Loss: 435.9004\n",
            "Validation Loss: 549.2418\n",
            "Epoch [92890/100000]\n",
            "Training Loss: 435.9183\n",
            "Validation Loss: 550.7067\n",
            "Epoch [92900/100000]\n",
            "Training Loss: 433.8309\n",
            "Validation Loss: 553.0687\n",
            "Epoch [92910/100000]\n",
            "Training Loss: 443.9993\n",
            "Validation Loss: 548.2952\n",
            "Epoch [92920/100000]\n",
            "Training Loss: 436.3101\n",
            "Validation Loss: 551.3506\n",
            "Epoch [92930/100000]\n",
            "Training Loss: 437.9568\n",
            "Validation Loss: 548.6312\n",
            "Epoch [92940/100000]\n",
            "Training Loss: 438.2438\n",
            "Validation Loss: 550.1721\n",
            "Epoch [92950/100000]\n",
            "Training Loss: 433.6752\n",
            "Validation Loss: 549.9330\n",
            "Epoch [92960/100000]\n",
            "Training Loss: 444.2112\n",
            "Validation Loss: 553.2834\n",
            "Epoch [92970/100000]\n",
            "Training Loss: 436.1332\n",
            "Validation Loss: 550.1046\n",
            "Epoch [92980/100000]\n",
            "Training Loss: 438.9876\n",
            "Validation Loss: 550.1924\n",
            "Epoch [92990/100000]\n",
            "Training Loss: 436.8183\n",
            "Validation Loss: 552.2610\n",
            "Epoch [93000/100000]\n",
            "Training Loss: 440.7649\n",
            "Validation Loss: 549.5551\n",
            "Epoch [93010/100000]\n",
            "Training Loss: 450.2657\n",
            "Validation Loss: 545.0812\n",
            "Epoch [93020/100000]\n",
            "Training Loss: 444.4341\n",
            "Validation Loss: 550.3624\n",
            "Epoch [93030/100000]\n",
            "Training Loss: 442.6136\n",
            "Validation Loss: 552.2515\n",
            "Epoch [93040/100000]\n",
            "Training Loss: 439.9122\n",
            "Validation Loss: 550.9512\n",
            "Epoch [93050/100000]\n",
            "Training Loss: 437.9559\n",
            "Validation Loss: 550.3506\n",
            "Epoch [93060/100000]\n",
            "Training Loss: 432.6523\n",
            "Validation Loss: 551.9667\n",
            "Epoch [93070/100000]\n",
            "Training Loss: 436.6345\n",
            "Validation Loss: 551.5235\n",
            "Epoch [93080/100000]\n",
            "Training Loss: 442.4265\n",
            "Validation Loss: 553.2429\n",
            "Epoch [93090/100000]\n",
            "Training Loss: 437.8937\n",
            "Validation Loss: 551.8273\n",
            "Epoch [93100/100000]\n",
            "Training Loss: 437.6251\n",
            "Validation Loss: 547.1233\n",
            "Epoch [93110/100000]\n",
            "Training Loss: 438.7279\n",
            "Validation Loss: 551.9084\n",
            "Epoch [93120/100000]\n",
            "Training Loss: 433.0125\n",
            "Validation Loss: 552.0319\n",
            "Epoch [93130/100000]\n",
            "Training Loss: 437.0233\n",
            "Validation Loss: 550.7774\n",
            "Epoch [93140/100000]\n",
            "Training Loss: 443.1028\n",
            "Validation Loss: 550.2550\n",
            "Epoch [93150/100000]\n",
            "Training Loss: 440.8061\n",
            "Validation Loss: 548.2717\n",
            "Epoch [93160/100000]\n",
            "Training Loss: 434.7053\n",
            "Validation Loss: 549.0097\n",
            "Epoch [93170/100000]\n",
            "Training Loss: 442.1008\n",
            "Validation Loss: 552.5178\n",
            "Epoch [93180/100000]\n",
            "Training Loss: 442.0663\n",
            "Validation Loss: 550.7031\n",
            "Epoch [93190/100000]\n",
            "Training Loss: 440.4273\n",
            "Validation Loss: 547.6553\n",
            "Epoch [93200/100000]\n",
            "Training Loss: 437.1276\n",
            "Validation Loss: 550.9947\n",
            "Epoch [93210/100000]\n",
            "Training Loss: 433.4587\n",
            "Validation Loss: 551.8479\n",
            "Epoch [93220/100000]\n",
            "Training Loss: 437.5067\n",
            "Validation Loss: 550.3336\n",
            "Epoch [93230/100000]\n",
            "Training Loss: 443.1496\n",
            "Validation Loss: 550.3318\n",
            "Epoch [93240/100000]\n",
            "Training Loss: 446.1039\n",
            "Validation Loss: 546.8697\n",
            "Epoch [93250/100000]\n",
            "Training Loss: 445.8550\n",
            "Validation Loss: 548.9019\n",
            "Epoch [93260/100000]\n",
            "Training Loss: 444.4924\n",
            "Validation Loss: 548.1243\n",
            "Epoch [93270/100000]\n",
            "Training Loss: 441.3468\n",
            "Validation Loss: 549.5039\n",
            "Epoch [93280/100000]\n",
            "Training Loss: 444.1184\n",
            "Validation Loss: 547.4188\n",
            "Epoch [93290/100000]\n",
            "Training Loss: 442.3358\n",
            "Validation Loss: 546.8974\n",
            "Epoch [93300/100000]\n",
            "Training Loss: 445.5143\n",
            "Validation Loss: 542.6052\n",
            "Epoch [93310/100000]\n",
            "Training Loss: 451.9695\n",
            "Validation Loss: 546.4570\n",
            "Epoch [93320/100000]\n",
            "Training Loss: 447.6599\n",
            "Validation Loss: 545.3882\n",
            "Epoch [93330/100000]\n",
            "Training Loss: 441.1864\n",
            "Validation Loss: 550.2731\n",
            "Epoch [93340/100000]\n",
            "Training Loss: 441.7705\n",
            "Validation Loss: 545.2052\n",
            "Epoch [93350/100000]\n",
            "Training Loss: 443.9770\n",
            "Validation Loss: 548.3199\n",
            "Epoch [93360/100000]\n",
            "Training Loss: 440.0986\n",
            "Validation Loss: 546.2364\n",
            "Epoch [93370/100000]\n",
            "Training Loss: 439.7261\n",
            "Validation Loss: 549.2257\n",
            "Epoch [93380/100000]\n",
            "Training Loss: 437.2254\n",
            "Validation Loss: 552.7704\n",
            "Epoch [93390/100000]\n",
            "Training Loss: 437.3336\n",
            "Validation Loss: 551.5377\n",
            "Epoch [93400/100000]\n",
            "Training Loss: 442.9307\n",
            "Validation Loss: 549.0410\n",
            "Epoch [93410/100000]\n",
            "Training Loss: 440.2081\n",
            "Validation Loss: 551.1791\n",
            "Epoch [93420/100000]\n",
            "Training Loss: 436.9316\n",
            "Validation Loss: 554.2073\n",
            "Epoch [93430/100000]\n",
            "Training Loss: 439.7427\n",
            "Validation Loss: 552.1639\n",
            "Epoch [93440/100000]\n",
            "Training Loss: 443.0142\n",
            "Validation Loss: 550.0332\n",
            "Epoch [93450/100000]\n",
            "Training Loss: 440.7450\n",
            "Validation Loss: 549.2837\n",
            "Epoch [93460/100000]\n",
            "Training Loss: 435.6893\n",
            "Validation Loss: 557.6061\n",
            "Epoch [93470/100000]\n",
            "Training Loss: 441.4127\n",
            "Validation Loss: 549.0999\n",
            "Epoch [93480/100000]\n",
            "Training Loss: 441.0491\n",
            "Validation Loss: 545.4061\n",
            "Epoch [93490/100000]\n",
            "Training Loss: 441.1975\n",
            "Validation Loss: 554.6658\n",
            "Epoch [93500/100000]\n",
            "Training Loss: 434.2460\n",
            "Validation Loss: 548.3249\n",
            "Epoch [93510/100000]\n",
            "Training Loss: 436.6349\n",
            "Validation Loss: 550.1297\n",
            "Epoch [93520/100000]\n",
            "Training Loss: 439.6278\n",
            "Validation Loss: 550.9340\n",
            "Epoch [93530/100000]\n",
            "Training Loss: 437.6606\n",
            "Validation Loss: 547.4585\n",
            "Epoch [93540/100000]\n",
            "Training Loss: 439.5415\n",
            "Validation Loss: 552.0836\n",
            "Epoch [93550/100000]\n",
            "Training Loss: 434.3508\n",
            "Validation Loss: 550.5183\n",
            "Epoch [93560/100000]\n",
            "Training Loss: 435.7581\n",
            "Validation Loss: 552.0294\n",
            "Epoch [93570/100000]\n",
            "Training Loss: 434.9605\n",
            "Validation Loss: 552.0562\n",
            "Epoch [93580/100000]\n",
            "Training Loss: 434.1675\n",
            "Validation Loss: 550.9526\n",
            "Epoch [93590/100000]\n",
            "Training Loss: 433.1036\n",
            "Validation Loss: 550.4729\n",
            "Epoch [93600/100000]\n",
            "Training Loss: 437.3407\n",
            "Validation Loss: 552.8473\n",
            "Epoch [93610/100000]\n",
            "Training Loss: 437.8060\n",
            "Validation Loss: 551.1303\n",
            "Epoch [93620/100000]\n",
            "Training Loss: 437.3349\n",
            "Validation Loss: 550.4045\n",
            "Epoch [93630/100000]\n",
            "Training Loss: 436.9433\n",
            "Validation Loss: 556.0827\n",
            "Epoch [93640/100000]\n",
            "Training Loss: 442.0480\n",
            "Validation Loss: 551.2422\n",
            "Epoch [93650/100000]\n",
            "Training Loss: 438.5165\n",
            "Validation Loss: 552.3302\n",
            "Epoch [93660/100000]\n",
            "Training Loss: 437.6095\n",
            "Validation Loss: 551.6974\n",
            "Epoch [93670/100000]\n",
            "Training Loss: 437.3675\n",
            "Validation Loss: 550.0289\n",
            "Epoch [93680/100000]\n",
            "Training Loss: 434.7047\n",
            "Validation Loss: 553.4361\n",
            "Epoch [93690/100000]\n",
            "Training Loss: 439.3299\n",
            "Validation Loss: 550.2056\n",
            "Epoch [93700/100000]\n",
            "Training Loss: 434.7549\n",
            "Validation Loss: 552.3412\n",
            "Epoch [93710/100000]\n",
            "Training Loss: 433.8384\n",
            "Validation Loss: 551.0980\n",
            "Epoch [93720/100000]\n",
            "Training Loss: 438.4971\n",
            "Validation Loss: 550.1188\n",
            "Epoch [93730/100000]\n",
            "Training Loss: 436.2991\n",
            "Validation Loss: 550.0806\n",
            "Epoch [93740/100000]\n",
            "Training Loss: 438.7262\n",
            "Validation Loss: 551.9718\n",
            "Epoch [93750/100000]\n",
            "Training Loss: 433.8061\n",
            "Validation Loss: 551.2924\n",
            "Epoch [93760/100000]\n",
            "Training Loss: 435.7330\n",
            "Validation Loss: 550.6562\n",
            "Epoch [93770/100000]\n",
            "Training Loss: 436.4059\n",
            "Validation Loss: 554.2771\n",
            "Epoch [93780/100000]\n",
            "Training Loss: 448.2526\n",
            "Validation Loss: 548.0244\n",
            "Epoch [93790/100000]\n",
            "Training Loss: 438.4767\n",
            "Validation Loss: 549.6591\n",
            "Epoch [93800/100000]\n",
            "Training Loss: 435.4866\n",
            "Validation Loss: 552.7720\n",
            "Epoch [93810/100000]\n",
            "Training Loss: 435.1101\n",
            "Validation Loss: 550.6235\n",
            "Epoch [93820/100000]\n",
            "Training Loss: 432.7803\n",
            "Validation Loss: 552.0432\n",
            "Epoch [93830/100000]\n",
            "Training Loss: 438.0395\n",
            "Validation Loss: 546.9728\n",
            "Epoch [93840/100000]\n",
            "Training Loss: 435.4873\n",
            "Validation Loss: 553.7177\n",
            "Epoch [93850/100000]\n",
            "Training Loss: 441.1236\n",
            "Validation Loss: 551.0291\n",
            "Epoch [93860/100000]\n",
            "Training Loss: 440.3943\n",
            "Validation Loss: 548.1175\n",
            "Epoch [93870/100000]\n",
            "Training Loss: 441.2549\n",
            "Validation Loss: 548.5895\n",
            "Epoch [93880/100000]\n",
            "Training Loss: 433.2826\n",
            "Validation Loss: 550.7036\n",
            "Epoch [93890/100000]\n",
            "Training Loss: 438.9648\n",
            "Validation Loss: 552.5648\n",
            "Epoch [93900/100000]\n",
            "Training Loss: 437.3180\n",
            "Validation Loss: 552.2259\n",
            "Epoch [93910/100000]\n",
            "Training Loss: 436.4772\n",
            "Validation Loss: 551.8538\n",
            "Epoch [93920/100000]\n",
            "Training Loss: 440.1840\n",
            "Validation Loss: 548.4436\n",
            "Epoch [93930/100000]\n",
            "Training Loss: 445.2707\n",
            "Validation Loss: 549.1317\n",
            "Epoch [93940/100000]\n",
            "Training Loss: 440.7663\n",
            "Validation Loss: 549.8710\n",
            "Epoch [93950/100000]\n",
            "Training Loss: 437.0261\n",
            "Validation Loss: 552.3001\n",
            "Epoch [93960/100000]\n",
            "Training Loss: 437.3143\n",
            "Validation Loss: 552.1644\n",
            "Epoch [93970/100000]\n",
            "Training Loss: 437.2781\n",
            "Validation Loss: 552.8522\n",
            "Epoch [93980/100000]\n",
            "Training Loss: 434.0695\n",
            "Validation Loss: 554.3351\n",
            "Epoch [93990/100000]\n",
            "Training Loss: 438.1938\n",
            "Validation Loss: 551.2902\n",
            "Epoch [94000/100000]\n",
            "Training Loss: 442.8535\n",
            "Validation Loss: 547.0278\n",
            "Epoch [94010/100000]\n",
            "Training Loss: 434.5963\n",
            "Validation Loss: 547.5190\n",
            "Epoch [94020/100000]\n",
            "Training Loss: 433.4248\n",
            "Validation Loss: 548.9389\n",
            "Epoch [94030/100000]\n",
            "Training Loss: 440.1432\n",
            "Validation Loss: 548.3635\n",
            "Epoch [94040/100000]\n",
            "Training Loss: 433.3250\n",
            "Validation Loss: 554.5342\n",
            "Epoch [94050/100000]\n",
            "Training Loss: 444.4356\n",
            "Validation Loss: 548.0131\n",
            "Epoch [94060/100000]\n",
            "Training Loss: 433.6841\n",
            "Validation Loss: 551.0201\n",
            "Epoch [94070/100000]\n",
            "Training Loss: 436.3203\n",
            "Validation Loss: 548.2282\n",
            "Epoch [94080/100000]\n",
            "Training Loss: 434.0345\n",
            "Validation Loss: 550.5133\n",
            "Epoch [94090/100000]\n",
            "Training Loss: 434.8716\n",
            "Validation Loss: 550.7570\n",
            "Epoch [94100/100000]\n",
            "Training Loss: 439.8934\n",
            "Validation Loss: 560.2838\n",
            "Epoch [94110/100000]\n",
            "Training Loss: 441.0222\n",
            "Validation Loss: 550.6102\n",
            "Epoch [94120/100000]\n",
            "Training Loss: 439.1806\n",
            "Validation Loss: 550.3552\n",
            "Epoch [94130/100000]\n",
            "Training Loss: 433.8548\n",
            "Validation Loss: 551.5063\n",
            "Epoch [94140/100000]\n",
            "Training Loss: 435.6575\n",
            "Validation Loss: 552.4134\n",
            "Epoch [94150/100000]\n",
            "Training Loss: 435.0521\n",
            "Validation Loss: 549.8112\n",
            "Epoch [94160/100000]\n",
            "Training Loss: 432.7520\n",
            "Validation Loss: 552.3128\n",
            "Epoch [94170/100000]\n",
            "Training Loss: 438.1290\n",
            "Validation Loss: 554.8654\n",
            "Epoch [94180/100000]\n",
            "Training Loss: 437.2137\n",
            "Validation Loss: 550.6314\n",
            "Epoch [94190/100000]\n",
            "Training Loss: 435.9190\n",
            "Validation Loss: 551.7463\n",
            "Epoch [94200/100000]\n",
            "Training Loss: 438.7250\n",
            "Validation Loss: 549.4044\n",
            "Epoch [94210/100000]\n",
            "Training Loss: 435.8630\n",
            "Validation Loss: 546.6894\n",
            "Epoch [94220/100000]\n",
            "Training Loss: 434.9058\n",
            "Validation Loss: 550.9868\n",
            "Epoch [94230/100000]\n",
            "Training Loss: 434.0847\n",
            "Validation Loss: 549.1228\n",
            "Epoch [94240/100000]\n",
            "Training Loss: 441.6046\n",
            "Validation Loss: 546.2797\n",
            "Epoch [94250/100000]\n",
            "Training Loss: 434.1203\n",
            "Validation Loss: 550.9967\n",
            "Epoch [94260/100000]\n",
            "Training Loss: 432.9014\n",
            "Validation Loss: 551.1019\n",
            "Epoch [94270/100000]\n",
            "Training Loss: 434.0850\n",
            "Validation Loss: 550.7845\n",
            "Epoch [94280/100000]\n",
            "Training Loss: 437.9841\n",
            "Validation Loss: 549.0182\n",
            "Epoch [94290/100000]\n",
            "Training Loss: 438.9839\n",
            "Validation Loss: 551.8990\n",
            "Epoch [94300/100000]\n",
            "Training Loss: 439.7294\n",
            "Validation Loss: 551.6074\n",
            "Epoch [94310/100000]\n",
            "Training Loss: 436.4440\n",
            "Validation Loss: 549.8939\n",
            "Epoch [94320/100000]\n",
            "Training Loss: 435.1994\n",
            "Validation Loss: 547.0112\n",
            "Epoch [94330/100000]\n",
            "Training Loss: 437.9858\n",
            "Validation Loss: 547.0537\n",
            "Epoch [94340/100000]\n",
            "Training Loss: 438.0556\n",
            "Validation Loss: 551.6081\n",
            "Epoch [94350/100000]\n",
            "Training Loss: 434.8137\n",
            "Validation Loss: 549.5670\n",
            "Epoch [94360/100000]\n",
            "Training Loss: 432.3497\n",
            "Validation Loss: 552.6102\n",
            "Epoch [94370/100000]\n",
            "Training Loss: 436.3012\n",
            "Validation Loss: 550.6511\n",
            "Epoch [94380/100000]\n",
            "Training Loss: 442.8094\n",
            "Validation Loss: 548.7871\n",
            "Epoch [94390/100000]\n",
            "Training Loss: 438.7636\n",
            "Validation Loss: 548.2578\n",
            "Epoch [94400/100000]\n",
            "Training Loss: 436.5758\n",
            "Validation Loss: 548.1255\n",
            "Epoch [94410/100000]\n",
            "Training Loss: 439.1718\n",
            "Validation Loss: 548.4771\n",
            "Epoch [94420/100000]\n",
            "Training Loss: 443.9990\n",
            "Validation Loss: 548.8242\n",
            "Epoch [94430/100000]\n",
            "Training Loss: 436.5590\n",
            "Validation Loss: 553.7941\n",
            "Epoch [94440/100000]\n",
            "Training Loss: 439.9744\n",
            "Validation Loss: 549.3966\n",
            "Epoch [94450/100000]\n",
            "Training Loss: 437.7784\n",
            "Validation Loss: 551.3425\n",
            "Epoch [94460/100000]\n",
            "Training Loss: 443.4538\n",
            "Validation Loss: 548.1541\n",
            "Epoch [94470/100000]\n",
            "Training Loss: 437.6754\n",
            "Validation Loss: 554.5204\n",
            "Epoch [94480/100000]\n",
            "Training Loss: 438.7756\n",
            "Validation Loss: 549.5609\n",
            "Epoch [94490/100000]\n",
            "Training Loss: 441.3976\n",
            "Validation Loss: 550.9191\n",
            "Epoch [94500/100000]\n",
            "Training Loss: 434.5334\n",
            "Validation Loss: 554.6299\n",
            "Epoch [94510/100000]\n",
            "Training Loss: 437.9164\n",
            "Validation Loss: 548.7090\n",
            "Epoch [94520/100000]\n",
            "Training Loss: 441.0908\n",
            "Validation Loss: 551.1696\n",
            "Epoch [94530/100000]\n",
            "Training Loss: 439.8391\n",
            "Validation Loss: 551.2854\n",
            "Epoch [94540/100000]\n",
            "Training Loss: 438.6688\n",
            "Validation Loss: 549.2966\n",
            "Epoch [94550/100000]\n",
            "Training Loss: 437.6614\n",
            "Validation Loss: 549.3410\n",
            "Epoch [94560/100000]\n",
            "Training Loss: 437.6649\n",
            "Validation Loss: 549.3205\n",
            "Epoch [94570/100000]\n",
            "Training Loss: 438.4371\n",
            "Validation Loss: 551.6719\n",
            "Epoch [94580/100000]\n",
            "Training Loss: 441.6161\n",
            "Validation Loss: 548.8173\n",
            "Epoch [94590/100000]\n",
            "Training Loss: 434.5779\n",
            "Validation Loss: 549.3271\n",
            "Epoch [94600/100000]\n",
            "Training Loss: 434.7359\n",
            "Validation Loss: 553.0577\n",
            "Epoch [94610/100000]\n",
            "Training Loss: 440.8578\n",
            "Validation Loss: 553.0785\n",
            "Epoch [94620/100000]\n",
            "Training Loss: 433.2919\n",
            "Validation Loss: 549.4548\n",
            "Epoch [94630/100000]\n",
            "Training Loss: 437.0122\n",
            "Validation Loss: 553.4903\n",
            "Epoch [94640/100000]\n",
            "Training Loss: 439.8645\n",
            "Validation Loss: 549.9658\n",
            "Epoch [94650/100000]\n",
            "Training Loss: 436.9421\n",
            "Validation Loss: 552.3501\n",
            "Epoch [94660/100000]\n",
            "Training Loss: 438.8315\n",
            "Validation Loss: 551.6316\n",
            "Epoch [94670/100000]\n",
            "Training Loss: 433.6115\n",
            "Validation Loss: 550.0594\n",
            "Epoch [94680/100000]\n",
            "Training Loss: 439.3521\n",
            "Validation Loss: 550.0099\n",
            "Epoch [94690/100000]\n",
            "Training Loss: 437.3966\n",
            "Validation Loss: 549.7457\n",
            "Epoch [94700/100000]\n",
            "Training Loss: 434.1385\n",
            "Validation Loss: 550.1466\n",
            "Epoch [94710/100000]\n",
            "Training Loss: 439.1563\n",
            "Validation Loss: 553.1824\n",
            "Epoch [94720/100000]\n",
            "Training Loss: 435.9239\n",
            "Validation Loss: 549.8713\n",
            "Epoch [94730/100000]\n",
            "Training Loss: 441.6712\n",
            "Validation Loss: 544.5667\n",
            "Epoch [94740/100000]\n",
            "Training Loss: 441.9784\n",
            "Validation Loss: 550.7469\n",
            "Epoch [94750/100000]\n",
            "Training Loss: 439.7906\n",
            "Validation Loss: 549.3991\n",
            "Epoch [94760/100000]\n",
            "Training Loss: 433.9254\n",
            "Validation Loss: 554.6664\n",
            "Epoch [94770/100000]\n",
            "Training Loss: 436.9004\n",
            "Validation Loss: 549.0931\n",
            "Epoch [94780/100000]\n",
            "Training Loss: 431.8030\n",
            "Validation Loss: 551.2171\n",
            "Epoch [94790/100000]\n",
            "Training Loss: 431.5908\n",
            "Validation Loss: 548.4993\n",
            "Epoch [94800/100000]\n",
            "Training Loss: 432.9908\n",
            "Validation Loss: 552.4396\n",
            "Epoch [94810/100000]\n",
            "Training Loss: 435.2713\n",
            "Validation Loss: 553.6958\n",
            "Epoch [94820/100000]\n",
            "Training Loss: 440.7487\n",
            "Validation Loss: 553.3785\n",
            "Epoch [94830/100000]\n",
            "Training Loss: 442.8867\n",
            "Validation Loss: 551.4225\n",
            "Epoch [94840/100000]\n",
            "Training Loss: 450.7141\n",
            "Validation Loss: 549.1284\n",
            "Epoch [94850/100000]\n",
            "Training Loss: 440.3627\n",
            "Validation Loss: 547.1887\n",
            "Epoch [94860/100000]\n",
            "Training Loss: 443.0436\n",
            "Validation Loss: 546.1367\n",
            "Epoch [94870/100000]\n",
            "Training Loss: 443.1877\n",
            "Validation Loss: 544.6902\n",
            "Epoch [94880/100000]\n",
            "Training Loss: 439.8601\n",
            "Validation Loss: 546.1992\n",
            "Epoch [94890/100000]\n",
            "Training Loss: 434.9170\n",
            "Validation Loss: 548.1998\n",
            "Epoch [94900/100000]\n",
            "Training Loss: 433.1591\n",
            "Validation Loss: 551.4041\n",
            "Epoch [94910/100000]\n",
            "Training Loss: 437.8765\n",
            "Validation Loss: 549.0284\n",
            "Epoch [94920/100000]\n",
            "Training Loss: 436.2719\n",
            "Validation Loss: 549.4288\n",
            "Epoch [94930/100000]\n",
            "Training Loss: 438.7634\n",
            "Validation Loss: 551.1832\n",
            "Epoch [94940/100000]\n",
            "Training Loss: 435.8785\n",
            "Validation Loss: 550.4829\n",
            "Epoch [94950/100000]\n",
            "Training Loss: 434.9186\n",
            "Validation Loss: 553.0302\n",
            "Epoch [94960/100000]\n",
            "Training Loss: 435.1569\n",
            "Validation Loss: 552.8728\n",
            "Epoch [94970/100000]\n",
            "Training Loss: 437.8388\n",
            "Validation Loss: 549.9297\n",
            "Epoch [94980/100000]\n",
            "Training Loss: 433.2022\n",
            "Validation Loss: 550.2816\n",
            "Epoch [94990/100000]\n",
            "Training Loss: 433.1187\n",
            "Validation Loss: 550.5214\n",
            "Epoch [95000/100000]\n",
            "Training Loss: 434.3512\n",
            "Validation Loss: 553.8060\n",
            "Epoch [95010/100000]\n",
            "Training Loss: 434.6502\n",
            "Validation Loss: 554.2791\n",
            "Epoch [95020/100000]\n",
            "Training Loss: 434.3351\n",
            "Validation Loss: 551.9811\n",
            "Epoch [95030/100000]\n",
            "Training Loss: 435.3806\n",
            "Validation Loss: 554.4852\n",
            "Epoch [95040/100000]\n",
            "Training Loss: 434.4158\n",
            "Validation Loss: 551.9238\n",
            "Epoch [95050/100000]\n",
            "Training Loss: 437.0766\n",
            "Validation Loss: 553.9529\n",
            "Epoch [95060/100000]\n",
            "Training Loss: 438.9586\n",
            "Validation Loss: 548.7372\n",
            "Epoch [95070/100000]\n",
            "Training Loss: 427.3440\n",
            "Validation Loss: 552.0327\n",
            "Epoch [95080/100000]\n",
            "Training Loss: 436.6666\n",
            "Validation Loss: 551.2443\n",
            "Epoch [95090/100000]\n",
            "Training Loss: 439.5627\n",
            "Validation Loss: 553.6515\n",
            "Epoch [95100/100000]\n",
            "Training Loss: 438.8261\n",
            "Validation Loss: 547.3818\n",
            "Epoch [95110/100000]\n",
            "Training Loss: 439.5808\n",
            "Validation Loss: 543.5538\n",
            "Epoch [95120/100000]\n",
            "Training Loss: 438.2344\n",
            "Validation Loss: 550.7628\n",
            "Epoch [95130/100000]\n",
            "Training Loss: 436.2367\n",
            "Validation Loss: 549.6667\n",
            "Epoch [95140/100000]\n",
            "Training Loss: 434.2102\n",
            "Validation Loss: 552.9326\n",
            "Epoch [95150/100000]\n",
            "Training Loss: 432.8341\n",
            "Validation Loss: 551.3137\n",
            "Epoch [95160/100000]\n",
            "Training Loss: 436.3667\n",
            "Validation Loss: 551.7605\n",
            "Epoch [95170/100000]\n",
            "Training Loss: 434.8918\n",
            "Validation Loss: 549.8821\n",
            "Epoch [95180/100000]\n",
            "Training Loss: 435.0934\n",
            "Validation Loss: 552.7725\n",
            "Epoch [95190/100000]\n",
            "Training Loss: 433.0973\n",
            "Validation Loss: 548.7184\n",
            "Epoch [95200/100000]\n",
            "Training Loss: 429.3955\n",
            "Validation Loss: 552.2659\n",
            "Epoch [95210/100000]\n",
            "Training Loss: 436.8798\n",
            "Validation Loss: 550.6233\n",
            "Epoch [95220/100000]\n",
            "Training Loss: 431.9588\n",
            "Validation Loss: 548.6199\n",
            "Epoch [95230/100000]\n",
            "Training Loss: 432.5963\n",
            "Validation Loss: 552.0287\n",
            "Epoch [95240/100000]\n",
            "Training Loss: 434.4583\n",
            "Validation Loss: 549.6862\n",
            "Epoch [95250/100000]\n",
            "Training Loss: 437.2620\n",
            "Validation Loss: 549.7173\n",
            "Epoch [95260/100000]\n",
            "Training Loss: 432.2617\n",
            "Validation Loss: 552.4466\n",
            "Epoch [95270/100000]\n",
            "Training Loss: 436.9071\n",
            "Validation Loss: 550.6259\n",
            "Epoch [95280/100000]\n",
            "Training Loss: 437.9240\n",
            "Validation Loss: 549.1666\n",
            "Epoch [95290/100000]\n",
            "Training Loss: 428.7675\n",
            "Validation Loss: 551.1608\n",
            "Epoch [95300/100000]\n",
            "Training Loss: 433.0678\n",
            "Validation Loss: 553.5319\n",
            "Epoch [95310/100000]\n",
            "Training Loss: 431.2328\n",
            "Validation Loss: 548.8946\n",
            "Epoch [95320/100000]\n",
            "Training Loss: 435.3528\n",
            "Validation Loss: 549.9116\n",
            "Epoch [95330/100000]\n",
            "Training Loss: 437.5323\n",
            "Validation Loss: 550.8399\n",
            "Epoch [95340/100000]\n",
            "Training Loss: 430.7389\n",
            "Validation Loss: 549.9510\n",
            "Epoch [95350/100000]\n",
            "Training Loss: 436.3773\n",
            "Validation Loss: 548.6876\n",
            "Epoch [95360/100000]\n",
            "Training Loss: 432.1279\n",
            "Validation Loss: 552.4528\n",
            "Epoch [95370/100000]\n",
            "Training Loss: 435.9489\n",
            "Validation Loss: 550.7704\n",
            "Epoch [95380/100000]\n",
            "Training Loss: 432.7453\n",
            "Validation Loss: 554.8262\n",
            "Epoch [95390/100000]\n",
            "Training Loss: 431.6060\n",
            "Validation Loss: 551.0695\n",
            "Epoch [95400/100000]\n",
            "Training Loss: 436.2484\n",
            "Validation Loss: 551.1656\n",
            "Epoch [95410/100000]\n",
            "Training Loss: 432.2216\n",
            "Validation Loss: 552.3491\n",
            "Epoch [95420/100000]\n",
            "Training Loss: 432.4078\n",
            "Validation Loss: 548.8473\n",
            "Epoch [95430/100000]\n",
            "Training Loss: 435.8519\n",
            "Validation Loss: 548.6294\n",
            "Epoch [95440/100000]\n",
            "Training Loss: 433.2657\n",
            "Validation Loss: 550.1186\n",
            "Epoch [95450/100000]\n",
            "Training Loss: 435.6240\n",
            "Validation Loss: 551.1746\n",
            "Epoch [95460/100000]\n",
            "Training Loss: 435.9344\n",
            "Validation Loss: 551.4682\n",
            "Epoch [95470/100000]\n",
            "Training Loss: 432.8112\n",
            "Validation Loss: 552.7791\n",
            "Epoch [95480/100000]\n",
            "Training Loss: 436.3795\n",
            "Validation Loss: 554.3002\n",
            "Epoch [95490/100000]\n",
            "Training Loss: 431.7648\n",
            "Validation Loss: 550.3703\n",
            "Epoch [95500/100000]\n",
            "Training Loss: 428.9133\n",
            "Validation Loss: 549.4064\n",
            "Epoch [95510/100000]\n",
            "Training Loss: 432.2677\n",
            "Validation Loss: 549.4027\n",
            "Epoch [95520/100000]\n",
            "Training Loss: 431.8858\n",
            "Validation Loss: 549.9044\n",
            "Epoch [95530/100000]\n",
            "Training Loss: 428.3363\n",
            "Validation Loss: 548.7371\n",
            "Epoch [95540/100000]\n",
            "Training Loss: 437.8277\n",
            "Validation Loss: 555.9435\n",
            "Epoch [95550/100000]\n",
            "Training Loss: 433.7101\n",
            "Validation Loss: 548.0190\n",
            "Epoch [95560/100000]\n",
            "Training Loss: 442.1651\n",
            "Validation Loss: 548.3485\n",
            "Epoch [95570/100000]\n",
            "Training Loss: 437.4052\n",
            "Validation Loss: 549.0547\n",
            "Epoch [95580/100000]\n",
            "Training Loss: 435.1199\n",
            "Validation Loss: 550.1772\n",
            "Epoch [95590/100000]\n",
            "Training Loss: 431.5310\n",
            "Validation Loss: 551.8261\n",
            "Epoch [95600/100000]\n",
            "Training Loss: 436.6340\n",
            "Validation Loss: 554.5153\n",
            "Epoch [95610/100000]\n",
            "Training Loss: 437.9789\n",
            "Validation Loss: 546.9270\n",
            "Epoch [95620/100000]\n",
            "Training Loss: 442.2259\n",
            "Validation Loss: 549.8491\n",
            "Epoch [95630/100000]\n",
            "Training Loss: 436.1203\n",
            "Validation Loss: 547.5367\n",
            "Epoch [95640/100000]\n",
            "Training Loss: 429.5462\n",
            "Validation Loss: 549.7735\n",
            "Epoch [95650/100000]\n",
            "Training Loss: 432.8552\n",
            "Validation Loss: 551.4130\n",
            "Epoch [95660/100000]\n",
            "Training Loss: 434.3207\n",
            "Validation Loss: 553.5256\n",
            "Epoch [95670/100000]\n",
            "Training Loss: 432.6675\n",
            "Validation Loss: 552.1853\n",
            "Epoch [95680/100000]\n",
            "Training Loss: 433.8391\n",
            "Validation Loss: 552.2535\n",
            "Epoch [95690/100000]\n",
            "Training Loss: 431.8986\n",
            "Validation Loss: 550.2000\n",
            "Epoch [95700/100000]\n",
            "Training Loss: 432.8539\n",
            "Validation Loss: 549.9797\n",
            "Epoch [95710/100000]\n",
            "Training Loss: 430.5671\n",
            "Validation Loss: 550.8430\n",
            "Epoch [95720/100000]\n",
            "Training Loss: 430.9906\n",
            "Validation Loss: 549.3685\n",
            "Epoch [95730/100000]\n",
            "Training Loss: 431.6346\n",
            "Validation Loss: 550.2012\n",
            "Epoch [95740/100000]\n",
            "Training Loss: 432.9214\n",
            "Validation Loss: 551.7257\n",
            "Epoch [95750/100000]\n",
            "Training Loss: 431.3833\n",
            "Validation Loss: 551.0577\n",
            "Epoch [95760/100000]\n",
            "Training Loss: 438.5133\n",
            "Validation Loss: 547.5834\n",
            "Epoch [95770/100000]\n",
            "Training Loss: 432.5800\n",
            "Validation Loss: 551.7024\n",
            "Epoch [95780/100000]\n",
            "Training Loss: 438.1078\n",
            "Validation Loss: 548.3931\n",
            "Epoch [95790/100000]\n",
            "Training Loss: 434.3139\n",
            "Validation Loss: 550.8475\n",
            "Epoch [95800/100000]\n",
            "Training Loss: 431.7022\n",
            "Validation Loss: 553.0452\n",
            "Epoch [95810/100000]\n",
            "Training Loss: 432.4406\n",
            "Validation Loss: 549.9969\n",
            "Epoch [95820/100000]\n",
            "Training Loss: 426.4292\n",
            "Validation Loss: 551.2189\n",
            "Epoch [95830/100000]\n",
            "Training Loss: 437.7430\n",
            "Validation Loss: 552.9766\n",
            "Epoch [95840/100000]\n",
            "Training Loss: 438.8660\n",
            "Validation Loss: 548.3376\n",
            "Epoch [95850/100000]\n",
            "Training Loss: 432.0392\n",
            "Validation Loss: 547.8945\n",
            "Epoch [95860/100000]\n",
            "Training Loss: 434.0629\n",
            "Validation Loss: 552.1709\n",
            "Epoch [95870/100000]\n",
            "Training Loss: 429.7921\n",
            "Validation Loss: 557.3938\n",
            "Epoch [95880/100000]\n",
            "Training Loss: 435.1229\n",
            "Validation Loss: 557.9639\n",
            "Epoch [95890/100000]\n",
            "Training Loss: 441.1544\n",
            "Validation Loss: 547.7844\n",
            "Epoch [95900/100000]\n",
            "Training Loss: 428.9027\n",
            "Validation Loss: 551.9093\n",
            "Epoch [95910/100000]\n",
            "Training Loss: 436.6632\n",
            "Validation Loss: 550.3933\n",
            "Epoch [95920/100000]\n",
            "Training Loss: 437.1983\n",
            "Validation Loss: 549.3445\n",
            "Epoch [95930/100000]\n",
            "Training Loss: 435.9478\n",
            "Validation Loss: 550.3004\n",
            "Epoch [95940/100000]\n",
            "Training Loss: 434.4933\n",
            "Validation Loss: 547.9973\n",
            "Epoch [95950/100000]\n",
            "Training Loss: 434.5812\n",
            "Validation Loss: 548.3458\n",
            "Epoch [95960/100000]\n",
            "Training Loss: 435.3789\n",
            "Validation Loss: 549.9727\n",
            "Epoch [95970/100000]\n",
            "Training Loss: 431.8696\n",
            "Validation Loss: 548.8231\n",
            "Epoch [95980/100000]\n",
            "Training Loss: 435.8747\n",
            "Validation Loss: 550.9424\n",
            "Epoch [95990/100000]\n",
            "Training Loss: 440.0375\n",
            "Validation Loss: 550.4897\n",
            "Epoch [96000/100000]\n",
            "Training Loss: 432.2260\n",
            "Validation Loss: 552.3300\n",
            "Epoch [96010/100000]\n",
            "Training Loss: 440.1077\n",
            "Validation Loss: 551.9919\n",
            "Epoch [96020/100000]\n",
            "Training Loss: 434.8080\n",
            "Validation Loss: 553.3465\n",
            "Epoch [96030/100000]\n",
            "Training Loss: 434.5292\n",
            "Validation Loss: 547.8558\n",
            "Epoch [96040/100000]\n",
            "Training Loss: 434.2977\n",
            "Validation Loss: 549.4332\n",
            "Epoch [96050/100000]\n",
            "Training Loss: 436.4636\n",
            "Validation Loss: 549.7523\n",
            "Epoch [96060/100000]\n",
            "Training Loss: 434.5056\n",
            "Validation Loss: 553.4358\n",
            "Epoch [96070/100000]\n",
            "Training Loss: 430.8896\n",
            "Validation Loss: 550.9080\n",
            "Epoch [96080/100000]\n",
            "Training Loss: 433.1595\n",
            "Validation Loss: 549.4445\n",
            "Epoch [96090/100000]\n",
            "Training Loss: 436.6794\n",
            "Validation Loss: 545.8497\n",
            "Epoch [96100/100000]\n",
            "Training Loss: 437.2084\n",
            "Validation Loss: 546.0250\n",
            "Epoch [96110/100000]\n",
            "Training Loss: 442.5118\n",
            "Validation Loss: 550.0928\n",
            "Epoch [96120/100000]\n",
            "Training Loss: 434.1112\n",
            "Validation Loss: 550.0889\n",
            "Epoch [96130/100000]\n",
            "Training Loss: 436.9329\n",
            "Validation Loss: 544.0060\n",
            "Epoch [96140/100000]\n",
            "Training Loss: 440.5764\n",
            "Validation Loss: 546.5121\n",
            "Epoch [96150/100000]\n",
            "Training Loss: 434.1812\n",
            "Validation Loss: 551.9174\n",
            "Epoch [96160/100000]\n",
            "Training Loss: 431.6960\n",
            "Validation Loss: 551.3810\n",
            "Epoch [96170/100000]\n",
            "Training Loss: 432.8319\n",
            "Validation Loss: 551.4023\n",
            "Epoch [96180/100000]\n",
            "Training Loss: 438.0814\n",
            "Validation Loss: 549.8297\n",
            "Epoch [96190/100000]\n",
            "Training Loss: 427.8871\n",
            "Validation Loss: 548.0800\n",
            "Epoch [96200/100000]\n",
            "Training Loss: 428.8577\n",
            "Validation Loss: 550.0499\n",
            "Epoch [96210/100000]\n",
            "Training Loss: 435.8294\n",
            "Validation Loss: 547.7028\n",
            "Epoch [96220/100000]\n",
            "Training Loss: 434.4647\n",
            "Validation Loss: 547.5153\n",
            "Epoch [96230/100000]\n",
            "Training Loss: 428.2010\n",
            "Validation Loss: 550.5803\n",
            "Epoch [96240/100000]\n",
            "Training Loss: 431.2613\n",
            "Validation Loss: 548.3922\n",
            "Epoch [96250/100000]\n",
            "Training Loss: 428.7557\n",
            "Validation Loss: 552.5176\n",
            "Epoch [96260/100000]\n",
            "Training Loss: 436.3358\n",
            "Validation Loss: 549.8543\n",
            "Epoch [96270/100000]\n",
            "Training Loss: 430.8276\n",
            "Validation Loss: 551.7477\n",
            "Epoch [96280/100000]\n",
            "Training Loss: 432.2334\n",
            "Validation Loss: 548.1225\n",
            "Epoch [96290/100000]\n",
            "Training Loss: 432.6747\n",
            "Validation Loss: 546.7972\n",
            "Epoch [96300/100000]\n",
            "Training Loss: 429.1038\n",
            "Validation Loss: 550.5222\n",
            "Epoch [96310/100000]\n",
            "Training Loss: 435.6566\n",
            "Validation Loss: 550.6108\n",
            "Epoch [96320/100000]\n",
            "Training Loss: 438.6250\n",
            "Validation Loss: 548.7249\n",
            "Epoch [96330/100000]\n",
            "Training Loss: 434.7303\n",
            "Validation Loss: 549.4506\n",
            "Epoch [96340/100000]\n",
            "Training Loss: 433.6439\n",
            "Validation Loss: 550.9384\n",
            "Epoch [96350/100000]\n",
            "Training Loss: 430.9425\n",
            "Validation Loss: 552.5053\n",
            "Epoch [96360/100000]\n",
            "Training Loss: 436.9333\n",
            "Validation Loss: 550.5916\n",
            "Epoch [96370/100000]\n",
            "Training Loss: 432.2968\n",
            "Validation Loss: 548.3873\n",
            "Epoch [96380/100000]\n",
            "Training Loss: 444.0069\n",
            "Validation Loss: 550.2612\n",
            "Epoch [96390/100000]\n",
            "Training Loss: 439.1601\n",
            "Validation Loss: 551.3549\n",
            "Epoch [96400/100000]\n",
            "Training Loss: 437.3389\n",
            "Validation Loss: 550.5993\n",
            "Epoch [96410/100000]\n",
            "Training Loss: 433.3219\n",
            "Validation Loss: 548.7209\n",
            "Epoch [96420/100000]\n",
            "Training Loss: 433.2925\n",
            "Validation Loss: 550.6277\n",
            "Epoch [96430/100000]\n",
            "Training Loss: 435.8279\n",
            "Validation Loss: 546.1805\n",
            "Epoch [96440/100000]\n",
            "Training Loss: 429.9909\n",
            "Validation Loss: 552.0190\n",
            "Epoch [96450/100000]\n",
            "Training Loss: 437.6675\n",
            "Validation Loss: 549.1619\n",
            "Epoch [96460/100000]\n",
            "Training Loss: 436.5817\n",
            "Validation Loss: 547.3818\n",
            "Epoch [96470/100000]\n",
            "Training Loss: 434.0104\n",
            "Validation Loss: 549.3630\n",
            "Epoch [96480/100000]\n",
            "Training Loss: 435.0625\n",
            "Validation Loss: 550.7089\n",
            "Epoch [96490/100000]\n",
            "Training Loss: 437.0439\n",
            "Validation Loss: 550.1893\n",
            "Epoch [96500/100000]\n",
            "Training Loss: 428.5073\n",
            "Validation Loss: 548.5417\n",
            "Epoch [96510/100000]\n",
            "Training Loss: 431.6096\n",
            "Validation Loss: 552.4259\n",
            "Epoch [96520/100000]\n",
            "Training Loss: 435.0948\n",
            "Validation Loss: 548.7705\n",
            "Epoch [96530/100000]\n",
            "Training Loss: 429.1569\n",
            "Validation Loss: 549.2914\n",
            "Epoch [96540/100000]\n",
            "Training Loss: 436.1508\n",
            "Validation Loss: 550.3878\n",
            "Epoch [96550/100000]\n",
            "Training Loss: 429.4339\n",
            "Validation Loss: 550.1561\n",
            "Epoch [96560/100000]\n",
            "Training Loss: 432.6854\n",
            "Validation Loss: 550.1227\n",
            "Epoch [96570/100000]\n",
            "Training Loss: 432.4604\n",
            "Validation Loss: 550.1126\n",
            "Epoch [96580/100000]\n",
            "Training Loss: 435.8357\n",
            "Validation Loss: 548.5409\n",
            "Epoch [96590/100000]\n",
            "Training Loss: 431.2304\n",
            "Validation Loss: 548.8770\n",
            "Epoch [96600/100000]\n",
            "Training Loss: 433.8597\n",
            "Validation Loss: 549.7904\n",
            "Epoch [96610/100000]\n",
            "Training Loss: 432.0942\n",
            "Validation Loss: 552.4599\n",
            "Epoch [96620/100000]\n",
            "Training Loss: 431.8396\n",
            "Validation Loss: 548.0522\n",
            "Epoch [96630/100000]\n",
            "Training Loss: 429.6331\n",
            "Validation Loss: 552.4182\n",
            "Epoch [96640/100000]\n",
            "Training Loss: 433.6979\n",
            "Validation Loss: 551.2555\n",
            "Epoch [96650/100000]\n",
            "Training Loss: 435.9872\n",
            "Validation Loss: 549.7455\n",
            "Epoch [96660/100000]\n",
            "Training Loss: 437.9230\n",
            "Validation Loss: 548.8769\n",
            "Epoch [96670/100000]\n",
            "Training Loss: 438.8654\n",
            "Validation Loss: 550.1143\n",
            "Epoch [96680/100000]\n",
            "Training Loss: 432.7895\n",
            "Validation Loss: 549.3497\n",
            "Epoch [96690/100000]\n",
            "Training Loss: 437.7727\n",
            "Validation Loss: 546.0527\n",
            "Epoch [96700/100000]\n",
            "Training Loss: 433.8511\n",
            "Validation Loss: 551.8707\n",
            "Epoch [96710/100000]\n",
            "Training Loss: 428.0366\n",
            "Validation Loss: 554.6735\n",
            "Epoch [96720/100000]\n",
            "Training Loss: 429.5426\n",
            "Validation Loss: 554.6694\n",
            "Epoch [96730/100000]\n",
            "Training Loss: 437.2608\n",
            "Validation Loss: 551.0748\n",
            "Epoch [96740/100000]\n",
            "Training Loss: 435.9577\n",
            "Validation Loss: 551.2775\n",
            "Epoch [96750/100000]\n",
            "Training Loss: 440.5429\n",
            "Validation Loss: 545.8913\n",
            "Epoch [96760/100000]\n",
            "Training Loss: 438.6688\n",
            "Validation Loss: 547.4717\n",
            "Epoch [96770/100000]\n",
            "Training Loss: 434.2437\n",
            "Validation Loss: 548.3326\n",
            "Epoch [96780/100000]\n",
            "Training Loss: 436.3361\n",
            "Validation Loss: 551.9457\n",
            "Epoch [96790/100000]\n",
            "Training Loss: 434.2325\n",
            "Validation Loss: 550.8627\n",
            "Epoch [96800/100000]\n",
            "Training Loss: 434.6253\n",
            "Validation Loss: 550.3809\n",
            "Epoch [96810/100000]\n",
            "Training Loss: 432.3885\n",
            "Validation Loss: 550.9533\n",
            "Epoch [96820/100000]\n",
            "Training Loss: 431.7234\n",
            "Validation Loss: 550.0786\n",
            "Epoch [96830/100000]\n",
            "Training Loss: 432.2940\n",
            "Validation Loss: 550.3466\n",
            "Epoch [96840/100000]\n",
            "Training Loss: 428.9657\n",
            "Validation Loss: 550.7681\n",
            "Epoch [96850/100000]\n",
            "Training Loss: 433.7876\n",
            "Validation Loss: 553.3408\n",
            "Epoch [96860/100000]\n",
            "Training Loss: 428.6144\n",
            "Validation Loss: 553.3288\n",
            "Epoch [96870/100000]\n",
            "Training Loss: 431.7780\n",
            "Validation Loss: 549.5391\n",
            "Epoch [96880/100000]\n",
            "Training Loss: 431.9863\n",
            "Validation Loss: 549.9619\n",
            "Epoch [96890/100000]\n",
            "Training Loss: 432.2442\n",
            "Validation Loss: 552.2285\n",
            "Epoch [96900/100000]\n",
            "Training Loss: 434.5346\n",
            "Validation Loss: 550.3466\n",
            "Epoch [96910/100000]\n",
            "Training Loss: 428.4620\n",
            "Validation Loss: 549.9315\n",
            "Epoch [96920/100000]\n",
            "Training Loss: 432.6103\n",
            "Validation Loss: 547.8160\n",
            "Epoch [96930/100000]\n",
            "Training Loss: 427.9926\n",
            "Validation Loss: 549.8072\n",
            "Epoch [96940/100000]\n",
            "Training Loss: 431.3970\n",
            "Validation Loss: 548.0471\n",
            "Epoch [96950/100000]\n",
            "Training Loss: 431.1976\n",
            "Validation Loss: 549.7301\n",
            "Epoch [96960/100000]\n",
            "Training Loss: 433.2565\n",
            "Validation Loss: 547.9392\n",
            "Epoch [96970/100000]\n",
            "Training Loss: 432.9874\n",
            "Validation Loss: 548.7265\n",
            "Epoch [96980/100000]\n",
            "Training Loss: 434.7771\n",
            "Validation Loss: 552.1407\n",
            "Epoch [96990/100000]\n",
            "Training Loss: 431.4064\n",
            "Validation Loss: 549.1863\n",
            "Epoch [97000/100000]\n",
            "Training Loss: 427.5129\n",
            "Validation Loss: 548.8585\n",
            "Epoch [97010/100000]\n",
            "Training Loss: 429.9494\n",
            "Validation Loss: 550.1346\n",
            "Epoch [97020/100000]\n",
            "Training Loss: 433.3666\n",
            "Validation Loss: 550.8799\n",
            "Epoch [97030/100000]\n",
            "Training Loss: 426.7146\n",
            "Validation Loss: 550.2436\n",
            "Epoch [97040/100000]\n",
            "Training Loss: 434.8477\n",
            "Validation Loss: 551.0417\n",
            "Epoch [97050/100000]\n",
            "Training Loss: 432.1375\n",
            "Validation Loss: 551.1219\n",
            "Epoch [97060/100000]\n",
            "Training Loss: 430.1902\n",
            "Validation Loss: 549.6384\n",
            "Epoch [97070/100000]\n",
            "Training Loss: 434.9267\n",
            "Validation Loss: 552.8401\n",
            "Epoch [97080/100000]\n",
            "Training Loss: 431.0024\n",
            "Validation Loss: 550.3005\n",
            "Epoch [97090/100000]\n",
            "Training Loss: 429.8807\n",
            "Validation Loss: 555.5001\n",
            "Epoch [97100/100000]\n",
            "Training Loss: 434.7508\n",
            "Validation Loss: 548.3420\n",
            "Epoch [97110/100000]\n",
            "Training Loss: 432.8917\n",
            "Validation Loss: 551.8473\n",
            "Epoch [97120/100000]\n",
            "Training Loss: 430.3336\n",
            "Validation Loss: 550.8546\n",
            "Epoch [97130/100000]\n",
            "Training Loss: 433.3306\n",
            "Validation Loss: 548.8722\n",
            "Epoch [97140/100000]\n",
            "Training Loss: 432.4427\n",
            "Validation Loss: 553.0095\n",
            "Epoch [97150/100000]\n",
            "Training Loss: 432.2742\n",
            "Validation Loss: 551.5195\n",
            "Epoch [97160/100000]\n",
            "Training Loss: 432.8217\n",
            "Validation Loss: 548.6780\n",
            "Epoch [97170/100000]\n",
            "Training Loss: 432.8609\n",
            "Validation Loss: 550.1445\n",
            "Epoch [97180/100000]\n",
            "Training Loss: 432.0733\n",
            "Validation Loss: 548.6954\n",
            "Epoch [97190/100000]\n",
            "Training Loss: 432.7197\n",
            "Validation Loss: 551.4835\n",
            "Epoch [97200/100000]\n",
            "Training Loss: 433.4490\n",
            "Validation Loss: 549.4215\n",
            "Epoch [97210/100000]\n",
            "Training Loss: 437.3050\n",
            "Validation Loss: 548.3432\n",
            "Epoch [97220/100000]\n",
            "Training Loss: 432.7725\n",
            "Validation Loss: 549.6775\n",
            "Epoch [97230/100000]\n",
            "Training Loss: 435.4005\n",
            "Validation Loss: 549.6879\n",
            "Epoch [97240/100000]\n",
            "Training Loss: 429.0659\n",
            "Validation Loss: 551.4922\n",
            "Epoch [97250/100000]\n",
            "Training Loss: 433.1179\n",
            "Validation Loss: 550.7745\n",
            "Epoch [97260/100000]\n",
            "Training Loss: 430.6860\n",
            "Validation Loss: 548.7681\n",
            "Epoch [97270/100000]\n",
            "Training Loss: 428.1855\n",
            "Validation Loss: 549.6351\n",
            "Epoch [97280/100000]\n",
            "Training Loss: 425.3957\n",
            "Validation Loss: 549.7437\n",
            "Epoch [97290/100000]\n",
            "Training Loss: 431.5371\n",
            "Validation Loss: 551.6951\n",
            "Epoch [97300/100000]\n",
            "Training Loss: 431.2557\n",
            "Validation Loss: 554.0843\n",
            "Epoch [97310/100000]\n",
            "Training Loss: 432.6493\n",
            "Validation Loss: 551.7730\n",
            "Epoch [97320/100000]\n",
            "Training Loss: 431.0740\n",
            "Validation Loss: 548.9750\n",
            "Epoch [97330/100000]\n",
            "Training Loss: 433.3078\n",
            "Validation Loss: 551.2584\n",
            "Epoch [97340/100000]\n",
            "Training Loss: 429.3070\n",
            "Validation Loss: 549.3420\n",
            "Epoch [97350/100000]\n",
            "Training Loss: 436.1579\n",
            "Validation Loss: 550.6613\n",
            "Epoch [97360/100000]\n",
            "Training Loss: 437.3456\n",
            "Validation Loss: 547.6719\n",
            "Epoch [97370/100000]\n",
            "Training Loss: 433.2844\n",
            "Validation Loss: 551.4632\n",
            "Epoch [97380/100000]\n",
            "Training Loss: 426.7668\n",
            "Validation Loss: 551.8380\n",
            "Epoch [97390/100000]\n",
            "Training Loss: 428.7129\n",
            "Validation Loss: 549.1295\n",
            "Epoch [97400/100000]\n",
            "Training Loss: 433.8883\n",
            "Validation Loss: 547.0754\n",
            "Epoch [97410/100000]\n",
            "Training Loss: 434.7629\n",
            "Validation Loss: 550.1432\n",
            "Epoch [97420/100000]\n",
            "Training Loss: 433.7282\n",
            "Validation Loss: 549.8475\n",
            "Epoch [97430/100000]\n",
            "Training Loss: 431.3943\n",
            "Validation Loss: 549.9377\n",
            "Epoch [97440/100000]\n",
            "Training Loss: 436.4577\n",
            "Validation Loss: 549.7880\n",
            "Epoch [97450/100000]\n",
            "Training Loss: 428.8666\n",
            "Validation Loss: 549.8651\n",
            "Epoch [97460/100000]\n",
            "Training Loss: 431.3034\n",
            "Validation Loss: 548.5117\n",
            "Epoch [97470/100000]\n",
            "Training Loss: 434.6554\n",
            "Validation Loss: 549.5605\n",
            "Epoch [97480/100000]\n",
            "Training Loss: 428.9339\n",
            "Validation Loss: 549.5909\n",
            "Epoch [97490/100000]\n",
            "Training Loss: 432.3849\n",
            "Validation Loss: 549.7286\n",
            "Epoch [97500/100000]\n",
            "Training Loss: 431.2101\n",
            "Validation Loss: 549.2697\n",
            "Epoch [97510/100000]\n",
            "Training Loss: 434.8922\n",
            "Validation Loss: 552.6407\n",
            "Epoch [97520/100000]\n",
            "Training Loss: 434.5402\n",
            "Validation Loss: 552.9188\n",
            "Epoch [97530/100000]\n",
            "Training Loss: 432.0955\n",
            "Validation Loss: 548.9012\n",
            "Epoch [97540/100000]\n",
            "Training Loss: 425.2032\n",
            "Validation Loss: 558.0895\n",
            "Epoch [97550/100000]\n",
            "Training Loss: 436.8394\n",
            "Validation Loss: 551.5430\n",
            "Epoch [97560/100000]\n",
            "Training Loss: 432.5234\n",
            "Validation Loss: 554.9209\n",
            "Epoch [97570/100000]\n",
            "Training Loss: 433.7618\n",
            "Validation Loss: 548.5128\n",
            "Epoch [97580/100000]\n",
            "Training Loss: 433.2173\n",
            "Validation Loss: 552.1685\n",
            "Epoch [97590/100000]\n",
            "Training Loss: 430.4724\n",
            "Validation Loss: 548.3634\n",
            "Epoch [97600/100000]\n",
            "Training Loss: 430.8778\n",
            "Validation Loss: 546.8923\n",
            "Epoch [97610/100000]\n",
            "Training Loss: 430.5057\n",
            "Validation Loss: 552.1190\n",
            "Epoch [97620/100000]\n",
            "Training Loss: 428.3618\n",
            "Validation Loss: 547.9527\n",
            "Epoch [97630/100000]\n",
            "Training Loss: 430.2137\n",
            "Validation Loss: 555.2428\n",
            "Epoch [97640/100000]\n",
            "Training Loss: 434.7024\n",
            "Validation Loss: 547.5857\n",
            "Epoch [97650/100000]\n",
            "Training Loss: 431.1247\n",
            "Validation Loss: 548.1269\n",
            "Epoch [97660/100000]\n",
            "Training Loss: 431.7616\n",
            "Validation Loss: 552.4263\n",
            "Epoch [97670/100000]\n",
            "Training Loss: 429.0710\n",
            "Validation Loss: 546.3206\n",
            "Epoch [97680/100000]\n",
            "Training Loss: 431.8394\n",
            "Validation Loss: 550.6638\n",
            "Epoch [97690/100000]\n",
            "Training Loss: 433.3136\n",
            "Validation Loss: 548.1243\n",
            "Epoch [97700/100000]\n",
            "Training Loss: 426.6509\n",
            "Validation Loss: 551.3538\n",
            "Epoch [97710/100000]\n",
            "Training Loss: 429.9839\n",
            "Validation Loss: 551.9834\n",
            "Epoch [97720/100000]\n",
            "Training Loss: 432.3998\n",
            "Validation Loss: 549.6343\n",
            "Epoch [97730/100000]\n",
            "Training Loss: 431.3266\n",
            "Validation Loss: 549.9966\n",
            "Epoch [97740/100000]\n",
            "Training Loss: 431.7573\n",
            "Validation Loss: 547.7398\n",
            "Epoch [97750/100000]\n",
            "Training Loss: 429.5931\n",
            "Validation Loss: 554.6183\n",
            "Epoch [97760/100000]\n",
            "Training Loss: 430.0207\n",
            "Validation Loss: 551.6086\n",
            "Epoch [97770/100000]\n",
            "Training Loss: 434.5470\n",
            "Validation Loss: 547.3141\n",
            "Epoch [97780/100000]\n",
            "Training Loss: 433.7455\n",
            "Validation Loss: 549.5632\n",
            "Epoch [97790/100000]\n",
            "Training Loss: 429.7921\n",
            "Validation Loss: 548.6221\n",
            "Epoch [97800/100000]\n",
            "Training Loss: 430.2117\n",
            "Validation Loss: 548.2352\n",
            "Epoch [97810/100000]\n",
            "Training Loss: 430.9882\n",
            "Validation Loss: 549.8057\n",
            "Epoch [97820/100000]\n",
            "Training Loss: 432.3244\n",
            "Validation Loss: 547.8278\n",
            "Epoch [97830/100000]\n",
            "Training Loss: 430.7469\n",
            "Validation Loss: 553.0348\n",
            "Epoch [97840/100000]\n",
            "Training Loss: 434.7577\n",
            "Validation Loss: 549.5549\n",
            "Epoch [97850/100000]\n",
            "Training Loss: 432.7381\n",
            "Validation Loss: 550.9069\n",
            "Epoch [97860/100000]\n",
            "Training Loss: 425.7863\n",
            "Validation Loss: 552.6856\n",
            "Epoch [97870/100000]\n",
            "Training Loss: 437.2162\n",
            "Validation Loss: 550.9484\n",
            "Epoch [97880/100000]\n",
            "Training Loss: 432.9751\n",
            "Validation Loss: 550.6288\n",
            "Epoch [97890/100000]\n",
            "Training Loss: 433.1327\n",
            "Validation Loss: 546.6547\n",
            "Epoch [97900/100000]\n",
            "Training Loss: 428.6829\n",
            "Validation Loss: 548.3144\n",
            "Epoch [97910/100000]\n",
            "Training Loss: 432.8050\n",
            "Validation Loss: 549.5330\n",
            "Epoch [97920/100000]\n",
            "Training Loss: 431.9891\n",
            "Validation Loss: 550.6729\n",
            "Epoch [97930/100000]\n",
            "Training Loss: 432.3689\n",
            "Validation Loss: 549.1669\n",
            "Epoch [97940/100000]\n",
            "Training Loss: 437.4098\n",
            "Validation Loss: 547.7236\n",
            "Epoch [97950/100000]\n",
            "Training Loss: 437.9875\n",
            "Validation Loss: 551.9335\n",
            "Epoch [97960/100000]\n",
            "Training Loss: 434.7606\n",
            "Validation Loss: 549.2202\n",
            "Epoch [97970/100000]\n",
            "Training Loss: 430.6323\n",
            "Validation Loss: 550.8034\n",
            "Epoch [97980/100000]\n",
            "Training Loss: 436.6667\n",
            "Validation Loss: 548.2273\n",
            "Epoch [97990/100000]\n",
            "Training Loss: 426.3089\n",
            "Validation Loss: 552.5955\n",
            "Epoch [98000/100000]\n",
            "Training Loss: 437.9384\n",
            "Validation Loss: 545.9245\n",
            "Epoch [98010/100000]\n",
            "Training Loss: 431.2244\n",
            "Validation Loss: 547.8892\n",
            "Epoch [98020/100000]\n",
            "Training Loss: 434.7421\n",
            "Validation Loss: 548.9705\n",
            "Epoch [98030/100000]\n",
            "Training Loss: 429.0690\n",
            "Validation Loss: 552.1863\n",
            "Epoch [98040/100000]\n",
            "Training Loss: 430.2042\n",
            "Validation Loss: 549.0615\n",
            "Epoch [98050/100000]\n",
            "Training Loss: 433.5070\n",
            "Validation Loss: 550.6208\n",
            "Epoch [98060/100000]\n",
            "Training Loss: 428.2246\n",
            "Validation Loss: 550.7952\n",
            "Epoch [98070/100000]\n",
            "Training Loss: 427.9725\n",
            "Validation Loss: 551.3282\n",
            "Epoch [98080/100000]\n",
            "Training Loss: 430.1576\n",
            "Validation Loss: 548.3041\n",
            "Epoch [98090/100000]\n",
            "Training Loss: 438.5941\n",
            "Validation Loss: 547.6308\n",
            "Epoch [98100/100000]\n",
            "Training Loss: 426.8472\n",
            "Validation Loss: 545.5824\n",
            "Epoch [98110/100000]\n",
            "Training Loss: 431.3725\n",
            "Validation Loss: 545.4301\n",
            "Epoch [98120/100000]\n",
            "Training Loss: 429.8621\n",
            "Validation Loss: 548.9473\n",
            "Epoch [98130/100000]\n",
            "Training Loss: 434.5428\n",
            "Validation Loss: 549.5209\n",
            "Epoch [98140/100000]\n",
            "Training Loss: 431.1780\n",
            "Validation Loss: 550.9905\n",
            "Epoch [98150/100000]\n",
            "Training Loss: 424.0198\n",
            "Validation Loss: 557.9332\n",
            "Epoch [98160/100000]\n",
            "Training Loss: 436.9214\n",
            "Validation Loss: 553.2824\n",
            "Epoch [98170/100000]\n",
            "Training Loss: 434.0800\n",
            "Validation Loss: 549.5551\n",
            "Epoch [98180/100000]\n",
            "Training Loss: 431.9484\n",
            "Validation Loss: 550.6200\n",
            "Epoch [98190/100000]\n",
            "Training Loss: 431.1649\n",
            "Validation Loss: 549.8578\n",
            "Epoch [98200/100000]\n",
            "Training Loss: 430.8791\n",
            "Validation Loss: 549.6015\n",
            "Epoch [98210/100000]\n",
            "Training Loss: 432.6032\n",
            "Validation Loss: 550.7574\n",
            "Epoch [98220/100000]\n",
            "Training Loss: 438.1654\n",
            "Validation Loss: 549.2380\n",
            "Epoch [98230/100000]\n",
            "Training Loss: 436.9807\n",
            "Validation Loss: 548.3220\n",
            "Epoch [98240/100000]\n",
            "Training Loss: 430.4037\n",
            "Validation Loss: 550.6113\n",
            "Epoch [98250/100000]\n",
            "Training Loss: 428.0889\n",
            "Validation Loss: 550.6038\n",
            "Epoch [98260/100000]\n",
            "Training Loss: 432.8642\n",
            "Validation Loss: 551.1322\n",
            "Epoch [98270/100000]\n",
            "Training Loss: 429.4253\n",
            "Validation Loss: 550.9562\n",
            "Epoch [98280/100000]\n",
            "Training Loss: 431.6311\n",
            "Validation Loss: 553.6267\n",
            "Epoch [98290/100000]\n",
            "Training Loss: 434.6253\n",
            "Validation Loss: 552.4122\n",
            "Epoch [98300/100000]\n",
            "Training Loss: 432.3361\n",
            "Validation Loss: 548.7791\n",
            "Epoch [98310/100000]\n",
            "Training Loss: 434.1208\n",
            "Validation Loss: 552.6956\n",
            "Epoch [98320/100000]\n",
            "Training Loss: 431.9259\n",
            "Validation Loss: 549.8183\n",
            "Epoch [98330/100000]\n",
            "Training Loss: 432.2952\n",
            "Validation Loss: 550.1568\n",
            "Epoch [98340/100000]\n",
            "Training Loss: 431.6272\n",
            "Validation Loss: 548.3721\n",
            "Epoch [98350/100000]\n",
            "Training Loss: 428.9481\n",
            "Validation Loss: 551.8040\n",
            "Epoch [98360/100000]\n",
            "Training Loss: 435.7332\n",
            "Validation Loss: 549.2609\n",
            "Epoch [98370/100000]\n",
            "Training Loss: 428.9234\n",
            "Validation Loss: 552.0851\n",
            "Epoch [98380/100000]\n",
            "Training Loss: 435.7450\n",
            "Validation Loss: 551.3557\n",
            "Epoch [98390/100000]\n",
            "Training Loss: 432.5272\n",
            "Validation Loss: 550.3343\n",
            "Epoch [98400/100000]\n",
            "Training Loss: 430.8037\n",
            "Validation Loss: 549.9372\n",
            "Epoch [98410/100000]\n",
            "Training Loss: 433.1039\n",
            "Validation Loss: 547.6314\n",
            "Epoch [98420/100000]\n",
            "Training Loss: 429.7902\n",
            "Validation Loss: 548.1401\n",
            "Epoch [98430/100000]\n",
            "Training Loss: 428.3402\n",
            "Validation Loss: 550.9982\n",
            "Epoch [98440/100000]\n",
            "Training Loss: 432.4444\n",
            "Validation Loss: 547.9385\n",
            "Epoch [98450/100000]\n",
            "Training Loss: 434.2780\n",
            "Validation Loss: 550.8901\n",
            "Epoch [98460/100000]\n",
            "Training Loss: 431.1624\n",
            "Validation Loss: 551.4205\n",
            "Epoch [98470/100000]\n",
            "Training Loss: 432.4810\n",
            "Validation Loss: 550.7274\n",
            "Epoch [98480/100000]\n",
            "Training Loss: 430.2734\n",
            "Validation Loss: 552.5539\n",
            "Epoch [98490/100000]\n",
            "Training Loss: 432.0444\n",
            "Validation Loss: 550.8290\n",
            "Epoch [98500/100000]\n",
            "Training Loss: 432.7287\n",
            "Validation Loss: 553.0674\n",
            "Epoch [98510/100000]\n",
            "Training Loss: 430.3614\n",
            "Validation Loss: 549.8187\n",
            "Epoch [98520/100000]\n",
            "Training Loss: 435.1748\n",
            "Validation Loss: 547.9391\n",
            "Epoch [98530/100000]\n",
            "Training Loss: 429.2824\n",
            "Validation Loss: 550.1308\n",
            "Epoch [98540/100000]\n",
            "Training Loss: 435.3319\n",
            "Validation Loss: 552.2952\n",
            "Epoch [98550/100000]\n",
            "Training Loss: 431.6761\n",
            "Validation Loss: 551.7388\n",
            "Epoch [98560/100000]\n",
            "Training Loss: 428.1667\n",
            "Validation Loss: 551.5363\n",
            "Epoch [98570/100000]\n",
            "Training Loss: 429.3460\n",
            "Validation Loss: 551.6177\n",
            "Epoch [98580/100000]\n",
            "Training Loss: 432.5318\n",
            "Validation Loss: 550.8418\n",
            "Epoch [98590/100000]\n",
            "Training Loss: 431.9802\n",
            "Validation Loss: 552.4617\n",
            "Epoch [98600/100000]\n",
            "Training Loss: 430.8832\n",
            "Validation Loss: 550.7895\n",
            "Epoch [98610/100000]\n",
            "Training Loss: 435.4293\n",
            "Validation Loss: 551.4628\n",
            "Epoch [98620/100000]\n",
            "Training Loss: 434.0438\n",
            "Validation Loss: 550.7698\n",
            "Epoch [98630/100000]\n",
            "Training Loss: 436.2592\n",
            "Validation Loss: 549.6246\n",
            "Epoch [98640/100000]\n",
            "Training Loss: 429.0566\n",
            "Validation Loss: 552.6259\n",
            "Epoch [98650/100000]\n",
            "Training Loss: 425.9487\n",
            "Validation Loss: 551.1213\n",
            "Epoch [98660/100000]\n",
            "Training Loss: 430.9294\n",
            "Validation Loss: 549.5558\n",
            "Epoch [98670/100000]\n",
            "Training Loss: 432.6440\n",
            "Validation Loss: 551.2719\n",
            "Epoch [98680/100000]\n",
            "Training Loss: 430.9197\n",
            "Validation Loss: 552.7617\n",
            "Epoch [98690/100000]\n",
            "Training Loss: 431.7711\n",
            "Validation Loss: 553.4094\n",
            "Epoch [98700/100000]\n",
            "Training Loss: 431.6733\n",
            "Validation Loss: 548.8120\n",
            "Epoch [98710/100000]\n",
            "Training Loss: 436.7996\n",
            "Validation Loss: 550.2162\n",
            "Epoch [98720/100000]\n",
            "Training Loss: 431.3802\n",
            "Validation Loss: 548.3990\n",
            "Epoch [98730/100000]\n",
            "Training Loss: 427.2158\n",
            "Validation Loss: 548.5450\n",
            "Epoch [98740/100000]\n",
            "Training Loss: 431.0816\n",
            "Validation Loss: 548.1301\n",
            "Epoch [98750/100000]\n",
            "Training Loss: 434.7896\n",
            "Validation Loss: 549.2983\n",
            "Epoch [98760/100000]\n",
            "Training Loss: 429.9992\n",
            "Validation Loss: 551.6105\n",
            "Epoch [98770/100000]\n",
            "Training Loss: 426.9042\n",
            "Validation Loss: 549.2191\n",
            "Epoch [98780/100000]\n",
            "Training Loss: 429.0883\n",
            "Validation Loss: 551.7266\n",
            "Epoch [98790/100000]\n",
            "Training Loss: 427.1938\n",
            "Validation Loss: 553.2300\n",
            "Epoch [98800/100000]\n",
            "Training Loss: 433.5217\n",
            "Validation Loss: 550.3816\n",
            "Epoch [98810/100000]\n",
            "Training Loss: 431.6120\n",
            "Validation Loss: 546.7466\n",
            "Epoch [98820/100000]\n",
            "Training Loss: 432.8228\n",
            "Validation Loss: 551.7064\n",
            "Epoch [98830/100000]\n",
            "Training Loss: 433.1942\n",
            "Validation Loss: 551.9773\n",
            "Epoch [98840/100000]\n",
            "Training Loss: 428.3406\n",
            "Validation Loss: 553.8424\n",
            "Epoch [98850/100000]\n",
            "Training Loss: 428.5470\n",
            "Validation Loss: 551.3964\n",
            "Epoch [98860/100000]\n",
            "Training Loss: 435.7226\n",
            "Validation Loss: 548.1982\n",
            "Epoch [98870/100000]\n",
            "Training Loss: 435.9686\n",
            "Validation Loss: 548.3195\n",
            "Epoch [98880/100000]\n",
            "Training Loss: 427.3724\n",
            "Validation Loss: 550.5930\n",
            "Epoch [98890/100000]\n",
            "Training Loss: 430.7268\n",
            "Validation Loss: 552.9373\n",
            "Epoch [98900/100000]\n",
            "Training Loss: 431.7196\n",
            "Validation Loss: 552.3687\n",
            "Epoch [98910/100000]\n",
            "Training Loss: 439.0441\n",
            "Validation Loss: 549.2906\n",
            "Epoch [98920/100000]\n",
            "Training Loss: 434.1822\n",
            "Validation Loss: 550.5654\n",
            "Epoch [98930/100000]\n",
            "Training Loss: 429.4695\n",
            "Validation Loss: 552.3630\n",
            "Epoch [98940/100000]\n",
            "Training Loss: 430.4310\n",
            "Validation Loss: 551.6356\n",
            "Epoch [98950/100000]\n",
            "Training Loss: 428.8386\n",
            "Validation Loss: 549.6328\n",
            "Epoch [98960/100000]\n",
            "Training Loss: 430.8209\n",
            "Validation Loss: 551.7789\n",
            "Epoch [98970/100000]\n",
            "Training Loss: 429.0254\n",
            "Validation Loss: 553.9966\n",
            "Epoch [98980/100000]\n",
            "Training Loss: 426.2307\n",
            "Validation Loss: 555.0837\n",
            "Epoch [98990/100000]\n",
            "Training Loss: 429.8336\n",
            "Validation Loss: 549.7020\n",
            "Epoch [99000/100000]\n",
            "Training Loss: 427.2872\n",
            "Validation Loss: 549.4608\n",
            "Epoch [99010/100000]\n",
            "Training Loss: 430.5993\n",
            "Validation Loss: 551.2562\n",
            "Epoch [99020/100000]\n",
            "Training Loss: 430.3077\n",
            "Validation Loss: 549.9203\n",
            "Epoch [99030/100000]\n",
            "Training Loss: 432.3738\n",
            "Validation Loss: 548.0969\n",
            "Epoch [99040/100000]\n",
            "Training Loss: 428.2292\n",
            "Validation Loss: 550.0701\n",
            "Epoch [99050/100000]\n",
            "Training Loss: 430.4962\n",
            "Validation Loss: 550.7054\n",
            "Epoch [99060/100000]\n",
            "Training Loss: 433.5489\n",
            "Validation Loss: 550.6484\n",
            "Epoch [99070/100000]\n",
            "Training Loss: 441.1577\n",
            "Validation Loss: 549.7164\n",
            "Epoch [99080/100000]\n",
            "Training Loss: 434.5590\n",
            "Validation Loss: 553.3675\n",
            "Epoch [99090/100000]\n",
            "Training Loss: 429.5817\n",
            "Validation Loss: 547.9684\n",
            "Epoch [99100/100000]\n",
            "Training Loss: 430.5714\n",
            "Validation Loss: 551.0967\n",
            "Epoch [99110/100000]\n",
            "Training Loss: 431.3663\n",
            "Validation Loss: 548.4564\n",
            "Epoch [99120/100000]\n",
            "Training Loss: 427.9027\n",
            "Validation Loss: 550.8306\n",
            "Epoch [99130/100000]\n",
            "Training Loss: 432.9294\n",
            "Validation Loss: 550.7186\n",
            "Epoch [99140/100000]\n",
            "Training Loss: 427.2368\n",
            "Validation Loss: 551.7581\n",
            "Epoch [99150/100000]\n",
            "Training Loss: 431.9016\n",
            "Validation Loss: 550.7629\n",
            "Epoch [99160/100000]\n",
            "Training Loss: 431.7278\n",
            "Validation Loss: 551.9618\n",
            "Epoch [99170/100000]\n",
            "Training Loss: 434.4824\n",
            "Validation Loss: 548.2523\n",
            "Epoch [99180/100000]\n",
            "Training Loss: 431.9703\n",
            "Validation Loss: 554.9418\n",
            "Epoch [99190/100000]\n",
            "Training Loss: 430.2370\n",
            "Validation Loss: 553.7023\n",
            "Epoch [99200/100000]\n",
            "Training Loss: 431.4460\n",
            "Validation Loss: 549.5767\n",
            "Epoch [99210/100000]\n",
            "Training Loss: 425.5130\n",
            "Validation Loss: 553.1981\n",
            "Epoch [99220/100000]\n",
            "Training Loss: 432.7502\n",
            "Validation Loss: 551.1337\n",
            "Epoch [99230/100000]\n",
            "Training Loss: 430.4965\n",
            "Validation Loss: 551.6058\n",
            "Epoch [99240/100000]\n",
            "Training Loss: 432.7036\n",
            "Validation Loss: 551.6254\n",
            "Epoch [99250/100000]\n",
            "Training Loss: 430.0283\n",
            "Validation Loss: 549.7070\n",
            "Epoch [99260/100000]\n",
            "Training Loss: 436.4600\n",
            "Validation Loss: 551.1782\n",
            "Epoch [99270/100000]\n",
            "Training Loss: 428.1105\n",
            "Validation Loss: 551.7949\n",
            "Epoch [99280/100000]\n",
            "Training Loss: 431.2426\n",
            "Validation Loss: 548.3953\n",
            "Epoch [99290/100000]\n",
            "Training Loss: 430.7392\n",
            "Validation Loss: 549.5657\n",
            "Epoch [99300/100000]\n",
            "Training Loss: 429.8393\n",
            "Validation Loss: 550.7994\n",
            "Epoch [99310/100000]\n",
            "Training Loss: 429.6523\n",
            "Validation Loss: 551.9947\n",
            "Epoch [99320/100000]\n",
            "Training Loss: 438.9461\n",
            "Validation Loss: 549.2048\n",
            "Epoch [99330/100000]\n",
            "Training Loss: 434.5594\n",
            "Validation Loss: 552.8547\n",
            "Epoch [99340/100000]\n",
            "Training Loss: 439.6296\n",
            "Validation Loss: 548.4675\n",
            "Epoch [99350/100000]\n",
            "Training Loss: 429.2574\n",
            "Validation Loss: 551.6562\n",
            "Epoch [99360/100000]\n",
            "Training Loss: 432.9355\n",
            "Validation Loss: 549.9125\n",
            "Epoch [99370/100000]\n",
            "Training Loss: 423.9196\n",
            "Validation Loss: 549.1342\n",
            "Epoch [99380/100000]\n",
            "Training Loss: 432.0810\n",
            "Validation Loss: 547.4727\n",
            "Epoch [99390/100000]\n",
            "Training Loss: 428.5251\n",
            "Validation Loss: 555.1096\n",
            "Epoch [99400/100000]\n",
            "Training Loss: 427.5148\n",
            "Validation Loss: 550.3258\n",
            "Epoch [99410/100000]\n",
            "Training Loss: 428.5310\n",
            "Validation Loss: 554.7848\n",
            "Epoch [99420/100000]\n",
            "Training Loss: 431.4318\n",
            "Validation Loss: 549.8533\n",
            "Epoch [99430/100000]\n",
            "Training Loss: 429.3536\n",
            "Validation Loss: 548.5579\n",
            "Epoch [99440/100000]\n",
            "Training Loss: 430.9059\n",
            "Validation Loss: 549.7349\n",
            "Epoch [99450/100000]\n",
            "Training Loss: 429.5224\n",
            "Validation Loss: 551.3964\n",
            "Epoch [99460/100000]\n",
            "Training Loss: 437.2829\n",
            "Validation Loss: 551.2185\n",
            "Epoch [99470/100000]\n",
            "Training Loss: 427.1873\n",
            "Validation Loss: 556.4397\n",
            "Epoch [99480/100000]\n",
            "Training Loss: 426.7157\n",
            "Validation Loss: 552.6139\n",
            "Epoch [99490/100000]\n",
            "Training Loss: 429.1964\n",
            "Validation Loss: 554.6536\n",
            "Epoch [99500/100000]\n",
            "Training Loss: 433.2790\n",
            "Validation Loss: 547.9808\n",
            "Epoch [99510/100000]\n",
            "Training Loss: 430.9767\n",
            "Validation Loss: 553.2609\n",
            "Epoch [99520/100000]\n",
            "Training Loss: 431.4957\n",
            "Validation Loss: 550.5932\n",
            "Epoch [99530/100000]\n",
            "Training Loss: 427.9387\n",
            "Validation Loss: 549.7975\n",
            "Epoch [99540/100000]\n",
            "Training Loss: 433.9907\n",
            "Validation Loss: 548.4353\n",
            "Epoch [99550/100000]\n",
            "Training Loss: 434.8194\n",
            "Validation Loss: 549.7756\n",
            "Epoch [99560/100000]\n",
            "Training Loss: 429.3616\n",
            "Validation Loss: 549.6222\n",
            "Epoch [99570/100000]\n",
            "Training Loss: 426.8679\n",
            "Validation Loss: 550.4053\n",
            "Epoch [99580/100000]\n",
            "Training Loss: 433.6333\n",
            "Validation Loss: 548.9212\n",
            "Epoch [99590/100000]\n",
            "Training Loss: 426.5857\n",
            "Validation Loss: 550.5973\n",
            "Epoch [99600/100000]\n",
            "Training Loss: 432.2812\n",
            "Validation Loss: 549.2683\n",
            "Epoch [99610/100000]\n",
            "Training Loss: 429.7653\n",
            "Validation Loss: 548.5654\n",
            "Epoch [99620/100000]\n",
            "Training Loss: 428.4969\n",
            "Validation Loss: 551.2133\n",
            "Epoch [99630/100000]\n",
            "Training Loss: 432.4315\n",
            "Validation Loss: 551.2590\n",
            "Epoch [99640/100000]\n",
            "Training Loss: 424.4112\n",
            "Validation Loss: 551.3943\n",
            "Epoch [99650/100000]\n",
            "Training Loss: 428.2202\n",
            "Validation Loss: 551.3772\n",
            "Epoch [99660/100000]\n",
            "Training Loss: 435.0910\n",
            "Validation Loss: 548.3431\n",
            "Epoch [99670/100000]\n",
            "Training Loss: 430.8728\n",
            "Validation Loss: 547.7088\n",
            "Epoch [99680/100000]\n",
            "Training Loss: 433.2573\n",
            "Validation Loss: 549.0564\n",
            "Epoch [99690/100000]\n",
            "Training Loss: 433.8401\n",
            "Validation Loss: 550.8470\n",
            "Epoch [99700/100000]\n",
            "Training Loss: 434.3801\n",
            "Validation Loss: 550.5034\n",
            "Epoch [99710/100000]\n",
            "Training Loss: 431.8160\n",
            "Validation Loss: 549.6464\n",
            "Epoch [99720/100000]\n",
            "Training Loss: 433.1206\n",
            "Validation Loss: 551.6506\n",
            "Epoch [99730/100000]\n",
            "Training Loss: 430.6486\n",
            "Validation Loss: 551.3541\n",
            "Epoch [99740/100000]\n",
            "Training Loss: 436.1168\n",
            "Validation Loss: 548.2961\n",
            "Epoch [99750/100000]\n",
            "Training Loss: 430.3590\n",
            "Validation Loss: 555.2153\n",
            "Epoch [99760/100000]\n",
            "Training Loss: 427.3945\n",
            "Validation Loss: 556.9872\n",
            "Epoch [99770/100000]\n",
            "Training Loss: 430.2931\n",
            "Validation Loss: 550.1166\n",
            "Epoch [99780/100000]\n",
            "Training Loss: 430.0075\n",
            "Validation Loss: 549.9340\n",
            "Epoch [99790/100000]\n",
            "Training Loss: 426.1315\n",
            "Validation Loss: 550.4059\n",
            "Epoch [99800/100000]\n",
            "Training Loss: 430.2022\n",
            "Validation Loss: 552.5632\n",
            "Epoch [99810/100000]\n",
            "Training Loss: 430.3872\n",
            "Validation Loss: 549.9225\n",
            "Epoch [99820/100000]\n",
            "Training Loss: 434.6618\n",
            "Validation Loss: 553.8962\n",
            "Epoch [99830/100000]\n",
            "Training Loss: 433.8536\n",
            "Validation Loss: 548.1607\n",
            "Epoch [99840/100000]\n",
            "Training Loss: 428.5284\n",
            "Validation Loss: 550.0499\n",
            "Epoch [99850/100000]\n",
            "Training Loss: 429.1471\n",
            "Validation Loss: 553.5596\n",
            "Epoch [99860/100000]\n",
            "Training Loss: 436.7452\n",
            "Validation Loss: 546.9794\n",
            "Epoch [99870/100000]\n",
            "Training Loss: 433.2627\n",
            "Validation Loss: 550.0419\n",
            "Epoch [99880/100000]\n",
            "Training Loss: 430.3449\n",
            "Validation Loss: 551.4893\n",
            "Epoch [99890/100000]\n",
            "Training Loss: 429.7278\n",
            "Validation Loss: 548.2238\n",
            "Epoch [99900/100000]\n",
            "Training Loss: 430.4789\n",
            "Validation Loss: 549.4288\n",
            "Epoch [99910/100000]\n",
            "Training Loss: 432.0900\n",
            "Validation Loss: 547.1238\n",
            "Epoch [99920/100000]\n",
            "Training Loss: 430.9536\n",
            "Validation Loss: 550.8527\n",
            "Epoch [99930/100000]\n",
            "Training Loss: 429.3358\n",
            "Validation Loss: 554.5796\n",
            "Epoch [99940/100000]\n",
            "Training Loss: 432.9820\n",
            "Validation Loss: 550.3378\n",
            "Epoch [99950/100000]\n",
            "Training Loss: 438.4102\n",
            "Validation Loss: 550.2621\n",
            "Epoch [99960/100000]\n",
            "Training Loss: 436.0615\n",
            "Validation Loss: 552.6216\n",
            "Epoch [99970/100000]\n",
            "Training Loss: 429.2986\n",
            "Validation Loss: 551.1096\n",
            "Epoch [99980/100000]\n",
            "Training Loss: 431.1263\n",
            "Validation Loss: 546.1644\n",
            "Epoch [99990/100000]\n",
            "Training Loss: 427.3334\n",
            "Validation Loss: 550.2427\n",
            "Early stopping at epoch 100000\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "num_epochs = 100000\n",
        "best_val_loss = 0.03\n",
        "patience = 100000\n",
        "patience_counter = 0\n",
        "val_threshold = 524\n",
        "\n",
        "print(\"Starting training...\")\n",
        "print(f\"Training data shape: {X_train_tensor.shape}\")\n",
        "print(f\"Validation data shape: {X_val_tensor.shape}\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    model.train()\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_outputs = model(X_val_tensor)\n",
        "        val_loss = criterion(val_outputs, y_val_tensor)\n",
        "\n",
        "        # Early stopping\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "            # Save the best model\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "        # elif val_loss.item() <= val_threshold:\n",
        "        #     print(f\"Early stopping at epoch {epoch+1}\")\n",
        "        #     break\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
        "        print(f'Training Loss: {loss.item():.4f}')\n",
        "        print(f'Validation Loss: {val_loss.item():.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHfWBR-tS1Sr",
        "outputId": "270e9ccb-4d6b-43d2-ac1e-6426c00675bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columns missing in df2: {'prim_disease_hct_Other leukemia', 'diabetes_Yes', 'tce_div_match_Permissive mismatched', 'dri_score_N/A - non-malignant indication', 'cyto_score_Favorable', 'vent_hist_No', 'cyto_score_detail_Intermediate', 'rituximab_nan', 'prim_disease_hct_HIS', 'rheum_issue_nan', 'mrd_hct_nan', 'hepatic_mild_Yes', 'cyto_score_Not tested', 'arrhythmia_Yes', 'hla_nmdp_6', 'prim_disease_hct_IMD', 'hepatic_mild_nan', 'cyto_score_detail_Favorable', 'comorbidity_score', 'rituximab_No', 'tce_imm_match_H/H', 'peptic_ulcer_Yes', 'hla_low_res_6', 'cyto_score_Other', 'psych_disturb_Yes', 'prim_disease_hct_HD', 'gvhd_proph_FKalone', 'hla_high_res_6', 'dri_score_Intermediate - TED AML case <missing cytogenetics', 'gvhd_proph_CSA +- others(not FK,MMF,MTX)', 'race_group_Asian', 'race_group_Native Hawaiian or other Pacific Islander', 'pulm_moderate_No', 'cardiac_nan', 'cyto_score_Poor', 'tce_match_nan', 'tce_div_match_nan', 'peptic_ulcer_nan', 'renal_issue_Yes', 'hla_match_a_high', 'tce_div_match_Bi-directional non-permissive', 'ethnicity_Not Hispanic or Latino', 'melphalan_dose_MEL', 'dri_score_N/A - disease not classifiable', 'cmv_status_-/+', 'obesity_Yes', 'hla_match_b_low', 'gvhd_proph_CSA alone', 'gvhd_proph_Parent Q = yes, but no agent', 'prim_disease_hct_IEA', 'hla_low_res_8', 'tce_imm_match_G/B', 'prod_type_PB', 'pulm_severe_Yes', 'ethnicity_Hispanic or Latino', 'gvhd_proph_CSA + MMF +- others(not FK)', 'tbi_status_TBI +- Other, <=cGy', 'obesity_nan', 'hla_match_c_high', 'prim_disease_hct_CML', 'cmv_status_-/-', 'tbi_status_No TBI', 'gvhd_proph_TDEPLETION +- other', 'conditioning_intensity_MAC', 'hepatic_mild_No', 'prim_disease_hct_IIS', 'cyto_score_nan', 'gvhd_proph_nan', 'pulm_severe_nan', 'in_vivo_tcd_nan', 'hla_match_a_low', 'obesity_Not done', 'dri_score_Intermediate', 'tbi_status_TBI + Cy +- Other', 'prim_disease_hct_Other acute leukemia', 'prod_type_BM', 'race_group_More than one race', 'hla_match_drb1_high', 'tce_div_match_GvH non-permissive', 'tce_div_match_HvG non-permissive', 'ID', 'donor_related_Multiple donor (non-UCB)', 'donor_related_nan', 'dri_score_N/A - pediatric', 'prim_disease_hct_MDS', 'sex_match_nan', 'prim_disease_hct_IPA', 'hla_high_res_8', 'rituximab_Yes', 'peptic_ulcer_Not done', 'cardiac_Not done', 'conditioning_intensity_NMA', 'race_group_White', 'cyto_score_detail_TBD', 'cardiac_No', 'psych_disturb_Not done', 'prior_tumor_No', 'dri_score_Very high', 'hepatic_severe_Not done', 'cardiac_Yes', 'peptic_ulcer_No', 'gvhd_proph_CDselect +- other', 'hepatic_mild_Not done', 'karnofsky_score', 'renal_issue_Not done', 'tbi_status_TBI +- Other, -cGy, fractionated', 'conditioning_intensity_No drugs reported', 'cyto_score_Normal', 'mrd_hct_Positive', 'dri_score_High', 'prim_disease_hct_MPN', 'prim_disease_hct_PCD', 'cyto_score_detail_Not tested', 'pulm_moderate_nan', 'pulm_severe_No', 'tbi_status_TBI +- Other, -cGy, unknown dose', 'mrd_hct_Negative', 'cmv_status_+/+', 'in_vivo_tcd_No', 'gvhd_proph_FK+ MMF +- others', 'ethnicity_nan', 'sex_match_M-M', 'psych_disturb_No', 'tce_imm_match_P/P', 'conditioning_intensity_nan', 'melphalan_dose_nan', 'tce_match_GvH non-permissive', 'prim_disease_hct_AI', 'melphalan_dose_N/A, Mel not given', 'cyto_score_TBD', 'tce_imm_match_H/B', 'diabetes_No', 'hla_low_res_10', 'prim_disease_hct_NHL', 'conditioning_intensity_RIC', 'tce_imm_match_G/G', 'hepatic_severe_Yes', 'hla_match_dqb1_high', 'tce_imm_match_P/B', 'sex_match_F-M', 'arrhythmia_nan', 'psych_disturb_nan', 'tce_imm_match_nan', 'prim_disease_hct_ALL', 'gvhd_proph_FK+ MTX +- others(not MMF)', 'cmv_status_nan', 'hla_match_b_high', 'year_hct', 'dri_score_nan', 'cyto_score_detail_nan', 'vent_hist_Yes', 'gvhd_proph_Other GVHD Prophylaxis', 'hla_high_res_10', 'prior_tumor_nan', 'tce_imm_match_P/G', 'diabetes_Not done', 'rheum_issue_Not done', 'gvhd_proph_Cyclophosphamide +- others', 'cmv_status_+/-', 'tce_imm_match_P/H', 'dri_score_High - TED AML case <missing cytogenetics', 'prim_disease_hct_AML', 'cyto_score_Intermediate', 'gvhd_proph_No GvHD Prophylaxis', 'in_vivo_tcd_Yes', 'hla_match_c_low', 'tce_match_Fully matched', 'gvhd_proph_CDselect alone', 'graft_type_Bone marrow', 'dri_score_TBD cytogenetics', 'obesity_No', 'prim_disease_hct_Solid tumor', 'tce_match_Permissive', 'rheum_issue_Yes', 'graft_type_Peripheral blood', 'pulm_severe_Not done', 'race_group_Black or African-American', 'arrhythmia_Not done', 'hla_match_dqb1_low', 'arrhythmia_No', 'pulm_moderate_Not done', 'cyto_score_detail_Poor', 'ethnicity_Non-resident of the U.S.', 'conditioning_intensity_N/A, F(pre-TED) not submitted', 'dri_score_Missing disease status', 'rheum_issue_No', 'renal_issue_No', 'conditioning_intensity_TBD', 'sex_match_F-F', 'prim_disease_hct_SAA', 'donor_age', 'diabetes_nan', 'prior_tumor_Not done', 'vent_hist_nan', 'gvhd_proph_TDEPLETION alone', 'renal_issue_nan', 'hepatic_severe_No', 'prior_tumor_Yes', 'age_at_hct', 'race_group_American Indian or Alaska Native', 'gvhd_proph_FK+- others(not MMF,MTX)', 'tce_match_HvG non-permissive', 'tbi_status_TBI +- Other, unknown dose', 'donor_related_Related', 'hla_match_drb1_low', 'hepatic_severe_nan', 'sex_match_M-F', 'dri_score_Low', 'tbi_status_TBI +- Other, >cGy', 'donor_related_Unrelated', 'pulm_moderate_Yes', 'gvhd_proph_Cyclophosphamide alone', 'tbi_status_TBI +- Other, -cGy, single', 'gvhd_proph_CSA + MTX +- others(not MMF,FK)'}\n",
            "Columns missing in df1: {'efs_time'}\n"
          ]
        }
      ],
      "source": [
        "# Example DataFrames\n",
        "df1 =X_val\n",
        "\n",
        "df2 = y_train\n",
        "\n",
        "# Get column names of both DataFrames\n",
        "columns_df1 = set(df1.columns)\n",
        "columns_df2 = set(df2.columns)\n",
        "\n",
        "# Find columns missing in df2 but present in df1\n",
        "missing_in_df2 = columns_df1 - columns_df2\n",
        "\n",
        "# Find columns missing in df1 but present in df2\n",
        "missing_in_df1 = columns_df2 - columns_df1\n",
        "\n",
        "# Output the results\n",
        "print(\"Columns missing in df2:\", missing_in_df2)\n",
        "print(\"Columns missing in df1:\", missing_in_df1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ct2qODyvUSK4"
      },
      "outputs": [],
      "source": [
        "#We are no longer using this section because the test data is missing the observed values\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "# with torch.no_grad():\n",
        "#     test_outputs = model(X_test_tensor)\n",
        "#     test_loss = criterion(test_outputs, y_test_tensor)\n",
        "#     print(f'Test Loss: {test_loss.item():.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZ5kGw5VUSIw"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "torch.save(model.state_dict(), 'model.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1nV6ig6yucm"
      },
      "source": [
        "## Output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLtET_WbyxAl"
      },
      "outputs": [],
      "source": [
        "# Generate predictions for the test set\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(X_test_tensor)\n",
        "\n",
        "# Convert to Numpy for proper output\n",
        "test_outputs = test_outputs.numpy().flatten().tolist()\n",
        "\n",
        "# Convert predictions to a DataFrame\n",
        "submission = pd.DataFrame({\n",
        "    'ID': dfTest['ID'],  # Assuming 'ID' is the identifier column\n",
        "    #'efs': test_outputs[:, 0],  # Predicted efs\n",
        "    '# prediction': test_outputs  # Predicted efs_time\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2G6ExhmIGAn_",
        "outputId": "ee082c6f-645e-4589-dcb8-ebd6f1cf189f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      ID  # prediction\n",
            "0  28800     33.041428\n",
            "1  28801     32.950745\n",
            "2  28802     25.749702\n"
          ]
        }
      ],
      "source": [
        "print(submission)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ua-2SGFgrEZ"
      },
      "source": [
        "## Suggested Changes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IE523Q65y2pG"
      },
      "source": [
        "### Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpE5YG-Egt2x",
        "outputId": "4f170809-dd2e-402e-e5df-971e1b8ee1b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch [4449/100000], Validation Loss: 547.8719\n",
            "Epoch [4450/100000], Validation Loss: 544.5656\n",
            "Epoch [4451/100000], Validation Loss: 550.2896\n",
            "Epoch [4452/100000], Validation Loss: 548.2507\n",
            "Epoch [4453/100000], Validation Loss: 551.0410\n",
            "Epoch [4454/100000], Validation Loss: 546.0304\n",
            "Epoch [4455/100000], Validation Loss: 550.4890\n",
            "Epoch [4456/100000], Validation Loss: 559.3771\n",
            "Epoch [4457/100000], Validation Loss: 552.8243\n",
            "Epoch [4458/100000], Validation Loss: 544.6044\n",
            "Epoch [4459/100000], Validation Loss: 544.8429\n",
            "Epoch [4460/100000], Validation Loss: 551.0281\n",
            "Epoch [4461/100000], Validation Loss: 545.2144\n",
            "Epoch [4462/100000], Validation Loss: 549.0888\n",
            "Epoch [4463/100000], Validation Loss: 547.0138\n",
            "Epoch [4464/100000], Validation Loss: 617.7866\n",
            "Epoch [4465/100000], Validation Loss: 551.4327\n",
            "Epoch [4466/100000], Validation Loss: 555.0651\n",
            "Epoch [4467/100000], Validation Loss: 550.7375\n",
            "Epoch [4468/100000], Validation Loss: 547.5479\n",
            "Epoch [4469/100000], Validation Loss: 552.3517\n",
            "Epoch [4470/100000], Validation Loss: 545.9127\n",
            "Epoch [4471/100000], Validation Loss: 543.7884\n",
            "Epoch [4472/100000], Validation Loss: 542.4382\n",
            "Epoch [4473/100000], Validation Loss: 545.0299\n",
            "Epoch [4474/100000], Validation Loss: 546.6147\n",
            "Epoch [4475/100000], Validation Loss: 545.9591\n",
            "Epoch [4476/100000], Validation Loss: 549.4060\n",
            "Epoch [4477/100000], Validation Loss: 546.5960\n",
            "Epoch [4478/100000], Validation Loss: 547.0384\n",
            "Epoch [4479/100000], Validation Loss: 551.8659\n",
            "Epoch [4480/100000], Validation Loss: 543.7792\n",
            "Epoch [4481/100000], Validation Loss: 547.1185\n",
            "Epoch [4482/100000], Validation Loss: 547.5136\n",
            "Epoch [4483/100000], Validation Loss: 551.0420\n",
            "Epoch [4484/100000], Validation Loss: 545.6764\n",
            "Epoch [4485/100000], Validation Loss: 541.7123\n",
            "Epoch [4486/100000], Validation Loss: 552.4152\n",
            "Epoch [4487/100000], Validation Loss: 545.1837\n",
            "Epoch [4488/100000], Validation Loss: 545.3442\n",
            "Epoch [4489/100000], Validation Loss: 551.0629\n",
            "Epoch [4490/100000], Validation Loss: 550.6861\n",
            "Epoch [4491/100000], Validation Loss: 547.7217\n",
            "Epoch [4492/100000], Validation Loss: 544.5885\n",
            "Epoch [4493/100000], Validation Loss: 554.1904\n",
            "Epoch [4494/100000], Validation Loss: 549.6502\n",
            "Epoch [4495/100000], Validation Loss: 550.8046\n",
            "Epoch [4496/100000], Validation Loss: 546.4434\n",
            "Epoch [4497/100000], Validation Loss: 548.3929\n",
            "Epoch [4498/100000], Validation Loss: 551.9402\n",
            "Epoch [4499/100000], Validation Loss: 554.5008\n",
            "Epoch [4500/100000], Validation Loss: 551.8246\n",
            "Epoch [4501/100000], Validation Loss: 544.1135\n",
            "Epoch [4502/100000], Validation Loss: 552.1418\n",
            "Epoch [4503/100000], Validation Loss: 549.3942\n",
            "Epoch [4504/100000], Validation Loss: 547.5854\n",
            "Epoch [4505/100000], Validation Loss: 552.9930\n",
            "Epoch [4506/100000], Validation Loss: 546.3272\n",
            "Epoch [4507/100000], Validation Loss: 544.0780\n",
            "Epoch [4508/100000], Validation Loss: 540.9660\n",
            "Epoch [4509/100000], Validation Loss: 544.7375\n",
            "Epoch [4510/100000], Validation Loss: 551.0410\n",
            "Epoch [4511/100000], Validation Loss: 547.7448\n",
            "Epoch [4512/100000], Validation Loss: 547.8208\n",
            "Epoch [4513/100000], Validation Loss: 545.1959\n",
            "Epoch [4514/100000], Validation Loss: 547.5206\n",
            "Epoch [4515/100000], Validation Loss: 550.9565\n",
            "Epoch [4516/100000], Validation Loss: 543.7292\n",
            "Epoch [4517/100000], Validation Loss: 546.8415\n",
            "Epoch [4518/100000], Validation Loss: 542.6359\n",
            "Epoch [4519/100000], Validation Loss: 549.2659\n",
            "Epoch [4520/100000], Validation Loss: 545.7406\n",
            "Epoch [4521/100000], Validation Loss: 549.1326\n",
            "Epoch [4522/100000], Validation Loss: 547.7048\n",
            "Epoch [4523/100000], Validation Loss: 546.5174\n",
            "Epoch [4524/100000], Validation Loss: 547.1090\n",
            "Epoch [4525/100000], Validation Loss: 545.4603\n",
            "Epoch [4526/100000], Validation Loss: 547.8496\n",
            "Epoch [4527/100000], Validation Loss: 553.7364\n",
            "Epoch [4528/100000], Validation Loss: 548.0611\n",
            "Epoch [4529/100000], Validation Loss: 547.9409\n",
            "Epoch [4530/100000], Validation Loss: 542.6932\n",
            "Epoch [4531/100000], Validation Loss: 545.5829\n",
            "Epoch [4532/100000], Validation Loss: 549.9266\n",
            "Epoch [4533/100000], Validation Loss: 548.5927\n",
            "Epoch [4534/100000], Validation Loss: 552.7105\n",
            "Epoch [4535/100000], Validation Loss: 547.8225\n",
            "Epoch [4536/100000], Validation Loss: 548.9506\n",
            "Epoch [4537/100000], Validation Loss: 543.5277\n",
            "Epoch [4538/100000], Validation Loss: 547.8179\n",
            "Epoch [4539/100000], Validation Loss: 545.4985\n",
            "Epoch [4540/100000], Validation Loss: 554.4593\n",
            "Epoch [4541/100000], Validation Loss: 583.1426\n",
            "Epoch [4542/100000], Validation Loss: 542.3350\n",
            "Epoch [4543/100000], Validation Loss: 554.8214\n",
            "Epoch [4544/100000], Validation Loss: 547.9067\n",
            "Epoch [4545/100000], Validation Loss: 551.6705\n",
            "Epoch [4546/100000], Validation Loss: 549.1909\n",
            "Epoch [4547/100000], Validation Loss: 555.7001\n",
            "Epoch [4548/100000], Validation Loss: 549.8110\n",
            "Epoch [4549/100000], Validation Loss: 548.6561\n",
            "Epoch [4550/100000], Validation Loss: 547.6820\n",
            "Epoch [4551/100000], Validation Loss: 554.0005\n",
            "Epoch [4552/100000], Validation Loss: 554.0983\n",
            "Epoch [4553/100000], Validation Loss: 565.1322\n",
            "Epoch [4554/100000], Validation Loss: 550.9244\n",
            "Epoch [4555/100000], Validation Loss: 548.3900\n",
            "Epoch [4556/100000], Validation Loss: 550.3207\n",
            "Epoch [4557/100000], Validation Loss: 548.8655\n",
            "Epoch [4558/100000], Validation Loss: 545.6967\n",
            "Epoch [4559/100000], Validation Loss: 553.9005\n",
            "Epoch [4560/100000], Validation Loss: 545.2036\n",
            "Epoch [4561/100000], Validation Loss: 553.2938\n",
            "Epoch [4562/100000], Validation Loss: 550.1063\n",
            "Epoch [4563/100000], Validation Loss: 546.1422\n",
            "Epoch [4564/100000], Validation Loss: 550.5150\n",
            "Epoch [4565/100000], Validation Loss: 557.0139\n",
            "Epoch [4566/100000], Validation Loss: 545.0580\n",
            "Epoch [4567/100000], Validation Loss: 546.9313\n",
            "Epoch [4568/100000], Validation Loss: 552.5968\n",
            "Epoch [4569/100000], Validation Loss: 546.8027\n",
            "Epoch [4570/100000], Validation Loss: 547.3778\n",
            "Epoch [4571/100000], Validation Loss: 549.3770\n",
            "Epoch [4572/100000], Validation Loss: 542.9756\n",
            "Epoch [4573/100000], Validation Loss: 546.2674\n",
            "Epoch [4574/100000], Validation Loss: 548.5293\n",
            "Epoch [4575/100000], Validation Loss: 544.1211\n",
            "Epoch [4576/100000], Validation Loss: 548.9018\n",
            "Epoch [4577/100000], Validation Loss: 547.2259\n",
            "Epoch [4578/100000], Validation Loss: 549.1347\n",
            "Epoch [4579/100000], Validation Loss: 542.9363\n",
            "Epoch [4580/100000], Validation Loss: 548.3590\n",
            "Epoch [4581/100000], Validation Loss: 547.2867\n",
            "Epoch [4582/100000], Validation Loss: 544.8669\n",
            "Epoch [4583/100000], Validation Loss: 547.7800\n",
            "Epoch [4584/100000], Validation Loss: 543.9655\n",
            "Epoch [4585/100000], Validation Loss: 549.5982\n",
            "Epoch [4586/100000], Validation Loss: 548.3131\n",
            "Epoch [4587/100000], Validation Loss: 550.2858\n",
            "Epoch [4588/100000], Validation Loss: 549.8478\n",
            "Epoch [4589/100000], Validation Loss: 546.2992\n",
            "Epoch [4590/100000], Validation Loss: 552.6343\n",
            "Epoch [4591/100000], Validation Loss: 543.9589\n",
            "Epoch [4592/100000], Validation Loss: 548.2333\n",
            "Epoch [4593/100000], Validation Loss: 542.5423\n",
            "Epoch [4594/100000], Validation Loss: 551.1455\n",
            "Epoch [4595/100000], Validation Loss: 548.6672\n",
            "Epoch [4596/100000], Validation Loss: 543.2102\n",
            "Epoch [4597/100000], Validation Loss: 544.9417\n",
            "Epoch [4598/100000], Validation Loss: 545.4504\n",
            "Epoch [4599/100000], Validation Loss: 548.6018\n",
            "Epoch [4600/100000], Validation Loss: 560.2685\n",
            "Epoch [4601/100000], Validation Loss: 552.6375\n",
            "Epoch [4602/100000], Validation Loss: 545.0467\n",
            "Epoch [4603/100000], Validation Loss: 545.6008\n",
            "Epoch [4604/100000], Validation Loss: 549.7858\n",
            "Epoch [4605/100000], Validation Loss: 551.6639\n",
            "Epoch [4606/100000], Validation Loss: 542.5448\n",
            "Epoch [4607/100000], Validation Loss: 552.0234\n",
            "Epoch [4608/100000], Validation Loss: 552.4427\n",
            "Epoch [4609/100000], Validation Loss: 545.5659\n",
            "Epoch [4610/100000], Validation Loss: 543.8254\n",
            "Epoch [4611/100000], Validation Loss: 544.2469\n",
            "Epoch [4612/100000], Validation Loss: 544.5147\n",
            "Epoch [4613/100000], Validation Loss: 549.8215\n",
            "Epoch [4614/100000], Validation Loss: 551.1522\n",
            "Epoch [4615/100000], Validation Loss: 546.8772\n",
            "Epoch [4616/100000], Validation Loss: 550.6626\n",
            "Epoch [4617/100000], Validation Loss: 551.1735\n",
            "Epoch [4618/100000], Validation Loss: 546.1997\n",
            "Epoch [4619/100000], Validation Loss: 556.3711\n",
            "Epoch [4620/100000], Validation Loss: 544.2555\n",
            "Epoch [4621/100000], Validation Loss: 547.4470\n",
            "Epoch [4622/100000], Validation Loss: 547.2219\n",
            "Epoch [4623/100000], Validation Loss: 549.4682\n",
            "Epoch [4624/100000], Validation Loss: 551.1344\n",
            "Epoch [4625/100000], Validation Loss: 548.4630\n",
            "Epoch [4626/100000], Validation Loss: 543.3086\n",
            "Epoch [4627/100000], Validation Loss: 548.7616\n",
            "Epoch [4628/100000], Validation Loss: 544.6638\n",
            "Epoch [4629/100000], Validation Loss: 551.2418\n",
            "Epoch [4630/100000], Validation Loss: 548.9012\n",
            "Epoch [4631/100000], Validation Loss: 548.2230\n",
            "Epoch [4632/100000], Validation Loss: 553.4553\n",
            "Epoch [4633/100000], Validation Loss: 545.3951\n",
            "Epoch [4634/100000], Validation Loss: 553.5091\n",
            "Epoch [4635/100000], Validation Loss: 547.8521\n",
            "Epoch [4636/100000], Validation Loss: 546.2434\n",
            "Epoch [4637/100000], Validation Loss: 547.4623\n",
            "Epoch [4638/100000], Validation Loss: 550.1694\n",
            "Epoch [4639/100000], Validation Loss: 546.6303\n",
            "Epoch [4640/100000], Validation Loss: 560.9610\n",
            "Epoch [4641/100000], Validation Loss: 552.2714\n",
            "Epoch [4642/100000], Validation Loss: 543.3598\n",
            "Epoch [4643/100000], Validation Loss: 547.0595\n",
            "Epoch [4644/100000], Validation Loss: 549.4818\n",
            "Epoch [4645/100000], Validation Loss: 546.4954\n",
            "Epoch [4646/100000], Validation Loss: 544.0351\n",
            "Epoch [4647/100000], Validation Loss: 544.0201\n",
            "Epoch [4648/100000], Validation Loss: 546.4511\n",
            "Epoch [4649/100000], Validation Loss: 548.1666\n",
            "Epoch [4650/100000], Validation Loss: 548.0134\n",
            "Epoch [4651/100000], Validation Loss: 545.3984\n",
            "Epoch [4652/100000], Validation Loss: 550.6467\n",
            "Epoch [4653/100000], Validation Loss: 546.8865\n",
            "Epoch [4654/100000], Validation Loss: 546.9696\n",
            "Epoch [4655/100000], Validation Loss: 547.8973\n",
            "Epoch [4656/100000], Validation Loss: 544.5676\n",
            "Epoch [4657/100000], Validation Loss: 550.9351\n",
            "Epoch [4658/100000], Validation Loss: 550.5028\n",
            "Epoch [4659/100000], Validation Loss: 548.5135\n",
            "Epoch [4660/100000], Validation Loss: 544.2239\n",
            "Epoch [4661/100000], Validation Loss: 548.4688\n",
            "Epoch [4662/100000], Validation Loss: 551.9939\n",
            "Epoch [4663/100000], Validation Loss: 544.6811\n",
            "Epoch [4664/100000], Validation Loss: 551.5665\n",
            "Epoch [4665/100000], Validation Loss: 545.8952\n",
            "Epoch [4666/100000], Validation Loss: 553.9126\n",
            "Epoch [4667/100000], Validation Loss: 549.5771\n",
            "Epoch [4668/100000], Validation Loss: 549.6850\n",
            "Epoch [4669/100000], Validation Loss: 541.3261\n",
            "Epoch [4670/100000], Validation Loss: 542.4595\n",
            "Epoch [4671/100000], Validation Loss: 546.8054\n",
            "Epoch [4672/100000], Validation Loss: 548.4250\n",
            "Epoch [4673/100000], Validation Loss: 550.1896\n",
            "Epoch [4674/100000], Validation Loss: 551.9475\n",
            "Epoch [4675/100000], Validation Loss: 551.0020\n",
            "Epoch [4676/100000], Validation Loss: 546.7976\n",
            "Epoch [4677/100000], Validation Loss: 540.6423\n",
            "Epoch [4678/100000], Validation Loss: 544.0577\n",
            "Epoch [4679/100000], Validation Loss: 546.0902\n",
            "Epoch [4680/100000], Validation Loss: 546.6098\n",
            "Epoch [4681/100000], Validation Loss: 546.8975\n",
            "Epoch [4682/100000], Validation Loss: 547.3523\n",
            "Epoch [4683/100000], Validation Loss: 546.3084\n",
            "Epoch [4684/100000], Validation Loss: 550.4633\n",
            "Epoch [4685/100000], Validation Loss: 554.2778\n",
            "Epoch [4686/100000], Validation Loss: 552.9804\n",
            "Epoch [4687/100000], Validation Loss: 547.7049\n",
            "Epoch [4688/100000], Validation Loss: 547.7586\n",
            "Epoch [4689/100000], Validation Loss: 551.7039\n",
            "Epoch [4690/100000], Validation Loss: 552.6042\n",
            "Epoch [4691/100000], Validation Loss: 549.6912\n",
            "Epoch [4692/100000], Validation Loss: 547.5439\n",
            "Epoch [4693/100000], Validation Loss: 558.7852\n",
            "Epoch [4694/100000], Validation Loss: 550.6972\n",
            "Epoch [4695/100000], Validation Loss: 556.0710\n",
            "Epoch [4696/100000], Validation Loss: 547.3556\n",
            "Epoch [4697/100000], Validation Loss: 549.4362\n",
            "Epoch [4698/100000], Validation Loss: 546.7583\n",
            "Epoch [4699/100000], Validation Loss: 554.0842\n",
            "Epoch [4700/100000], Validation Loss: 548.8452\n",
            "Epoch [4701/100000], Validation Loss: 546.2649\n",
            "Epoch [4702/100000], Validation Loss: 546.4797\n",
            "Epoch [4703/100000], Validation Loss: 548.9633\n",
            "Epoch [4704/100000], Validation Loss: 547.8886\n",
            "Epoch [4705/100000], Validation Loss: 543.1246\n",
            "Epoch [4706/100000], Validation Loss: 544.1915\n",
            "Epoch [4707/100000], Validation Loss: 548.8449\n",
            "Epoch [4708/100000], Validation Loss: 551.9399\n",
            "Epoch [4709/100000], Validation Loss: 546.3957\n",
            "Epoch [4710/100000], Validation Loss: 546.5348\n",
            "Epoch [4711/100000], Validation Loss: 544.5201\n",
            "Epoch [4712/100000], Validation Loss: 543.4711\n",
            "Epoch [4713/100000], Validation Loss: 556.8892\n",
            "Epoch [4714/100000], Validation Loss: 546.6404\n",
            "Epoch [4715/100000], Validation Loss: 551.1353\n",
            "Epoch [4716/100000], Validation Loss: 549.2531\n",
            "Epoch [4717/100000], Validation Loss: 549.4936\n",
            "Epoch [4718/100000], Validation Loss: 550.0299\n",
            "Epoch [4719/100000], Validation Loss: 546.7625\n",
            "Epoch [4720/100000], Validation Loss: 545.4555\n",
            "Epoch [4721/100000], Validation Loss: 549.9755\n",
            "Epoch [4722/100000], Validation Loss: 552.1079\n",
            "Epoch [4723/100000], Validation Loss: 544.7920\n",
            "Epoch [4724/100000], Validation Loss: 553.7014\n",
            "Epoch [4725/100000], Validation Loss: 548.7983\n",
            "Epoch [4726/100000], Validation Loss: 555.3200\n",
            "Epoch [4727/100000], Validation Loss: 547.3485\n",
            "Epoch [4728/100000], Validation Loss: 545.3400\n",
            "Epoch [4729/100000], Validation Loss: 540.2486\n",
            "Epoch [4730/100000], Validation Loss: 553.1995\n",
            "Epoch [4731/100000], Validation Loss: 548.7644\n",
            "Epoch [4732/100000], Validation Loss: 545.8998\n",
            "Epoch [4733/100000], Validation Loss: 554.7850\n",
            "Epoch [4734/100000], Validation Loss: 569.9278\n",
            "Epoch [4735/100000], Validation Loss: 544.0566\n",
            "Epoch [4736/100000], Validation Loss: 549.5923\n",
            "Epoch [4737/100000], Validation Loss: 551.4517\n",
            "Epoch [4738/100000], Validation Loss: 550.8998\n",
            "Epoch [4739/100000], Validation Loss: 547.8732\n",
            "Epoch [4740/100000], Validation Loss: 554.4961\n",
            "Epoch [4741/100000], Validation Loss: 555.2560\n",
            "Epoch [4742/100000], Validation Loss: 548.1077\n",
            "Epoch [4743/100000], Validation Loss: 550.6034\n",
            "Epoch [4744/100000], Validation Loss: 554.5069\n",
            "Epoch [4745/100000], Validation Loss: 556.1061\n",
            "Epoch [4746/100000], Validation Loss: 542.0189\n",
            "Epoch [4747/100000], Validation Loss: 544.1902\n",
            "Epoch [4748/100000], Validation Loss: 549.6648\n",
            "Epoch [4749/100000], Validation Loss: 547.2716\n",
            "Epoch [4750/100000], Validation Loss: 547.1986\n",
            "Epoch [4751/100000], Validation Loss: 547.6669\n",
            "Epoch [4752/100000], Validation Loss: 553.9500\n",
            "Epoch [4753/100000], Validation Loss: 548.3269\n",
            "Epoch [4754/100000], Validation Loss: 548.5985\n",
            "Epoch [4755/100000], Validation Loss: 548.2681\n",
            "Epoch [4756/100000], Validation Loss: 548.4213\n",
            "Epoch [4757/100000], Validation Loss: 547.9435\n",
            "Epoch [4758/100000], Validation Loss: 546.0444\n",
            "Epoch [4759/100000], Validation Loss: 548.4701\n",
            "Epoch [4760/100000], Validation Loss: 560.1167\n",
            "Epoch [4761/100000], Validation Loss: 556.7718\n",
            "Epoch [4762/100000], Validation Loss: 549.2358\n",
            "Epoch [4763/100000], Validation Loss: 549.6633\n",
            "Epoch [4764/100000], Validation Loss: 546.6140\n",
            "Epoch [4765/100000], Validation Loss: 541.7249\n",
            "Epoch [4766/100000], Validation Loss: 554.0858\n",
            "Epoch [4767/100000], Validation Loss: 545.4871\n",
            "Epoch [4768/100000], Validation Loss: 547.3705\n",
            "Epoch [4769/100000], Validation Loss: 541.0813\n",
            "Epoch [4770/100000], Validation Loss: 546.5376\n",
            "Epoch [4771/100000], Validation Loss: 547.2206\n",
            "Epoch [4772/100000], Validation Loss: 547.1363\n",
            "Epoch [4773/100000], Validation Loss: 546.3849\n",
            "Epoch [4774/100000], Validation Loss: 549.4266\n",
            "Epoch [4775/100000], Validation Loss: 544.9637\n",
            "Epoch [4776/100000], Validation Loss: 554.1108\n",
            "Epoch [4777/100000], Validation Loss: 552.9313\n",
            "Epoch [4778/100000], Validation Loss: 549.2254\n",
            "Epoch [4779/100000], Validation Loss: 546.7295\n",
            "Epoch [4780/100000], Validation Loss: 557.6048\n",
            "Epoch [4781/100000], Validation Loss: 548.3685\n",
            "Epoch [4782/100000], Validation Loss: 546.0013\n",
            "Epoch [4783/100000], Validation Loss: 546.3686\n",
            "Epoch [4784/100000], Validation Loss: 547.8780\n",
            "Epoch [4785/100000], Validation Loss: 546.0135\n",
            "Epoch [4786/100000], Validation Loss: 544.4875\n",
            "Epoch [4787/100000], Validation Loss: 546.1078\n",
            "Epoch [4788/100000], Validation Loss: 550.5085\n",
            "Epoch [4789/100000], Validation Loss: 544.2491\n",
            "Epoch [4790/100000], Validation Loss: 545.8119\n",
            "Epoch [4791/100000], Validation Loss: 543.3690\n",
            "Epoch [4792/100000], Validation Loss: 555.2208\n",
            "Epoch [4793/100000], Validation Loss: 549.7144\n",
            "Epoch [4794/100000], Validation Loss: 548.0663\n",
            "Epoch [4795/100000], Validation Loss: 546.4949\n",
            "Epoch [4796/100000], Validation Loss: 545.2996\n",
            "Epoch [4797/100000], Validation Loss: 548.3120\n",
            "Epoch [4798/100000], Validation Loss: 548.7863\n",
            "Epoch [4799/100000], Validation Loss: 573.7240\n",
            "Epoch [4800/100000], Validation Loss: 547.5544\n",
            "Epoch [4801/100000], Validation Loss: 543.0476\n",
            "Epoch [4802/100000], Validation Loss: 546.8583\n",
            "Epoch [4803/100000], Validation Loss: 553.5627\n",
            "Epoch [4804/100000], Validation Loss: 546.1330\n",
            "Epoch [4805/100000], Validation Loss: 547.6335\n",
            "Epoch [4806/100000], Validation Loss: 550.8695\n",
            "Epoch [4807/100000], Validation Loss: 545.1698\n",
            "Epoch [4808/100000], Validation Loss: 545.8093\n",
            "Epoch [4809/100000], Validation Loss: 543.7743\n",
            "Epoch [4810/100000], Validation Loss: 554.1295\n",
            "Epoch [4811/100000], Validation Loss: 549.6111\n",
            "Epoch [4812/100000], Validation Loss: 552.2130\n",
            "Epoch [4813/100000], Validation Loss: 543.7072\n",
            "Epoch [4814/100000], Validation Loss: 547.1041\n",
            "Epoch [4815/100000], Validation Loss: 555.9016\n",
            "Epoch [4816/100000], Validation Loss: 555.7674\n",
            "Epoch [4817/100000], Validation Loss: 548.8889\n",
            "Epoch [4818/100000], Validation Loss: 548.6440\n",
            "Epoch [4819/100000], Validation Loss: 554.2642\n",
            "Epoch [4820/100000], Validation Loss: 549.1648\n",
            "Epoch [4821/100000], Validation Loss: 546.0223\n",
            "Epoch [4822/100000], Validation Loss: 546.8436\n",
            "Epoch [4823/100000], Validation Loss: 546.8010\n",
            "Epoch [4824/100000], Validation Loss: 548.0613\n",
            "Epoch [4825/100000], Validation Loss: 546.9189\n",
            "Epoch [4826/100000], Validation Loss: 545.4630\n",
            "Epoch [4827/100000], Validation Loss: 542.6314\n",
            "Epoch [4828/100000], Validation Loss: 550.6894\n",
            "Epoch [4829/100000], Validation Loss: 545.7566\n",
            "Epoch [4830/100000], Validation Loss: 550.3379\n",
            "Epoch [4831/100000], Validation Loss: 545.8487\n",
            "Epoch [4832/100000], Validation Loss: 543.5749\n",
            "Epoch [4833/100000], Validation Loss: 555.8238\n",
            "Epoch [4834/100000], Validation Loss: 545.6255\n",
            "Epoch [4835/100000], Validation Loss: 546.5796\n",
            "Epoch [4836/100000], Validation Loss: 548.6818\n",
            "Epoch [4837/100000], Validation Loss: 551.9615\n",
            "Epoch [4838/100000], Validation Loss: 555.5171\n",
            "Epoch [4839/100000], Validation Loss: 548.3426\n",
            "Epoch [4840/100000], Validation Loss: 550.8359\n",
            "Epoch [4841/100000], Validation Loss: 548.0080\n",
            "Epoch [4842/100000], Validation Loss: 545.5372\n",
            "Epoch [4843/100000], Validation Loss: 541.6801\n",
            "Epoch [4844/100000], Validation Loss: 547.7841\n",
            "Epoch [4845/100000], Validation Loss: 550.8091\n",
            "Epoch [4846/100000], Validation Loss: 543.9311\n",
            "Epoch [4847/100000], Validation Loss: 545.3838\n",
            "Epoch [4848/100000], Validation Loss: 549.6842\n",
            "Epoch [4849/100000], Validation Loss: 553.7705\n",
            "Epoch [4850/100000], Validation Loss: 551.9149\n",
            "Epoch [4851/100000], Validation Loss: 543.2988\n",
            "Epoch [4852/100000], Validation Loss: 550.3974\n",
            "Epoch [4853/100000], Validation Loss: 550.9199\n",
            "Epoch [4854/100000], Validation Loss: 550.6771\n",
            "Epoch [4855/100000], Validation Loss: 557.3249\n",
            "Epoch [4856/100000], Validation Loss: 554.0106\n",
            "Epoch [4857/100000], Validation Loss: 544.3388\n",
            "Epoch [4858/100000], Validation Loss: 546.8482\n",
            "Epoch [4859/100000], Validation Loss: 545.4486\n",
            "Epoch [4860/100000], Validation Loss: 553.6351\n",
            "Epoch [4861/100000], Validation Loss: 544.8929\n",
            "Epoch [4862/100000], Validation Loss: 549.4251\n",
            "Epoch [4863/100000], Validation Loss: 547.6459\n",
            "Epoch [4864/100000], Validation Loss: 548.7876\n",
            "Epoch [4865/100000], Validation Loss: 543.8906\n",
            "Epoch [4866/100000], Validation Loss: 545.2212\n",
            "Epoch [4867/100000], Validation Loss: 548.5868\n",
            "Epoch [4868/100000], Validation Loss: 547.2334\n",
            "Epoch [4869/100000], Validation Loss: 551.3528\n",
            "Epoch [4870/100000], Validation Loss: 552.5319\n",
            "Epoch [4871/100000], Validation Loss: 552.4931\n",
            "Epoch [4872/100000], Validation Loss: 553.3427\n",
            "Epoch [4873/100000], Validation Loss: 544.9232\n",
            "Epoch [4874/100000], Validation Loss: 547.0310\n",
            "Epoch [4875/100000], Validation Loss: 543.6709\n",
            "Epoch [4876/100000], Validation Loss: 549.9693\n",
            "Epoch [4877/100000], Validation Loss: 548.8818\n",
            "Epoch [4878/100000], Validation Loss: 547.2620\n",
            "Epoch [4879/100000], Validation Loss: 548.1321\n",
            "Epoch [4880/100000], Validation Loss: 545.7896\n",
            "Epoch [4881/100000], Validation Loss: 545.6807\n",
            "Epoch [4882/100000], Validation Loss: 548.9312\n",
            "Epoch [4883/100000], Validation Loss: 545.1551\n",
            "Epoch [4884/100000], Validation Loss: 549.4186\n",
            "Epoch [4885/100000], Validation Loss: 545.8349\n",
            "Epoch [4886/100000], Validation Loss: 546.0810\n",
            "Epoch [4887/100000], Validation Loss: 557.8593\n",
            "Epoch [4888/100000], Validation Loss: 546.1590\n",
            "Epoch [4889/100000], Validation Loss: 552.1268\n",
            "Epoch [4890/100000], Validation Loss: 550.6224\n",
            "Epoch [4891/100000], Validation Loss: 549.8912\n",
            "Epoch [4892/100000], Validation Loss: 546.6330\n",
            "Epoch [4893/100000], Validation Loss: 545.6251\n",
            "Epoch [4894/100000], Validation Loss: 548.2556\n",
            "Epoch [4895/100000], Validation Loss: 547.8795\n",
            "Epoch [4896/100000], Validation Loss: 553.5244\n",
            "Epoch [4897/100000], Validation Loss: 546.9964\n",
            "Epoch [4898/100000], Validation Loss: 547.4515\n",
            "Epoch [4899/100000], Validation Loss: 548.6989\n",
            "Epoch [4900/100000], Validation Loss: 552.9130\n",
            "Epoch [4901/100000], Validation Loss: 548.3301\n",
            "Epoch [4902/100000], Validation Loss: 547.8032\n",
            "Epoch [4903/100000], Validation Loss: 552.8343\n",
            "Epoch [4904/100000], Validation Loss: 547.3666\n",
            "Epoch [4905/100000], Validation Loss: 545.3897\n",
            "Epoch [4906/100000], Validation Loss: 546.0633\n",
            "Epoch [4907/100000], Validation Loss: 552.8239\n",
            "Epoch [4908/100000], Validation Loss: 548.4519\n",
            "Epoch [4909/100000], Validation Loss: 549.0533\n",
            "Epoch [4910/100000], Validation Loss: 557.1608\n",
            "Epoch [4911/100000], Validation Loss: 554.5443\n",
            "Epoch [4912/100000], Validation Loss: 549.2848\n",
            "Epoch [4913/100000], Validation Loss: 545.3990\n",
            "Epoch [4914/100000], Validation Loss: 551.3490\n",
            "Epoch [4915/100000], Validation Loss: 539.2830\n",
            "Epoch [4916/100000], Validation Loss: 548.0713\n",
            "Epoch [4917/100000], Validation Loss: 544.3435\n",
            "Epoch [4918/100000], Validation Loss: 568.6328\n",
            "Epoch [4919/100000], Validation Loss: 545.6872\n",
            "Epoch [4920/100000], Validation Loss: 545.0199\n",
            "Epoch [4921/100000], Validation Loss: 554.4083\n",
            "Epoch [4922/100000], Validation Loss: 547.8324\n",
            "Epoch [4923/100000], Validation Loss: 544.0158\n",
            "Epoch [4924/100000], Validation Loss: 544.7545\n",
            "Epoch [4925/100000], Validation Loss: 551.4453\n",
            "Epoch [4926/100000], Validation Loss: 548.0586\n",
            "Epoch [4927/100000], Validation Loss: 547.6060\n",
            "Epoch [4928/100000], Validation Loss: 546.9368\n",
            "Epoch [4929/100000], Validation Loss: 550.7141\n",
            "Epoch [4930/100000], Validation Loss: 547.5827\n",
            "Epoch [4931/100000], Validation Loss: 547.2175\n",
            "Epoch [4932/100000], Validation Loss: 548.0564\n",
            "Epoch [4933/100000], Validation Loss: 546.2499\n",
            "Epoch [4934/100000], Validation Loss: 543.2633\n",
            "Epoch [4935/100000], Validation Loss: 551.1770\n",
            "Epoch [4936/100000], Validation Loss: 546.0351\n",
            "Epoch [4937/100000], Validation Loss: 546.6747\n",
            "Epoch [4938/100000], Validation Loss: 549.8547\n",
            "Epoch [4939/100000], Validation Loss: 556.1623\n",
            "Epoch [4940/100000], Validation Loss: 550.2856\n",
            "Epoch [4941/100000], Validation Loss: 548.8809\n",
            "Epoch [4942/100000], Validation Loss: 553.2893\n",
            "Epoch [4943/100000], Validation Loss: 545.1232\n",
            "Epoch [4944/100000], Validation Loss: 547.8621\n",
            "Epoch [4945/100000], Validation Loss: 551.3877\n",
            "Epoch [4946/100000], Validation Loss: 549.8223\n",
            "Epoch [4947/100000], Validation Loss: 548.7955\n",
            "Epoch [4948/100000], Validation Loss: 540.6842\n",
            "Epoch [4949/100000], Validation Loss: 548.3012\n",
            "Epoch [4950/100000], Validation Loss: 544.5216\n",
            "Epoch [4951/100000], Validation Loss: 551.2677\n",
            "Epoch [4952/100000], Validation Loss: 544.1231\n",
            "Epoch [4953/100000], Validation Loss: 548.2321\n",
            "Epoch [4954/100000], Validation Loss: 550.7199\n",
            "Epoch [4955/100000], Validation Loss: 544.3186\n",
            "Epoch [4956/100000], Validation Loss: 553.5486\n",
            "Epoch [4957/100000], Validation Loss: 550.6867\n",
            "Epoch [4958/100000], Validation Loss: 548.3078\n",
            "Epoch [4959/100000], Validation Loss: 549.7968\n",
            "Epoch [4960/100000], Validation Loss: 546.0333\n",
            "Epoch [4961/100000], Validation Loss: 555.8866\n",
            "Epoch [4962/100000], Validation Loss: 546.4048\n",
            "Epoch [4963/100000], Validation Loss: 545.9507\n",
            "Epoch [4964/100000], Validation Loss: 546.1497\n",
            "Epoch [4965/100000], Validation Loss: 549.5620\n",
            "Epoch [4966/100000], Validation Loss: 549.8553\n",
            "Epoch [4967/100000], Validation Loss: 551.8192\n",
            "Epoch [4968/100000], Validation Loss: 547.0148\n",
            "Epoch [4969/100000], Validation Loss: 547.4834\n",
            "Epoch [4970/100000], Validation Loss: 552.9709\n",
            "Epoch [4971/100000], Validation Loss: 546.3499\n",
            "Epoch [4972/100000], Validation Loss: 547.4162\n",
            "Epoch [4973/100000], Validation Loss: 550.8229\n",
            "Epoch [4974/100000], Validation Loss: 549.8615\n",
            "Epoch [4975/100000], Validation Loss: 548.8802\n",
            "Epoch [4976/100000], Validation Loss: 546.6861\n",
            "Epoch [4977/100000], Validation Loss: 549.1014\n",
            "Epoch [4978/100000], Validation Loss: 545.1818\n",
            "Epoch [4979/100000], Validation Loss: 545.7341\n",
            "Epoch [4980/100000], Validation Loss: 546.9850\n",
            "Epoch [4981/100000], Validation Loss: 547.3963\n",
            "Epoch [4982/100000], Validation Loss: 548.7177\n",
            "Epoch [4983/100000], Validation Loss: 550.4303\n",
            "Epoch [4984/100000], Validation Loss: 551.4537\n",
            "Epoch [4985/100000], Validation Loss: 547.3638\n",
            "Epoch [4986/100000], Validation Loss: 547.6986\n",
            "Epoch [4987/100000], Validation Loss: 548.9208\n",
            "Epoch [4988/100000], Validation Loss: 547.6448\n",
            "Epoch [4989/100000], Validation Loss: 549.8052\n",
            "Epoch [4990/100000], Validation Loss: 553.4070\n",
            "Epoch [4991/100000], Validation Loss: 543.6524\n",
            "Epoch [4992/100000], Validation Loss: 545.4134\n",
            "Epoch [4993/100000], Validation Loss: 556.7260\n",
            "Epoch [4994/100000], Validation Loss: 551.9173\n",
            "Epoch [4995/100000], Validation Loss: 548.3866\n",
            "Epoch [4996/100000], Validation Loss: 548.9387\n",
            "Epoch [4997/100000], Validation Loss: 546.3943\n",
            "Epoch [4998/100000], Validation Loss: 544.8035\n",
            "Epoch [4999/100000], Validation Loss: 548.9976\n",
            "Epoch [5000/100000], Validation Loss: 547.4159\n",
            "Epoch [5001/100000], Validation Loss: 544.5701\n",
            "Epoch [5002/100000], Validation Loss: 547.5251\n",
            "Epoch [5003/100000], Validation Loss: 552.6473\n",
            "Epoch [5004/100000], Validation Loss: 550.6434\n",
            "Epoch [5005/100000], Validation Loss: 544.1001\n",
            "Epoch [5006/100000], Validation Loss: 545.8467\n",
            "Epoch [5007/100000], Validation Loss: 543.3177\n",
            "Epoch [5008/100000], Validation Loss: 550.9976\n",
            "Epoch [5009/100000], Validation Loss: 547.7944\n",
            "Epoch [5010/100000], Validation Loss: 550.0279\n",
            "Epoch [5011/100000], Validation Loss: 546.1170\n",
            "Epoch [5012/100000], Validation Loss: 553.3494\n",
            "Epoch [5013/100000], Validation Loss: 546.4571\n",
            "Epoch [5014/100000], Validation Loss: 549.1517\n",
            "Epoch [5015/100000], Validation Loss: 549.3097\n",
            "Epoch [5016/100000], Validation Loss: 542.9903\n",
            "Epoch [5017/100000], Validation Loss: 545.9714\n",
            "Epoch [5018/100000], Validation Loss: 547.6353\n",
            "Epoch [5019/100000], Validation Loss: 543.3618\n",
            "Epoch [5020/100000], Validation Loss: 546.0597\n",
            "Epoch [5021/100000], Validation Loss: 548.6445\n",
            "Epoch [5022/100000], Validation Loss: 546.3199\n",
            "Epoch [5023/100000], Validation Loss: 547.5308\n",
            "Epoch [5024/100000], Validation Loss: 548.8119\n",
            "Epoch [5025/100000], Validation Loss: 548.3042\n",
            "Epoch [5026/100000], Validation Loss: 552.7838\n",
            "Epoch [5027/100000], Validation Loss: 548.8386\n",
            "Epoch [5028/100000], Validation Loss: 545.8200\n",
            "Epoch [5029/100000], Validation Loss: 546.6978\n",
            "Epoch [5030/100000], Validation Loss: 547.5588\n",
            "Epoch [5031/100000], Validation Loss: 545.4332\n",
            "Epoch [5032/100000], Validation Loss: 545.5654\n",
            "Epoch [5033/100000], Validation Loss: 583.6708\n",
            "Epoch [5034/100000], Validation Loss: 551.9892\n",
            "Epoch [5035/100000], Validation Loss: 545.6552\n",
            "Epoch [5036/100000], Validation Loss: 547.7609\n",
            "Epoch [5037/100000], Validation Loss: 547.8118\n",
            "Epoch [5038/100000], Validation Loss: 547.7000\n",
            "Epoch [5039/100000], Validation Loss: 550.3076\n",
            "Epoch [5040/100000], Validation Loss: 549.3634\n",
            "Epoch [5041/100000], Validation Loss: 546.5446\n",
            "Epoch [5042/100000], Validation Loss: 551.0831\n",
            "Epoch [5043/100000], Validation Loss: 545.7572\n",
            "Epoch [5044/100000], Validation Loss: 552.8391\n",
            "Epoch [5045/100000], Validation Loss: 546.1088\n",
            "Epoch [5046/100000], Validation Loss: 549.9889\n",
            "Epoch [5047/100000], Validation Loss: 544.8842\n",
            "Epoch [5048/100000], Validation Loss: 548.7343\n",
            "Epoch [5049/100000], Validation Loss: 563.6803\n",
            "Epoch [5050/100000], Validation Loss: 546.2254\n",
            "Epoch [5051/100000], Validation Loss: 560.6248\n",
            "Epoch [5052/100000], Validation Loss: 546.8010\n",
            "Epoch [5053/100000], Validation Loss: 554.0690\n",
            "Epoch [5054/100000], Validation Loss: 543.2710\n",
            "Epoch [5055/100000], Validation Loss: 552.1876\n",
            "Epoch [5056/100000], Validation Loss: 550.8414\n",
            "Epoch [5057/100000], Validation Loss: 551.9105\n",
            "Epoch [5058/100000], Validation Loss: 550.4631\n",
            "Epoch [5059/100000], Validation Loss: 546.4424\n",
            "Epoch [5060/100000], Validation Loss: 554.6991\n",
            "Epoch [5061/100000], Validation Loss: 547.9156\n",
            "Epoch [5062/100000], Validation Loss: 549.6449\n",
            "Epoch [5063/100000], Validation Loss: 545.1136\n",
            "Epoch [5064/100000], Validation Loss: 548.1077\n",
            "Epoch [5065/100000], Validation Loss: 547.5017\n",
            "Epoch [5066/100000], Validation Loss: 549.5923\n",
            "Epoch [5067/100000], Validation Loss: 552.4019\n",
            "Epoch [5068/100000], Validation Loss: 551.3575\n",
            "Epoch [5069/100000], Validation Loss: 574.9279\n",
            "Epoch [5070/100000], Validation Loss: 548.0182\n",
            "Epoch [5071/100000], Validation Loss: 546.2240\n",
            "Epoch [5072/100000], Validation Loss: 548.8389\n",
            "Epoch [5073/100000], Validation Loss: 547.3801\n",
            "Epoch [5074/100000], Validation Loss: 546.1483\n",
            "Epoch [5075/100000], Validation Loss: 550.4402\n",
            "Epoch [5076/100000], Validation Loss: 552.0103\n",
            "Epoch [5077/100000], Validation Loss: 546.0734\n",
            "Epoch [5078/100000], Validation Loss: 549.2684\n",
            "Epoch [5079/100000], Validation Loss: 546.9534\n",
            "Epoch [5080/100000], Validation Loss: 542.3531\n",
            "Epoch [5081/100000], Validation Loss: 548.6946\n",
            "Epoch [5082/100000], Validation Loss: 545.3518\n",
            "Epoch [5083/100000], Validation Loss: 548.0915\n",
            "Epoch [5084/100000], Validation Loss: 552.3164\n",
            "Epoch [5085/100000], Validation Loss: 545.4204\n",
            "Epoch [5086/100000], Validation Loss: 545.3045\n",
            "Epoch [5087/100000], Validation Loss: 548.2628\n",
            "Epoch [5088/100000], Validation Loss: 546.1237\n",
            "Epoch [5089/100000], Validation Loss: 552.7396\n",
            "Epoch [5090/100000], Validation Loss: 548.7142\n",
            "Epoch [5091/100000], Validation Loss: 547.0132\n",
            "Epoch [5092/100000], Validation Loss: 544.6577\n",
            "Epoch [5093/100000], Validation Loss: 546.5550\n",
            "Epoch [5094/100000], Validation Loss: 549.1084\n",
            "Epoch [5095/100000], Validation Loss: 548.0041\n",
            "Epoch [5096/100000], Validation Loss: 554.5316\n",
            "Epoch [5097/100000], Validation Loss: 546.8997\n",
            "Epoch [5098/100000], Validation Loss: 548.1656\n",
            "Epoch [5099/100000], Validation Loss: 567.0973\n",
            "Epoch [5100/100000], Validation Loss: 545.1651\n",
            "Epoch [5101/100000], Validation Loss: 557.3718\n",
            "Epoch [5102/100000], Validation Loss: 544.5086\n",
            "Epoch [5103/100000], Validation Loss: 543.9104\n",
            "Epoch [5104/100000], Validation Loss: 553.0288\n",
            "Epoch [5105/100000], Validation Loss: 549.0848\n",
            "Epoch [5106/100000], Validation Loss: 553.0501\n",
            "Epoch [5107/100000], Validation Loss: 547.3674\n",
            "Epoch [5108/100000], Validation Loss: 545.4107\n",
            "Epoch [5109/100000], Validation Loss: 547.9664\n",
            "Epoch [5110/100000], Validation Loss: 552.7553\n",
            "Epoch [5111/100000], Validation Loss: 551.7464\n",
            "Epoch [5112/100000], Validation Loss: 543.1866\n",
            "Epoch [5113/100000], Validation Loss: 549.8968\n",
            "Epoch [5114/100000], Validation Loss: 542.7782\n",
            "Epoch [5115/100000], Validation Loss: 552.4586\n",
            "Epoch [5116/100000], Validation Loss: 548.5493\n",
            "Epoch [5117/100000], Validation Loss: 548.3116\n",
            "Epoch [5118/100000], Validation Loss: 545.2347\n",
            "Epoch [5119/100000], Validation Loss: 552.0026\n",
            "Epoch [5120/100000], Validation Loss: 553.5070\n",
            "Epoch [5121/100000], Validation Loss: 547.2391\n",
            "Epoch [5122/100000], Validation Loss: 543.0759\n",
            "Epoch [5123/100000], Validation Loss: 551.4816\n",
            "Epoch [5124/100000], Validation Loss: 547.5561\n",
            "Epoch [5125/100000], Validation Loss: 548.5688\n",
            "Epoch [5126/100000], Validation Loss: 551.2983\n",
            "Epoch [5127/100000], Validation Loss: 546.1375\n",
            "Epoch [5128/100000], Validation Loss: 545.9496\n",
            "Epoch [5129/100000], Validation Loss: 545.1822\n",
            "Epoch [5130/100000], Validation Loss: 548.4007\n",
            "Epoch [5131/100000], Validation Loss: 547.9789\n",
            "Epoch [5132/100000], Validation Loss: 551.2037\n",
            "Epoch [5133/100000], Validation Loss: 547.4175\n",
            "Epoch [5134/100000], Validation Loss: 547.9284\n",
            "Epoch [5135/100000], Validation Loss: 551.6036\n",
            "Epoch [5136/100000], Validation Loss: 548.9721\n",
            "Epoch [5137/100000], Validation Loss: 548.7981\n",
            "Epoch [5138/100000], Validation Loss: 547.9536\n",
            "Epoch [5139/100000], Validation Loss: 548.0904\n",
            "Epoch [5140/100000], Validation Loss: 545.2393\n",
            "Epoch [5141/100000], Validation Loss: 542.2716\n",
            "Epoch [5142/100000], Validation Loss: 545.2899\n",
            "Epoch [5143/100000], Validation Loss: 544.2091\n",
            "Epoch [5144/100000], Validation Loss: 547.1719\n",
            "Epoch [5145/100000], Validation Loss: 546.9781\n",
            "Epoch [5146/100000], Validation Loss: 545.3619\n",
            "Epoch [5147/100000], Validation Loss: 545.6489\n",
            "Epoch [5148/100000], Validation Loss: 559.0590\n",
            "Epoch [5149/100000], Validation Loss: 546.6051\n",
            "Epoch [5150/100000], Validation Loss: 547.2716\n",
            "Epoch [5151/100000], Validation Loss: 546.6708\n",
            "Epoch [5152/100000], Validation Loss: 549.6859\n",
            "Epoch [5153/100000], Validation Loss: 550.3480\n",
            "Epoch [5154/100000], Validation Loss: 548.4452\n",
            "Epoch [5155/100000], Validation Loss: 548.4776\n",
            "Epoch [5156/100000], Validation Loss: 544.3311\n",
            "Epoch [5157/100000], Validation Loss: 548.9450\n",
            "Epoch [5158/100000], Validation Loss: 551.8063\n",
            "Epoch [5159/100000], Validation Loss: 544.8930\n",
            "Epoch [5160/100000], Validation Loss: 545.0326\n",
            "Epoch [5161/100000], Validation Loss: 548.6893\n",
            "Epoch [5162/100000], Validation Loss: 540.6170\n",
            "Epoch [5163/100000], Validation Loss: 547.1865\n",
            "Epoch [5164/100000], Validation Loss: 547.7811\n",
            "Epoch [5165/100000], Validation Loss: 551.2482\n",
            "Epoch [5166/100000], Validation Loss: 548.0236\n",
            "Epoch [5167/100000], Validation Loss: 546.1467\n",
            "Epoch [5168/100000], Validation Loss: 542.9970\n",
            "Epoch [5169/100000], Validation Loss: 548.9079\n",
            "Epoch [5170/100000], Validation Loss: 556.3787\n",
            "Epoch [5171/100000], Validation Loss: 553.0334\n",
            "Epoch [5172/100000], Validation Loss: 552.1810\n",
            "Epoch [5173/100000], Validation Loss: 543.7215\n",
            "Epoch [5174/100000], Validation Loss: 545.5267\n",
            "Epoch [5175/100000], Validation Loss: 547.2717\n",
            "Epoch [5176/100000], Validation Loss: 546.8314\n",
            "Epoch [5177/100000], Validation Loss: 550.3677\n",
            "Epoch [5178/100000], Validation Loss: 547.1228\n",
            "Epoch [5179/100000], Validation Loss: 545.9506\n",
            "Epoch [5180/100000], Validation Loss: 546.6606\n",
            "Epoch [5181/100000], Validation Loss: 546.0216\n",
            "Epoch [5182/100000], Validation Loss: 546.2771\n",
            "Epoch [5183/100000], Validation Loss: 548.7893\n",
            "Epoch [5184/100000], Validation Loss: 549.5317\n",
            "Epoch [5185/100000], Validation Loss: 546.4685\n",
            "Epoch [5186/100000], Validation Loss: 548.3480\n",
            "Epoch [5187/100000], Validation Loss: 551.8385\n",
            "Epoch [5188/100000], Validation Loss: 544.7542\n",
            "Epoch [5189/100000], Validation Loss: 545.4651\n",
            "Epoch [5190/100000], Validation Loss: 556.1705\n",
            "Epoch [5191/100000], Validation Loss: 550.6599\n",
            "Epoch [5192/100000], Validation Loss: 547.8759\n",
            "Epoch [5193/100000], Validation Loss: 553.4458\n",
            "Epoch [5194/100000], Validation Loss: 550.1974\n",
            "Epoch [5195/100000], Validation Loss: 545.2466\n",
            "Epoch [5196/100000], Validation Loss: 547.4341\n",
            "Epoch [5197/100000], Validation Loss: 563.2085\n",
            "Epoch [5198/100000], Validation Loss: 547.1555\n",
            "Epoch [5199/100000], Validation Loss: 548.2093\n",
            "Epoch [5200/100000], Validation Loss: 550.5736\n",
            "Epoch [5201/100000], Validation Loss: 554.6957\n",
            "Epoch [5202/100000], Validation Loss: 547.2099\n",
            "Epoch [5203/100000], Validation Loss: 549.0151\n",
            "Epoch [5204/100000], Validation Loss: 546.3320\n",
            "Epoch [5205/100000], Validation Loss: 546.3056\n",
            "Epoch [5206/100000], Validation Loss: 548.3841\n",
            "Epoch [5207/100000], Validation Loss: 548.9172\n",
            "Epoch [5208/100000], Validation Loss: 547.4567\n",
            "Epoch [5209/100000], Validation Loss: 551.0835\n",
            "Epoch [5210/100000], Validation Loss: 552.3488\n",
            "Epoch [5211/100000], Validation Loss: 549.8730\n",
            "Epoch [5212/100000], Validation Loss: 552.1117\n",
            "Epoch [5213/100000], Validation Loss: 548.3957\n",
            "Epoch [5214/100000], Validation Loss: 554.2999\n",
            "Epoch [5215/100000], Validation Loss: 547.8733\n",
            "Epoch [5216/100000], Validation Loss: 547.9428\n",
            "Epoch [5217/100000], Validation Loss: 546.7912\n",
            "Epoch [5218/100000], Validation Loss: 544.1579\n",
            "Epoch [5219/100000], Validation Loss: 557.6343\n",
            "Epoch [5220/100000], Validation Loss: 550.1692\n",
            "Epoch [5221/100000], Validation Loss: 545.5724\n",
            "Epoch [5222/100000], Validation Loss: 548.9166\n",
            "Epoch [5223/100000], Validation Loss: 554.9828\n",
            "Epoch [5224/100000], Validation Loss: 548.9749\n",
            "Epoch [5225/100000], Validation Loss: 544.2690\n",
            "Epoch [5226/100000], Validation Loss: 549.5843\n",
            "Epoch [5227/100000], Validation Loss: 549.1300\n",
            "Epoch [5228/100000], Validation Loss: 547.1186\n",
            "Epoch [5229/100000], Validation Loss: 548.8097\n",
            "Epoch [5230/100000], Validation Loss: 547.6778\n",
            "Epoch [5231/100000], Validation Loss: 550.9628\n",
            "Epoch [5232/100000], Validation Loss: 545.4016\n",
            "Epoch [5233/100000], Validation Loss: 544.0871\n",
            "Epoch [5234/100000], Validation Loss: 546.6466\n",
            "Epoch [5235/100000], Validation Loss: 546.4223\n",
            "Epoch [5236/100000], Validation Loss: 549.0402\n",
            "Epoch [5237/100000], Validation Loss: 552.5087\n",
            "Epoch [5238/100000], Validation Loss: 552.0199\n",
            "Epoch [5239/100000], Validation Loss: 547.6176\n",
            "Epoch [5240/100000], Validation Loss: 553.8511\n",
            "Epoch [5241/100000], Validation Loss: 573.0930\n",
            "Epoch [5242/100000], Validation Loss: 547.8222\n",
            "Epoch [5243/100000], Validation Loss: 544.6698\n",
            "Epoch [5244/100000], Validation Loss: 548.6642\n",
            "Epoch [5245/100000], Validation Loss: 544.5322\n",
            "Epoch [5246/100000], Validation Loss: 551.3998\n",
            "Epoch [5247/100000], Validation Loss: 548.4108\n",
            "Epoch [5248/100000], Validation Loss: 543.8826\n",
            "Epoch [5249/100000], Validation Loss: 544.3778\n",
            "Epoch [5250/100000], Validation Loss: 555.0581\n",
            "Epoch [5251/100000], Validation Loss: 552.9074\n",
            "Epoch [5252/100000], Validation Loss: 544.6772\n",
            "Epoch [5253/100000], Validation Loss: 545.8343\n",
            "Epoch [5254/100000], Validation Loss: 547.2800\n",
            "Epoch [5255/100000], Validation Loss: 552.9071\n",
            "Epoch [5256/100000], Validation Loss: 557.8179\n",
            "Epoch [5257/100000], Validation Loss: 552.7280\n",
            "Epoch [5258/100000], Validation Loss: 547.1188\n",
            "Epoch [5259/100000], Validation Loss: 550.7686\n",
            "Epoch [5260/100000], Validation Loss: 549.6087\n",
            "Epoch [5261/100000], Validation Loss: 546.6606\n",
            "Epoch [5262/100000], Validation Loss: 550.8610\n",
            "Epoch [5263/100000], Validation Loss: 551.3590\n",
            "Epoch [5264/100000], Validation Loss: 555.2477\n",
            "Epoch [5265/100000], Validation Loss: 548.0804\n",
            "Epoch [5266/100000], Validation Loss: 545.3714\n",
            "Epoch [5267/100000], Validation Loss: 546.0731\n",
            "Epoch [5268/100000], Validation Loss: 551.2387\n",
            "Epoch [5269/100000], Validation Loss: 553.4944\n",
            "Epoch [5270/100000], Validation Loss: 547.2755\n",
            "Epoch [5271/100000], Validation Loss: 546.6509\n",
            "Epoch [5272/100000], Validation Loss: 547.9649\n",
            "Epoch [5273/100000], Validation Loss: 552.1245\n",
            "Epoch [5274/100000], Validation Loss: 544.9388\n",
            "Epoch [5275/100000], Validation Loss: 552.3193\n",
            "Epoch [5276/100000], Validation Loss: 548.2631\n",
            "Epoch [5277/100000], Validation Loss: 544.7684\n",
            "Epoch [5278/100000], Validation Loss: 546.2895\n",
            "Epoch [5279/100000], Validation Loss: 551.1482\n",
            "Epoch [5280/100000], Validation Loss: 544.9957\n",
            "Epoch [5281/100000], Validation Loss: 544.8916\n",
            "Epoch [5282/100000], Validation Loss: 545.9961\n",
            "Epoch [5283/100000], Validation Loss: 547.5066\n",
            "Epoch [5284/100000], Validation Loss: 553.2911\n",
            "Epoch [5285/100000], Validation Loss: 551.1985\n",
            "Epoch [5286/100000], Validation Loss: 546.7544\n",
            "Epoch [5287/100000], Validation Loss: 548.7105\n",
            "Epoch [5288/100000], Validation Loss: 551.7577\n",
            "Epoch [5289/100000], Validation Loss: 544.6585\n",
            "Epoch [5290/100000], Validation Loss: 547.3561\n",
            "Epoch [5291/100000], Validation Loss: 550.5552\n",
            "Epoch [5292/100000], Validation Loss: 548.3289\n",
            "Epoch [5293/100000], Validation Loss: 546.4848\n",
            "Epoch [5294/100000], Validation Loss: 544.9914\n",
            "Epoch [5295/100000], Validation Loss: 550.1182\n",
            "Epoch [5296/100000], Validation Loss: 553.9982\n",
            "Epoch [5297/100000], Validation Loss: 546.6331\n",
            "Epoch [5298/100000], Validation Loss: 543.4513\n",
            "Epoch [5299/100000], Validation Loss: 544.2994\n",
            "Epoch [5300/100000], Validation Loss: 550.9023\n",
            "Epoch [5301/100000], Validation Loss: 556.6478\n",
            "Epoch [5302/100000], Validation Loss: 544.0018\n",
            "Epoch [5303/100000], Validation Loss: 544.5637\n",
            "Epoch [5304/100000], Validation Loss: 552.8898\n",
            "Epoch [5305/100000], Validation Loss: 546.5072\n",
            "Epoch [5306/100000], Validation Loss: 543.5922\n",
            "Epoch [5307/100000], Validation Loss: 544.1857\n",
            "Epoch [5308/100000], Validation Loss: 546.1716\n",
            "Epoch [5309/100000], Validation Loss: 549.8209\n",
            "Epoch [5310/100000], Validation Loss: 546.1067\n",
            "Epoch [5311/100000], Validation Loss: 548.0733\n",
            "Epoch [5312/100000], Validation Loss: 554.2305\n",
            "Epoch [5313/100000], Validation Loss: 552.4965\n",
            "Epoch [5314/100000], Validation Loss: 542.9372\n",
            "Epoch [5315/100000], Validation Loss: 541.4550\n",
            "Epoch [5316/100000], Validation Loss: 543.2630\n",
            "Epoch [5317/100000], Validation Loss: 544.4963\n",
            "Epoch [5318/100000], Validation Loss: 549.2394\n",
            "Epoch [5319/100000], Validation Loss: 551.9016\n",
            "Epoch [5320/100000], Validation Loss: 546.0198\n",
            "Epoch [5321/100000], Validation Loss: 551.1026\n",
            "Epoch [5322/100000], Validation Loss: 544.8639\n",
            "Epoch [5323/100000], Validation Loss: 541.6428\n",
            "Epoch [5324/100000], Validation Loss: 548.1817\n",
            "Epoch [5325/100000], Validation Loss: 549.6469\n",
            "Epoch [5326/100000], Validation Loss: 548.8302\n",
            "Epoch [5327/100000], Validation Loss: 549.7083\n",
            "Epoch [5328/100000], Validation Loss: 552.2920\n",
            "Epoch [5329/100000], Validation Loss: 554.7576\n",
            "Epoch [5330/100000], Validation Loss: 547.3986\n",
            "Epoch [5331/100000], Validation Loss: 547.9627\n",
            "Epoch [5332/100000], Validation Loss: 557.3049\n",
            "Epoch [5333/100000], Validation Loss: 548.5993\n",
            "Epoch [5334/100000], Validation Loss: 545.4571\n",
            "Epoch [5335/100000], Validation Loss: 583.5975\n",
            "Epoch [5336/100000], Validation Loss: 550.9185\n",
            "Epoch [5337/100000], Validation Loss: 549.9980\n",
            "Epoch [5338/100000], Validation Loss: 548.7356\n",
            "Epoch [5339/100000], Validation Loss: 548.7506\n",
            "Epoch [5340/100000], Validation Loss: 547.0218\n",
            "Epoch [5341/100000], Validation Loss: 548.0362\n",
            "Epoch [5342/100000], Validation Loss: 552.5345\n",
            "Epoch [5343/100000], Validation Loss: 560.5476\n",
            "Epoch [5344/100000], Validation Loss: 546.9068\n",
            "Epoch [5345/100000], Validation Loss: 551.7138\n",
            "Epoch [5346/100000], Validation Loss: 548.2404\n",
            "Epoch [5347/100000], Validation Loss: 547.2790\n",
            "Epoch [5348/100000], Validation Loss: 546.7096\n",
            "Epoch [5349/100000], Validation Loss: 547.4349\n",
            "Epoch [5350/100000], Validation Loss: 554.1614\n",
            "Epoch [5351/100000], Validation Loss: 544.7879\n",
            "Epoch [5352/100000], Validation Loss: 548.9342\n",
            "Epoch [5353/100000], Validation Loss: 550.4162\n",
            "Epoch [5354/100000], Validation Loss: 548.2945\n",
            "Epoch [5355/100000], Validation Loss: 554.0006\n",
            "Epoch [5356/100000], Validation Loss: 546.1509\n",
            "Epoch [5357/100000], Validation Loss: 556.9698\n",
            "Epoch [5358/100000], Validation Loss: 545.4475\n",
            "Epoch [5359/100000], Validation Loss: 555.2672\n",
            "Epoch [5360/100000], Validation Loss: 546.6679\n",
            "Epoch [5361/100000], Validation Loss: 544.7842\n",
            "Epoch [5362/100000], Validation Loss: 552.9042\n",
            "Epoch [5363/100000], Validation Loss: 547.8819\n",
            "Epoch [5364/100000], Validation Loss: 551.9445\n",
            "Epoch [5365/100000], Validation Loss: 553.0219\n",
            "Epoch [5366/100000], Validation Loss: 545.7458\n",
            "Epoch [5367/100000], Validation Loss: 553.1275\n",
            "Epoch [5368/100000], Validation Loss: 543.3629\n",
            "Epoch [5369/100000], Validation Loss: 546.7784\n",
            "Epoch [5370/100000], Validation Loss: 576.5982\n",
            "Epoch [5371/100000], Validation Loss: 549.0482\n",
            "Epoch [5372/100000], Validation Loss: 549.3183\n",
            "Epoch [5373/100000], Validation Loss: 547.4194\n",
            "Epoch [5374/100000], Validation Loss: 547.2168\n",
            "Epoch [5375/100000], Validation Loss: 547.8001\n",
            "Epoch [5376/100000], Validation Loss: 550.9596\n",
            "Epoch [5377/100000], Validation Loss: 547.9541\n",
            "Epoch [5378/100000], Validation Loss: 544.5790\n",
            "Epoch [5379/100000], Validation Loss: 551.9301\n",
            "Epoch [5380/100000], Validation Loss: 553.8259\n",
            "Epoch [5381/100000], Validation Loss: 554.8329\n",
            "Epoch [5382/100000], Validation Loss: 550.7273\n",
            "Epoch [5383/100000], Validation Loss: 547.1627\n",
            "Epoch [5384/100000], Validation Loss: 545.9663\n",
            "Epoch [5385/100000], Validation Loss: 547.0217\n",
            "Epoch [5386/100000], Validation Loss: 549.6031\n",
            "Epoch [5387/100000], Validation Loss: 549.8041\n",
            "Epoch [5388/100000], Validation Loss: 548.9875\n",
            "Epoch [5389/100000], Validation Loss: 548.6899\n",
            "Epoch [5390/100000], Validation Loss: 551.1630\n",
            "Epoch [5391/100000], Validation Loss: 546.7310\n",
            "Epoch [5392/100000], Validation Loss: 545.8388\n",
            "Epoch [5393/100000], Validation Loss: 553.4729\n",
            "Epoch [5394/100000], Validation Loss: 550.5917\n",
            "Epoch [5395/100000], Validation Loss: 551.3997\n",
            "Epoch [5396/100000], Validation Loss: 545.0256\n",
            "Epoch [5397/100000], Validation Loss: 545.1973\n",
            "Epoch [5398/100000], Validation Loss: 547.3237\n",
            "Epoch [5399/100000], Validation Loss: 548.7781\n",
            "Epoch [5400/100000], Validation Loss: 549.0179\n",
            "Epoch [5401/100000], Validation Loss: 546.8231\n",
            "Epoch [5402/100000], Validation Loss: 545.2786\n",
            "Epoch [5403/100000], Validation Loss: 547.8229\n",
            "Epoch [5404/100000], Validation Loss: 549.0682\n",
            "Epoch [5405/100000], Validation Loss: 551.1826\n",
            "Epoch [5406/100000], Validation Loss: 544.3765\n",
            "Epoch [5407/100000], Validation Loss: 546.1273\n",
            "Epoch [5408/100000], Validation Loss: 549.5695\n",
            "Epoch [5409/100000], Validation Loss: 548.9566\n",
            "Epoch [5410/100000], Validation Loss: 546.9364\n",
            "Epoch [5411/100000], Validation Loss: 555.2269\n",
            "Epoch [5412/100000], Validation Loss: 549.9544\n",
            "Epoch [5413/100000], Validation Loss: 548.8131\n",
            "Epoch [5414/100000], Validation Loss: 552.8689\n",
            "Epoch [5415/100000], Validation Loss: 548.8353\n",
            "Epoch [5416/100000], Validation Loss: 546.4181\n",
            "Epoch [5417/100000], Validation Loss: 547.0081\n",
            "Epoch [5418/100000], Validation Loss: 547.4505\n",
            "Epoch [5419/100000], Validation Loss: 550.1742\n",
            "Epoch [5420/100000], Validation Loss: 551.5496\n",
            "Epoch [5421/100000], Validation Loss: 548.4188\n",
            "Epoch [5422/100000], Validation Loss: 565.1582\n",
            "Epoch [5423/100000], Validation Loss: 545.3828\n",
            "Epoch [5424/100000], Validation Loss: 547.8802\n",
            "Epoch [5425/100000], Validation Loss: 547.3040\n",
            "Epoch [5426/100000], Validation Loss: 548.3378\n",
            "Epoch [5427/100000], Validation Loss: 546.2618\n",
            "Epoch [5428/100000], Validation Loss: 545.3325\n",
            "Epoch [5429/100000], Validation Loss: 546.8938\n",
            "Epoch [5430/100000], Validation Loss: 547.6520\n",
            "Epoch [5431/100000], Validation Loss: 547.3868\n",
            "Epoch [5432/100000], Validation Loss: 560.7655\n",
            "Epoch [5433/100000], Validation Loss: 567.2201\n",
            "Epoch [5434/100000], Validation Loss: 549.4852\n",
            "Epoch [5435/100000], Validation Loss: 546.5927\n",
            "Epoch [5436/100000], Validation Loss: 547.9004\n",
            "Epoch [5437/100000], Validation Loss: 550.0486\n",
            "Epoch [5438/100000], Validation Loss: 546.0887\n",
            "Epoch [5439/100000], Validation Loss: 551.7932\n",
            "Epoch [5440/100000], Validation Loss: 551.7046\n",
            "Epoch [5441/100000], Validation Loss: 546.9532\n",
            "Epoch [5442/100000], Validation Loss: 545.6521\n",
            "Epoch [5443/100000], Validation Loss: 542.5259\n",
            "Epoch [5444/100000], Validation Loss: 546.1782\n",
            "Epoch [5445/100000], Validation Loss: 550.2001\n",
            "Epoch [5446/100000], Validation Loss: 549.3378\n",
            "Epoch [5447/100000], Validation Loss: 549.9087\n",
            "Epoch [5448/100000], Validation Loss: 543.6011\n",
            "Epoch [5449/100000], Validation Loss: 547.4486\n",
            "Epoch [5450/100000], Validation Loss: 551.2461\n",
            "Epoch [5451/100000], Validation Loss: 550.0414\n",
            "Epoch [5452/100000], Validation Loss: 551.0847\n",
            "Epoch [5453/100000], Validation Loss: 552.5018\n",
            "Epoch [5454/100000], Validation Loss: 546.0896\n",
            "Epoch [5455/100000], Validation Loss: 549.0767\n",
            "Epoch [5456/100000], Validation Loss: 545.8719\n",
            "Epoch [5457/100000], Validation Loss: 546.7130\n",
            "Epoch [5458/100000], Validation Loss: 552.2518\n",
            "Epoch [5459/100000], Validation Loss: 548.6252\n",
            "Epoch [5460/100000], Validation Loss: 548.2823\n",
            "Epoch [5461/100000], Validation Loss: 554.3966\n",
            "Epoch [5462/100000], Validation Loss: 546.1013\n",
            "Epoch [5463/100000], Validation Loss: 547.6905\n",
            "Epoch [5464/100000], Validation Loss: 546.1071\n",
            "Epoch [5465/100000], Validation Loss: 545.1419\n",
            "Epoch [5466/100000], Validation Loss: 548.0747\n",
            "Epoch [5467/100000], Validation Loss: 568.4227\n",
            "Epoch [5468/100000], Validation Loss: 546.5071\n",
            "Epoch [5469/100000], Validation Loss: 551.6406\n",
            "Epoch [5470/100000], Validation Loss: 551.1738\n",
            "Epoch [5471/100000], Validation Loss: 543.6916\n",
            "Epoch [5472/100000], Validation Loss: 549.9675\n",
            "Epoch [5473/100000], Validation Loss: 546.6689\n",
            "Epoch [5474/100000], Validation Loss: 543.9886\n",
            "Epoch [5475/100000], Validation Loss: 548.7682\n",
            "Epoch [5476/100000], Validation Loss: 550.9785\n",
            "Epoch [5477/100000], Validation Loss: 549.5928\n",
            "Epoch [5478/100000], Validation Loss: 544.1483\n",
            "Epoch [5479/100000], Validation Loss: 547.3874\n",
            "Epoch [5480/100000], Validation Loss: 549.0754\n",
            "Epoch [5481/100000], Validation Loss: 544.3770\n",
            "Epoch [5482/100000], Validation Loss: 550.4353\n",
            "Epoch [5483/100000], Validation Loss: 549.6903\n",
            "Epoch [5484/100000], Validation Loss: 547.9299\n",
            "Epoch [5485/100000], Validation Loss: 551.8077\n",
            "Epoch [5486/100000], Validation Loss: 549.3134\n",
            "Epoch [5487/100000], Validation Loss: 552.5258\n",
            "Epoch [5488/100000], Validation Loss: 548.2825\n",
            "Epoch [5489/100000], Validation Loss: 551.4336\n",
            "Epoch [5490/100000], Validation Loss: 545.9783\n",
            "Epoch [5491/100000], Validation Loss: 544.2899\n",
            "Epoch [5492/100000], Validation Loss: 549.2922\n",
            "Epoch [5493/100000], Validation Loss: 553.1538\n",
            "Epoch [5494/100000], Validation Loss: 545.7241\n",
            "Epoch [5495/100000], Validation Loss: 543.9813\n",
            "Epoch [5496/100000], Validation Loss: 554.5876\n",
            "Epoch [5497/100000], Validation Loss: 550.5403\n",
            "Epoch [5498/100000], Validation Loss: 550.6794\n",
            "Epoch [5499/100000], Validation Loss: 555.7539\n",
            "Epoch [5500/100000], Validation Loss: 549.0559\n",
            "Epoch [5501/100000], Validation Loss: 547.6347\n",
            "Epoch [5502/100000], Validation Loss: 546.3656\n",
            "Epoch [5503/100000], Validation Loss: 546.7886\n",
            "Epoch [5504/100000], Validation Loss: 552.1714\n",
            "Epoch [5505/100000], Validation Loss: 545.4656\n",
            "Epoch [5506/100000], Validation Loss: 547.3298\n",
            "Epoch [5507/100000], Validation Loss: 548.1725\n",
            "Epoch [5508/100000], Validation Loss: 546.2882\n",
            "Epoch [5509/100000], Validation Loss: 552.9477\n",
            "Epoch [5510/100000], Validation Loss: 550.0057\n",
            "Epoch [5511/100000], Validation Loss: 545.3618\n",
            "Epoch [5512/100000], Validation Loss: 548.7961\n",
            "Epoch [5513/100000], Validation Loss: 555.5349\n",
            "Epoch [5514/100000], Validation Loss: 545.6747\n",
            "Epoch [5515/100000], Validation Loss: 553.4525\n",
            "Epoch [5516/100000], Validation Loss: 547.5045\n",
            "Epoch [5517/100000], Validation Loss: 547.5910\n",
            "Epoch [5518/100000], Validation Loss: 542.3585\n",
            "Epoch [5519/100000], Validation Loss: 546.6541\n",
            "Epoch [5520/100000], Validation Loss: 557.1950\n",
            "Epoch [5521/100000], Validation Loss: 548.9354\n",
            "Epoch [5522/100000], Validation Loss: 547.7255\n",
            "Epoch [5523/100000], Validation Loss: 548.2952\n",
            "Epoch [5524/100000], Validation Loss: 554.5746\n",
            "Epoch [5525/100000], Validation Loss: 551.8125\n",
            "Epoch [5526/100000], Validation Loss: 548.9557\n",
            "Epoch [5527/100000], Validation Loss: 549.5481\n",
            "Epoch [5528/100000], Validation Loss: 549.5063\n",
            "Epoch [5529/100000], Validation Loss: 552.9536\n",
            "Epoch [5530/100000], Validation Loss: 548.0296\n",
            "Epoch [5531/100000], Validation Loss: 546.6209\n",
            "Epoch [5532/100000], Validation Loss: 550.4637\n",
            "Epoch [5533/100000], Validation Loss: 548.0119\n",
            "Epoch [5534/100000], Validation Loss: 554.6921\n",
            "Epoch [5535/100000], Validation Loss: 546.1724\n",
            "Epoch [5536/100000], Validation Loss: 555.8250\n",
            "Epoch [5537/100000], Validation Loss: 546.1364\n",
            "Epoch [5538/100000], Validation Loss: 555.7966\n",
            "Epoch [5539/100000], Validation Loss: 546.5043\n",
            "Epoch [5540/100000], Validation Loss: 544.4986\n",
            "Epoch [5541/100000], Validation Loss: 543.3960\n",
            "Epoch [5542/100000], Validation Loss: 542.4609\n",
            "Epoch [5543/100000], Validation Loss: 548.3377\n",
            "Epoch [5544/100000], Validation Loss: 550.4515\n",
            "Epoch [5545/100000], Validation Loss: 543.4390\n",
            "Epoch [5546/100000], Validation Loss: 548.4513\n",
            "Epoch [5547/100000], Validation Loss: 550.8765\n",
            "Epoch [5548/100000], Validation Loss: 549.9980\n",
            "Epoch [5549/100000], Validation Loss: 545.0446\n",
            "Epoch [5550/100000], Validation Loss: 547.4650\n",
            "Epoch [5551/100000], Validation Loss: 555.2287\n",
            "Epoch [5552/100000], Validation Loss: 548.2156\n",
            "Epoch [5553/100000], Validation Loss: 548.3552\n",
            "Epoch [5554/100000], Validation Loss: 544.7561\n",
            "Epoch [5555/100000], Validation Loss: 546.2893\n",
            "Epoch [5556/100000], Validation Loss: 549.3952\n",
            "Epoch [5557/100000], Validation Loss: 545.3710\n",
            "Epoch [5558/100000], Validation Loss: 554.9482\n",
            "Epoch [5559/100000], Validation Loss: 548.1119\n",
            "Epoch [5560/100000], Validation Loss: 553.9036\n",
            "Epoch [5561/100000], Validation Loss: 548.2803\n",
            "Epoch [5562/100000], Validation Loss: 548.8125\n",
            "Epoch [5563/100000], Validation Loss: 544.8174\n",
            "Epoch [5564/100000], Validation Loss: 550.3157\n",
            "Epoch [5565/100000], Validation Loss: 549.2415\n",
            "Epoch [5566/100000], Validation Loss: 553.5766\n",
            "Epoch [5567/100000], Validation Loss: 549.0905\n",
            "Epoch [5568/100000], Validation Loss: 553.2525\n",
            "Epoch [5569/100000], Validation Loss: 546.2227\n",
            "Epoch [5570/100000], Validation Loss: 551.5751\n",
            "Epoch [5571/100000], Validation Loss: 553.5280\n",
            "Epoch [5572/100000], Validation Loss: 546.4849\n",
            "Epoch [5573/100000], Validation Loss: 541.5986\n",
            "Epoch [5574/100000], Validation Loss: 548.5209\n",
            "Epoch [5575/100000], Validation Loss: 546.9815\n",
            "Epoch [5576/100000], Validation Loss: 545.6675\n",
            "Epoch [5577/100000], Validation Loss: 545.6659\n",
            "Epoch [5578/100000], Validation Loss: 549.4215\n",
            "Epoch [5579/100000], Validation Loss: 545.2324\n",
            "Epoch [5580/100000], Validation Loss: 544.3611\n",
            "Epoch [5581/100000], Validation Loss: 551.8353\n",
            "Epoch [5582/100000], Validation Loss: 548.4605\n",
            "Epoch [5583/100000], Validation Loss: 544.1036\n",
            "Epoch [5584/100000], Validation Loss: 545.1414\n",
            "Epoch [5585/100000], Validation Loss: 548.5676\n",
            "Epoch [5586/100000], Validation Loss: 546.7141\n",
            "Epoch [5587/100000], Validation Loss: 547.0737\n",
            "Epoch [5588/100000], Validation Loss: 545.5833\n",
            "Epoch [5589/100000], Validation Loss: 552.0842\n",
            "Epoch [5590/100000], Validation Loss: 552.5506\n",
            "Epoch [5591/100000], Validation Loss: 552.6313\n",
            "Epoch [5592/100000], Validation Loss: 550.6054\n",
            "Epoch [5593/100000], Validation Loss: 548.9205\n",
            "Epoch [5594/100000], Validation Loss: 546.6062\n",
            "Epoch [5595/100000], Validation Loss: 551.1196\n",
            "Epoch [5596/100000], Validation Loss: 546.1538\n",
            "Epoch [5597/100000], Validation Loss: 551.7433\n",
            "Epoch [5598/100000], Validation Loss: 557.8404\n",
            "Epoch [5599/100000], Validation Loss: 548.5099\n",
            "Epoch [5600/100000], Validation Loss: 547.9806\n",
            "Epoch [5601/100000], Validation Loss: 557.1818\n",
            "Epoch [5602/100000], Validation Loss: 549.2329\n",
            "Epoch [5603/100000], Validation Loss: 550.3846\n",
            "Epoch [5604/100000], Validation Loss: 545.5866\n",
            "Epoch [5605/100000], Validation Loss: 551.3701\n",
            "Epoch [5606/100000], Validation Loss: 548.1753\n",
            "Epoch [5607/100000], Validation Loss: 548.6956\n",
            "Epoch [5608/100000], Validation Loss: 540.8745\n",
            "Epoch [5609/100000], Validation Loss: 548.1930\n",
            "Epoch [5610/100000], Validation Loss: 546.9597\n",
            "Epoch [5611/100000], Validation Loss: 550.3957\n",
            "Epoch [5612/100000], Validation Loss: 546.9609\n",
            "Epoch [5613/100000], Validation Loss: 545.3619\n",
            "Epoch [5614/100000], Validation Loss: 545.3721\n",
            "Epoch [5615/100000], Validation Loss: 547.5840\n",
            "Epoch [5616/100000], Validation Loss: 548.2900\n",
            "Epoch [5617/100000], Validation Loss: 549.4790\n",
            "Epoch [5618/100000], Validation Loss: 549.5184\n",
            "Epoch [5619/100000], Validation Loss: 544.7941\n",
            "Epoch [5620/100000], Validation Loss: 545.6633\n",
            "Epoch [5621/100000], Validation Loss: 549.7284\n",
            "Epoch [5622/100000], Validation Loss: 553.9392\n",
            "Epoch [5623/100000], Validation Loss: 546.1408\n",
            "Epoch [5624/100000], Validation Loss: 544.3631\n",
            "Epoch [5625/100000], Validation Loss: 552.1384\n",
            "Epoch [5626/100000], Validation Loss: 543.8087\n",
            "Epoch [5627/100000], Validation Loss: 549.7538\n",
            "Epoch [5628/100000], Validation Loss: 550.7918\n",
            "Epoch [5629/100000], Validation Loss: 548.3678\n",
            "Epoch [5630/100000], Validation Loss: 551.7076\n",
            "Epoch [5631/100000], Validation Loss: 546.1135\n",
            "Epoch [5632/100000], Validation Loss: 551.4660\n",
            "Epoch [5633/100000], Validation Loss: 546.9205\n",
            "Epoch [5634/100000], Validation Loss: 552.5591\n",
            "Epoch [5635/100000], Validation Loss: 546.2261\n",
            "Epoch [5636/100000], Validation Loss: 547.4341\n",
            "Epoch [5637/100000], Validation Loss: 550.5818\n",
            "Epoch [5638/100000], Validation Loss: 543.9987\n",
            "Epoch [5639/100000], Validation Loss: 548.2295\n",
            "Epoch [5640/100000], Validation Loss: 548.9215\n",
            "Epoch [5641/100000], Validation Loss: 547.1885\n",
            "Epoch [5642/100000], Validation Loss: 547.6899\n",
            "Epoch [5643/100000], Validation Loss: 547.7573\n",
            "Epoch [5644/100000], Validation Loss: 547.9507\n",
            "Epoch [5645/100000], Validation Loss: 549.1671\n",
            "Epoch [5646/100000], Validation Loss: 549.4551\n",
            "Epoch [5647/100000], Validation Loss: 549.2275\n",
            "Epoch [5648/100000], Validation Loss: 555.4081\n",
            "Epoch [5649/100000], Validation Loss: 551.0991\n",
            "Epoch [5650/100000], Validation Loss: 552.5680\n",
            "Epoch [5651/100000], Validation Loss: 546.3479\n",
            "Epoch [5652/100000], Validation Loss: 549.1103\n",
            "Epoch [5653/100000], Validation Loss: 546.0430\n",
            "Epoch [5654/100000], Validation Loss: 552.8027\n",
            "Epoch [5655/100000], Validation Loss: 544.4880\n",
            "Epoch [5656/100000], Validation Loss: 550.3750\n",
            "Epoch [5657/100000], Validation Loss: 551.9569\n",
            "Epoch [5658/100000], Validation Loss: 547.3300\n",
            "Epoch [5659/100000], Validation Loss: 545.5363\n",
            "Epoch [5660/100000], Validation Loss: 550.2831\n",
            "Epoch [5661/100000], Validation Loss: 556.7073\n",
            "Epoch [5662/100000], Validation Loss: 552.0335\n",
            "Epoch [5663/100000], Validation Loss: 545.7566\n",
            "Epoch [5664/100000], Validation Loss: 555.8762\n",
            "Epoch [5665/100000], Validation Loss: 552.2379\n",
            "Epoch [5666/100000], Validation Loss: 547.3797\n",
            "Epoch [5667/100000], Validation Loss: 547.1244\n",
            "Epoch [5668/100000], Validation Loss: 595.6133\n",
            "Epoch [5669/100000], Validation Loss: 551.8581\n",
            "Epoch [5670/100000], Validation Loss: 545.7145\n",
            "Epoch [5671/100000], Validation Loss: 552.3244\n",
            "Epoch [5672/100000], Validation Loss: 548.0931\n",
            "Epoch [5673/100000], Validation Loss: 549.5465\n",
            "Epoch [5674/100000], Validation Loss: 549.7212\n",
            "Epoch [5675/100000], Validation Loss: 552.8975\n",
            "Epoch [5676/100000], Validation Loss: 551.0224\n",
            "Epoch [5677/100000], Validation Loss: 553.7394\n",
            "Epoch [5678/100000], Validation Loss: 598.1098\n",
            "Epoch [5679/100000], Validation Loss: 544.2166\n",
            "Epoch [5680/100000], Validation Loss: 549.1925\n",
            "Epoch [5681/100000], Validation Loss: 548.4328\n",
            "Epoch [5682/100000], Validation Loss: 548.0053\n",
            "Epoch [5683/100000], Validation Loss: 549.3840\n",
            "Epoch [5684/100000], Validation Loss: 550.9773\n",
            "Epoch [5685/100000], Validation Loss: 552.6768\n",
            "Epoch [5686/100000], Validation Loss: 544.4002\n",
            "Epoch [5687/100000], Validation Loss: 545.9569\n",
            "Epoch [5688/100000], Validation Loss: 546.0166\n",
            "Epoch [5689/100000], Validation Loss: 551.1341\n",
            "Epoch [5690/100000], Validation Loss: 545.1921\n",
            "Epoch [5691/100000], Validation Loss: 546.0534\n",
            "Epoch [5692/100000], Validation Loss: 563.9993\n",
            "Epoch [5693/100000], Validation Loss: 547.4926\n",
            "Epoch [5694/100000], Validation Loss: 549.0346\n",
            "Epoch [5695/100000], Validation Loss: 545.1240\n",
            "Epoch [5696/100000], Validation Loss: 545.0743\n",
            "Epoch [5697/100000], Validation Loss: 546.6782\n",
            "Epoch [5698/100000], Validation Loss: 548.0902\n",
            "Epoch [5699/100000], Validation Loss: 547.3698\n",
            "Epoch [5700/100000], Validation Loss: 550.1698\n",
            "Epoch [5701/100000], Validation Loss: 544.5266\n",
            "Epoch [5702/100000], Validation Loss: 556.7399\n",
            "Epoch [5703/100000], Validation Loss: 549.7786\n",
            "Epoch [5704/100000], Validation Loss: 546.2202\n",
            "Epoch [5705/100000], Validation Loss: 548.3188\n",
            "Epoch [5706/100000], Validation Loss: 544.6850\n",
            "Epoch [5707/100000], Validation Loss: 546.3585\n",
            "Epoch [5708/100000], Validation Loss: 547.2997\n",
            "Epoch [5709/100000], Validation Loss: 550.6729\n",
            "Epoch [5710/100000], Validation Loss: 549.5152\n",
            "Epoch [5711/100000], Validation Loss: 546.9774\n",
            "Epoch [5712/100000], Validation Loss: 547.5214\n",
            "Epoch [5713/100000], Validation Loss: 544.5448\n",
            "Epoch [5714/100000], Validation Loss: 549.9010\n",
            "Epoch [5715/100000], Validation Loss: 547.4817\n",
            "Epoch [5716/100000], Validation Loss: 548.4490\n",
            "Epoch [5717/100000], Validation Loss: 551.8100\n",
            "Epoch [5718/100000], Validation Loss: 548.2639\n",
            "Epoch [5719/100000], Validation Loss: 555.6663\n",
            "Epoch [5720/100000], Validation Loss: 558.3961\n",
            "Epoch [5721/100000], Validation Loss: 546.4593\n",
            "Epoch [5722/100000], Validation Loss: 553.1769\n",
            "Epoch [5723/100000], Validation Loss: 550.4253\n",
            "Epoch [5724/100000], Validation Loss: 547.3196\n",
            "Epoch [5725/100000], Validation Loss: 545.8475\n",
            "Epoch [5726/100000], Validation Loss: 549.2528\n",
            "Epoch [5727/100000], Validation Loss: 544.4953\n",
            "Epoch [5728/100000], Validation Loss: 549.0605\n",
            "Epoch [5729/100000], Validation Loss: 549.7634\n",
            "Epoch [5730/100000], Validation Loss: 549.6990\n",
            "Epoch [5731/100000], Validation Loss: 547.5826\n",
            "Epoch [5732/100000], Validation Loss: 546.4154\n",
            "Epoch [5733/100000], Validation Loss: 552.3471\n",
            "Epoch [5734/100000], Validation Loss: 548.8792\n",
            "Epoch [5735/100000], Validation Loss: 543.8048\n",
            "Epoch [5736/100000], Validation Loss: 548.2879\n",
            "Epoch [5737/100000], Validation Loss: 547.3491\n",
            "Epoch [5738/100000], Validation Loss: 547.4835\n",
            "Epoch [5739/100000], Validation Loss: 550.9314\n",
            "Epoch [5740/100000], Validation Loss: 549.0329\n",
            "Epoch [5741/100000], Validation Loss: 544.8049\n",
            "Epoch [5742/100000], Validation Loss: 547.9664\n",
            "Epoch [5743/100000], Validation Loss: 545.4528\n",
            "Epoch [5744/100000], Validation Loss: 557.4968\n",
            "Epoch [5745/100000], Validation Loss: 542.0317\n",
            "Epoch [5746/100000], Validation Loss: 549.2001\n",
            "Epoch [5747/100000], Validation Loss: 548.1509\n",
            "Epoch [5748/100000], Validation Loss: 545.6016\n",
            "Epoch [5749/100000], Validation Loss: 544.9383\n",
            "Epoch [5750/100000], Validation Loss: 546.5182\n",
            "Epoch [5751/100000], Validation Loss: 548.2848\n",
            "Epoch [5752/100000], Validation Loss: 548.4189\n",
            "Epoch [5753/100000], Validation Loss: 549.3672\n",
            "Epoch [5754/100000], Validation Loss: 547.6380\n",
            "Epoch [5755/100000], Validation Loss: 553.7272\n",
            "Epoch [5756/100000], Validation Loss: 545.8303\n",
            "Epoch [5757/100000], Validation Loss: 554.1818\n",
            "Epoch [5758/100000], Validation Loss: 541.2696\n",
            "Epoch [5759/100000], Validation Loss: 547.2099\n",
            "Epoch [5760/100000], Validation Loss: 547.8381\n",
            "Epoch [5761/100000], Validation Loss: 550.2753\n",
            "Epoch [5762/100000], Validation Loss: 549.5124\n",
            "Epoch [5763/100000], Validation Loss: 552.6508\n",
            "Epoch [5764/100000], Validation Loss: 547.9398\n",
            "Epoch [5765/100000], Validation Loss: 549.5730\n",
            "Epoch [5766/100000], Validation Loss: 547.8794\n",
            "Epoch [5767/100000], Validation Loss: 548.5337\n",
            "Epoch [5768/100000], Validation Loss: 545.6913\n",
            "Epoch [5769/100000], Validation Loss: 547.4748\n",
            "Epoch [5770/100000], Validation Loss: 545.3145\n",
            "Epoch [5771/100000], Validation Loss: 554.1051\n",
            "Epoch [5772/100000], Validation Loss: 544.7926\n",
            "Epoch [5773/100000], Validation Loss: 546.0934\n",
            "Epoch [5774/100000], Validation Loss: 546.6626\n",
            "Epoch [5775/100000], Validation Loss: 548.1090\n",
            "Epoch [5776/100000], Validation Loss: 547.0766\n",
            "Epoch [5777/100000], Validation Loss: 551.6758\n",
            "Epoch [5778/100000], Validation Loss: 551.8239\n",
            "Epoch [5779/100000], Validation Loss: 546.3061\n",
            "Epoch [5780/100000], Validation Loss: 546.2833\n",
            "Epoch [5781/100000], Validation Loss: 563.3222\n",
            "Epoch [5782/100000], Validation Loss: 548.3159\n",
            "Epoch [5783/100000], Validation Loss: 551.0763\n",
            "Epoch [5784/100000], Validation Loss: 548.9950\n",
            "Epoch [5785/100000], Validation Loss: 547.1470\n",
            "Epoch [5786/100000], Validation Loss: 548.9645\n",
            "Epoch [5787/100000], Validation Loss: 548.1155\n",
            "Epoch [5788/100000], Validation Loss: 556.2281\n",
            "Epoch [5789/100000], Validation Loss: 543.3775\n",
            "Epoch [5790/100000], Validation Loss: 547.4279\n",
            "Epoch [5791/100000], Validation Loss: 557.1326\n",
            "Epoch [5792/100000], Validation Loss: 551.0296\n",
            "Epoch [5793/100000], Validation Loss: 553.4015\n",
            "Epoch [5794/100000], Validation Loss: 552.2263\n",
            "Epoch [5795/100000], Validation Loss: 554.6996\n",
            "Epoch [5796/100000], Validation Loss: 548.7415\n",
            "Epoch [5797/100000], Validation Loss: 549.6759\n",
            "Epoch [5798/100000], Validation Loss: 553.8080\n",
            "Epoch [5799/100000], Validation Loss: 545.0889\n",
            "Epoch [5800/100000], Validation Loss: 550.0246\n",
            "Epoch [5801/100000], Validation Loss: 548.2932\n",
            "Epoch [5802/100000], Validation Loss: 551.5845\n",
            "Epoch [5803/100000], Validation Loss: 546.9566\n",
            "Epoch [5804/100000], Validation Loss: 551.6225\n",
            "Epoch [5805/100000], Validation Loss: 553.4500\n",
            "Epoch [5806/100000], Validation Loss: 546.2024\n",
            "Epoch [5807/100000], Validation Loss: 549.3061\n",
            "Epoch [5808/100000], Validation Loss: 555.7190\n",
            "Epoch [5809/100000], Validation Loss: 558.8229\n",
            "Epoch [5810/100000], Validation Loss: 555.1505\n",
            "Epoch [5811/100000], Validation Loss: 548.5454\n",
            "Epoch [5812/100000], Validation Loss: 547.6852\n",
            "Epoch [5813/100000], Validation Loss: 546.3267\n",
            "Epoch [5814/100000], Validation Loss: 552.3715\n",
            "Epoch [5815/100000], Validation Loss: 558.5203\n",
            "Epoch [5816/100000], Validation Loss: 551.1692\n",
            "Epoch [5817/100000], Validation Loss: 550.6021\n",
            "Epoch [5818/100000], Validation Loss: 547.9192\n",
            "Epoch [5819/100000], Validation Loss: 549.6108\n",
            "Epoch [5820/100000], Validation Loss: 547.1345\n",
            "Epoch [5821/100000], Validation Loss: 547.9057\n",
            "Epoch [5822/100000], Validation Loss: 544.9114\n",
            "Epoch [5823/100000], Validation Loss: 545.7766\n",
            "Epoch [5824/100000], Validation Loss: 548.6152\n",
            "Epoch [5825/100000], Validation Loss: 547.7759\n",
            "Epoch [5826/100000], Validation Loss: 547.9922\n",
            "Epoch [5827/100000], Validation Loss: 561.7177\n",
            "Epoch [5828/100000], Validation Loss: 549.5167\n",
            "Epoch [5829/100000], Validation Loss: 547.9394\n",
            "Epoch [5830/100000], Validation Loss: 547.0947\n",
            "Epoch [5831/100000], Validation Loss: 544.4680\n",
            "Epoch [5832/100000], Validation Loss: 553.3747\n",
            "Epoch [5833/100000], Validation Loss: 549.5616\n",
            "Epoch [5834/100000], Validation Loss: 554.6532\n",
            "Epoch [5835/100000], Validation Loss: 550.7930\n",
            "Epoch [5836/100000], Validation Loss: 546.6885\n",
            "Epoch [5837/100000], Validation Loss: 547.3282\n",
            "Epoch [5838/100000], Validation Loss: 547.5517\n",
            "Epoch [5839/100000], Validation Loss: 547.9541\n",
            "Epoch [5840/100000], Validation Loss: 552.0741\n",
            "Epoch [5841/100000], Validation Loss: 550.8855\n",
            "Epoch [5842/100000], Validation Loss: 544.3350\n",
            "Epoch [5843/100000], Validation Loss: 555.8156\n",
            "Epoch [5844/100000], Validation Loss: 548.7127\n",
            "Epoch [5845/100000], Validation Loss: 554.3481\n",
            "Epoch [5846/100000], Validation Loss: 551.2270\n",
            "Epoch [5847/100000], Validation Loss: 548.8717\n",
            "Epoch [5848/100000], Validation Loss: 546.2543\n",
            "Epoch [5849/100000], Validation Loss: 550.8068\n",
            "Epoch [5850/100000], Validation Loss: 551.7269\n",
            "Epoch [5851/100000], Validation Loss: 550.3807\n",
            "Epoch [5852/100000], Validation Loss: 544.0718\n",
            "Epoch [5853/100000], Validation Loss: 544.6220\n",
            "Epoch [5854/100000], Validation Loss: 551.8291\n",
            "Epoch [5855/100000], Validation Loss: 549.5088\n",
            "Epoch [5856/100000], Validation Loss: 544.0161\n",
            "Epoch [5857/100000], Validation Loss: 545.8034\n",
            "Epoch [5858/100000], Validation Loss: 550.4923\n",
            "Epoch [5859/100000], Validation Loss: 550.8736\n",
            "Epoch [5860/100000], Validation Loss: 550.1555\n",
            "Epoch [5861/100000], Validation Loss: 547.2750\n",
            "Epoch [5862/100000], Validation Loss: 547.8934\n",
            "Epoch [5863/100000], Validation Loss: 545.1401\n",
            "Epoch [5864/100000], Validation Loss: 550.7687\n",
            "Epoch [5865/100000], Validation Loss: 548.0522\n",
            "Epoch [5866/100000], Validation Loss: 547.9184\n",
            "Epoch [5867/100000], Validation Loss: 543.1527\n",
            "Epoch [5868/100000], Validation Loss: 545.3395\n",
            "Epoch [5869/100000], Validation Loss: 550.0745\n",
            "Epoch [5870/100000], Validation Loss: 547.5822\n",
            "Epoch [5871/100000], Validation Loss: 550.7090\n",
            "Epoch [5872/100000], Validation Loss: 547.9700\n",
            "Epoch [5873/100000], Validation Loss: 550.8968\n",
            "Epoch [5874/100000], Validation Loss: 543.7996\n",
            "Epoch [5875/100000], Validation Loss: 550.0986\n",
            "Epoch [5876/100000], Validation Loss: 553.3903\n",
            "Epoch [5877/100000], Validation Loss: 548.6304\n",
            "Epoch [5878/100000], Validation Loss: 549.3237\n",
            "Epoch [5879/100000], Validation Loss: 553.2082\n",
            "Epoch [5880/100000], Validation Loss: 546.1307\n",
            "Epoch [5881/100000], Validation Loss: 549.1421\n",
            "Epoch [5882/100000], Validation Loss: 547.2318\n",
            "Epoch [5883/100000], Validation Loss: 548.2655\n",
            "Epoch [5884/100000], Validation Loss: 551.0544\n",
            "Epoch [5885/100000], Validation Loss: 551.3835\n",
            "Epoch [5886/100000], Validation Loss: 548.8169\n",
            "Epoch [5887/100000], Validation Loss: 551.9450\n",
            "Epoch [5888/100000], Validation Loss: 548.1933\n",
            "Epoch [5889/100000], Validation Loss: 550.4530\n",
            "Epoch [5890/100000], Validation Loss: 545.8644\n",
            "Epoch [5891/100000], Validation Loss: 544.8016\n",
            "Epoch [5892/100000], Validation Loss: 547.6693\n",
            "Epoch [5893/100000], Validation Loss: 549.7953\n",
            "Epoch [5894/100000], Validation Loss: 551.9166\n",
            "Epoch [5895/100000], Validation Loss: 549.5271\n",
            "Epoch [5896/100000], Validation Loss: 549.2372\n",
            "Epoch [5897/100000], Validation Loss: 550.5362\n",
            "Epoch [5898/100000], Validation Loss: 547.0926\n",
            "Epoch [5899/100000], Validation Loss: 546.4621\n",
            "Epoch [5900/100000], Validation Loss: 544.6754\n",
            "Epoch [5901/100000], Validation Loss: 546.5345\n",
            "Epoch [5902/100000], Validation Loss: 551.7421\n",
            "Epoch [5903/100000], Validation Loss: 578.8289\n",
            "Epoch [5904/100000], Validation Loss: 546.2123\n",
            "Epoch [5905/100000], Validation Loss: 549.0811\n",
            "Epoch [5906/100000], Validation Loss: 550.1421\n",
            "Epoch [5907/100000], Validation Loss: 553.1792\n",
            "Epoch [5908/100000], Validation Loss: 547.6785\n",
            "Epoch [5909/100000], Validation Loss: 546.7955\n",
            "Epoch [5910/100000], Validation Loss: 548.4397\n",
            "Epoch [5911/100000], Validation Loss: 551.1157\n",
            "Epoch [5912/100000], Validation Loss: 554.7602\n",
            "Epoch [5913/100000], Validation Loss: 549.2826\n",
            "Epoch [5914/100000], Validation Loss: 555.0984\n",
            "Epoch [5915/100000], Validation Loss: 552.6680\n",
            "Epoch [5916/100000], Validation Loss: 547.5382\n",
            "Epoch [5917/100000], Validation Loss: 550.0070\n",
            "Epoch [5918/100000], Validation Loss: 549.7967\n",
            "Epoch [5919/100000], Validation Loss: 555.3631\n",
            "Epoch [5920/100000], Validation Loss: 551.3353\n",
            "Epoch [5921/100000], Validation Loss: 549.4135\n",
            "Epoch [5922/100000], Validation Loss: 545.8460\n",
            "Epoch [5923/100000], Validation Loss: 551.2586\n",
            "Epoch [5924/100000], Validation Loss: 548.5178\n",
            "Epoch [5925/100000], Validation Loss: 551.4214\n",
            "Epoch [5926/100000], Validation Loss: 544.9989\n",
            "Epoch [5927/100000], Validation Loss: 548.0858\n",
            "Epoch [5928/100000], Validation Loss: 544.8220\n",
            "Epoch [5929/100000], Validation Loss: 552.7223\n",
            "Epoch [5930/100000], Validation Loss: 569.6136\n",
            "Epoch [5931/100000], Validation Loss: 548.8409\n",
            "Epoch [5932/100000], Validation Loss: 548.7209\n",
            "Epoch [5933/100000], Validation Loss: 550.6021\n",
            "Epoch [5934/100000], Validation Loss: 546.8802\n",
            "Epoch [5935/100000], Validation Loss: 548.2312\n",
            "Epoch [5936/100000], Validation Loss: 549.6347\n",
            "Epoch [5937/100000], Validation Loss: 546.4477\n",
            "Epoch [5938/100000], Validation Loss: 552.0855\n",
            "Epoch [5939/100000], Validation Loss: 553.6927\n",
            "Epoch [5940/100000], Validation Loss: 553.9178\n",
            "Epoch [5941/100000], Validation Loss: 558.7806\n",
            "Epoch [5942/100000], Validation Loss: 549.5983\n",
            "Epoch [5943/100000], Validation Loss: 554.3150\n",
            "Epoch [5944/100000], Validation Loss: 552.0658\n",
            "Epoch [5945/100000], Validation Loss: 549.5401\n",
            "Epoch [5946/100000], Validation Loss: 554.3173\n",
            "Epoch [5947/100000], Validation Loss: 544.5825\n",
            "Epoch [5948/100000], Validation Loss: 547.8390\n",
            "Epoch [5949/100000], Validation Loss: 553.5560\n",
            "Epoch [5950/100000], Validation Loss: 556.7233\n",
            "Epoch [5951/100000], Validation Loss: 553.4910\n",
            "Epoch [5952/100000], Validation Loss: 549.4540\n",
            "Epoch [5953/100000], Validation Loss: 551.4959\n",
            "Epoch [5954/100000], Validation Loss: 550.5981\n",
            "Epoch [5955/100000], Validation Loss: 549.5260\n",
            "Epoch [5956/100000], Validation Loss: 547.7556\n",
            "Epoch [5957/100000], Validation Loss: 545.6132\n",
            "Epoch [5958/100000], Validation Loss: 548.1643\n",
            "Epoch [5959/100000], Validation Loss: 560.0695\n",
            "Epoch [5960/100000], Validation Loss: 547.8963\n",
            "Epoch [5961/100000], Validation Loss: 552.9942\n",
            "Epoch [5962/100000], Validation Loss: 548.1425\n",
            "Epoch [5963/100000], Validation Loss: 546.9603\n",
            "Epoch [5964/100000], Validation Loss: 546.4033\n",
            "Epoch [5965/100000], Validation Loss: 552.0778\n",
            "Epoch [5966/100000], Validation Loss: 552.5530\n",
            "Epoch [5967/100000], Validation Loss: 544.2535\n",
            "Epoch [5968/100000], Validation Loss: 550.4780\n",
            "Epoch [5969/100000], Validation Loss: 546.8248\n",
            "Epoch [5970/100000], Validation Loss: 545.1600\n",
            "Epoch [5971/100000], Validation Loss: 548.5624\n",
            "Epoch [5972/100000], Validation Loss: 550.0694\n",
            "Epoch [5973/100000], Validation Loss: 550.5270\n",
            "Epoch [5974/100000], Validation Loss: 545.5138\n",
            "Epoch [5975/100000], Validation Loss: 549.2360\n",
            "Epoch [5976/100000], Validation Loss: 544.2714\n",
            "Epoch [5977/100000], Validation Loss: 556.9903\n",
            "Epoch [5978/100000], Validation Loss: 550.4167\n",
            "Epoch [5979/100000], Validation Loss: 546.0674\n",
            "Epoch [5980/100000], Validation Loss: 552.4819\n",
            "Epoch [5981/100000], Validation Loss: 543.7985\n",
            "Epoch [5982/100000], Validation Loss: 545.8824\n",
            "Epoch [5983/100000], Validation Loss: 559.9448\n",
            "Epoch [5984/100000], Validation Loss: 553.3082\n",
            "Epoch [5985/100000], Validation Loss: 547.8587\n",
            "Epoch [5986/100000], Validation Loss: 546.6187\n",
            "Epoch [5987/100000], Validation Loss: 545.6673\n",
            "Epoch [5988/100000], Validation Loss: 550.5105\n",
            "Epoch [5989/100000], Validation Loss: 550.3425\n",
            "Epoch [5990/100000], Validation Loss: 548.2063\n",
            "Epoch [5991/100000], Validation Loss: 541.1840\n",
            "Epoch [5992/100000], Validation Loss: 548.4057\n",
            "Epoch [5993/100000], Validation Loss: 548.8079\n",
            "Epoch [5994/100000], Validation Loss: 547.2616\n",
            "Epoch [5995/100000], Validation Loss: 547.8122\n",
            "Epoch [5996/100000], Validation Loss: 550.1658\n",
            "Epoch [5997/100000], Validation Loss: 556.4434\n",
            "Epoch [5998/100000], Validation Loss: 555.7536\n",
            "Epoch [5999/100000], Validation Loss: 544.6708\n",
            "Epoch [6000/100000], Validation Loss: 545.6928\n",
            "Epoch [6001/100000], Validation Loss: 543.9746\n",
            "Epoch [6002/100000], Validation Loss: 549.2436\n",
            "Epoch [6003/100000], Validation Loss: 547.3797\n",
            "Epoch [6004/100000], Validation Loss: 547.0934\n",
            "Epoch [6005/100000], Validation Loss: 552.3854\n",
            "Epoch [6006/100000], Validation Loss: 546.8547\n",
            "Epoch [6007/100000], Validation Loss: 557.0087\n",
            "Epoch [6008/100000], Validation Loss: 543.5479\n",
            "Epoch [6009/100000], Validation Loss: 550.1665\n",
            "Epoch [6010/100000], Validation Loss: 549.0687\n",
            "Epoch [6011/100000], Validation Loss: 546.1164\n",
            "Epoch [6012/100000], Validation Loss: 555.1492\n",
            "Epoch [6013/100000], Validation Loss: 548.3327\n",
            "Epoch [6014/100000], Validation Loss: 545.2059\n",
            "Epoch [6015/100000], Validation Loss: 545.0180\n",
            "Epoch [6016/100000], Validation Loss: 560.7690\n",
            "Epoch [6017/100000], Validation Loss: 545.8440\n",
            "Epoch [6018/100000], Validation Loss: 549.7931\n",
            "Epoch [6019/100000], Validation Loss: 549.8453\n",
            "Epoch [6020/100000], Validation Loss: 553.4986\n",
            "Epoch [6021/100000], Validation Loss: 547.5590\n",
            "Epoch [6022/100000], Validation Loss: 553.7177\n",
            "Epoch [6023/100000], Validation Loss: 546.9208\n",
            "Epoch [6024/100000], Validation Loss: 551.3657\n",
            "Epoch [6025/100000], Validation Loss: 546.6884\n",
            "Epoch [6026/100000], Validation Loss: 547.5919\n",
            "Epoch [6027/100000], Validation Loss: 547.2491\n",
            "Epoch [6028/100000], Validation Loss: 549.5361\n",
            "Epoch [6029/100000], Validation Loss: 545.7573\n",
            "Epoch [6030/100000], Validation Loss: 546.1057\n",
            "Epoch [6031/100000], Validation Loss: 550.7869\n",
            "Epoch [6032/100000], Validation Loss: 548.6974\n",
            "Epoch [6033/100000], Validation Loss: 544.4299\n",
            "Epoch [6034/100000], Validation Loss: 552.0866\n",
            "Epoch [6035/100000], Validation Loss: 552.6218\n",
            "Epoch [6036/100000], Validation Loss: 549.0542\n",
            "Epoch [6037/100000], Validation Loss: 548.0985\n",
            "Epoch [6038/100000], Validation Loss: 547.3704\n",
            "Epoch [6039/100000], Validation Loss: 547.1443\n",
            "Epoch [6040/100000], Validation Loss: 545.7119\n",
            "Epoch [6041/100000], Validation Loss: 545.8632\n",
            "Epoch [6042/100000], Validation Loss: 550.0702\n",
            "Epoch [6043/100000], Validation Loss: 550.8998\n",
            "Epoch [6044/100000], Validation Loss: 555.2052\n",
            "Epoch [6045/100000], Validation Loss: 546.8919\n",
            "Epoch [6046/100000], Validation Loss: 553.0819\n",
            "Epoch [6047/100000], Validation Loss: 549.6309\n",
            "Epoch [6048/100000], Validation Loss: 544.6048\n",
            "Epoch [6049/100000], Validation Loss: 557.0679\n",
            "Epoch [6050/100000], Validation Loss: 545.5612\n",
            "Epoch [6051/100000], Validation Loss: 545.9542\n",
            "Epoch [6052/100000], Validation Loss: 544.0115\n",
            "Epoch [6053/100000], Validation Loss: 550.3992\n",
            "Epoch [6054/100000], Validation Loss: 545.8242\n",
            "Epoch [6055/100000], Validation Loss: 553.0415\n",
            "Epoch [6056/100000], Validation Loss: 546.9110\n",
            "Epoch [6057/100000], Validation Loss: 551.2758\n",
            "Epoch [6058/100000], Validation Loss: 548.5019\n",
            "Epoch [6059/100000], Validation Loss: 549.2656\n",
            "Epoch [6060/100000], Validation Loss: 552.2756\n",
            "Epoch [6061/100000], Validation Loss: 547.9646\n",
            "Epoch [6062/100000], Validation Loss: 547.5492\n",
            "Epoch [6063/100000], Validation Loss: 549.4567\n",
            "Epoch [6064/100000], Validation Loss: 546.1412\n",
            "Epoch [6065/100000], Validation Loss: 550.8550\n",
            "Epoch [6066/100000], Validation Loss: 556.0196\n",
            "Epoch [6067/100000], Validation Loss: 551.1553\n",
            "Epoch [6068/100000], Validation Loss: 549.4456\n",
            "Epoch [6069/100000], Validation Loss: 550.9633\n",
            "Epoch [6070/100000], Validation Loss: 547.9996\n",
            "Epoch [6071/100000], Validation Loss: 547.5923\n",
            "Epoch [6072/100000], Validation Loss: 548.6281\n",
            "Epoch [6073/100000], Validation Loss: 550.0365\n",
            "Epoch [6074/100000], Validation Loss: 543.9666\n",
            "Epoch [6075/100000], Validation Loss: 547.0472\n",
            "Epoch [6076/100000], Validation Loss: 551.5971\n",
            "Epoch [6077/100000], Validation Loss: 550.4717\n",
            "Epoch [6078/100000], Validation Loss: 551.4183\n",
            "Epoch [6079/100000], Validation Loss: 550.3847\n",
            "Epoch [6080/100000], Validation Loss: 560.9313\n",
            "Epoch [6081/100000], Validation Loss: 555.7408\n",
            "Epoch [6082/100000], Validation Loss: 551.1900\n",
            "Epoch [6083/100000], Validation Loss: 556.6127\n",
            "Epoch [6084/100000], Validation Loss: 547.7139\n",
            "Epoch [6085/100000], Validation Loss: 547.8355\n",
            "Epoch [6086/100000], Validation Loss: 554.9769\n",
            "Epoch [6087/100000], Validation Loss: 548.4714\n",
            "Epoch [6088/100000], Validation Loss: 550.0075\n",
            "Epoch [6089/100000], Validation Loss: 553.7427\n",
            "Epoch [6090/100000], Validation Loss: 548.7384\n",
            "Epoch [6091/100000], Validation Loss: 547.6167\n",
            "Epoch [6092/100000], Validation Loss: 556.3515\n",
            "Epoch [6093/100000], Validation Loss: 560.7143\n",
            "Epoch [6094/100000], Validation Loss: 545.7976\n",
            "Epoch [6095/100000], Validation Loss: 548.4559\n",
            "Epoch [6096/100000], Validation Loss: 550.3209\n",
            "Epoch [6097/100000], Validation Loss: 547.2699\n",
            "Epoch [6098/100000], Validation Loss: 547.0743\n",
            "Epoch [6099/100000], Validation Loss: 553.6458\n",
            "Epoch [6100/100000], Validation Loss: 547.5698\n",
            "Epoch [6101/100000], Validation Loss: 551.4053\n",
            "Epoch [6102/100000], Validation Loss: 549.0269\n",
            "Epoch [6103/100000], Validation Loss: 546.8639\n",
            "Epoch [6104/100000], Validation Loss: 550.2030\n",
            "Epoch [6105/100000], Validation Loss: 553.0621\n",
            "Epoch [6106/100000], Validation Loss: 547.4045\n",
            "Epoch [6107/100000], Validation Loss: 542.2940\n",
            "Epoch [6108/100000], Validation Loss: 545.2266\n",
            "Epoch [6109/100000], Validation Loss: 554.3628\n",
            "Epoch [6110/100000], Validation Loss: 548.3754\n",
            "Epoch [6111/100000], Validation Loss: 553.1260\n",
            "Epoch [6112/100000], Validation Loss: 552.0720\n",
            "Epoch [6113/100000], Validation Loss: 543.5240\n",
            "Epoch [6114/100000], Validation Loss: 551.7577\n",
            "Epoch [6115/100000], Validation Loss: 549.2084\n",
            "Epoch [6116/100000], Validation Loss: 546.9139\n",
            "Epoch [6117/100000], Validation Loss: 546.8490\n",
            "Epoch [6118/100000], Validation Loss: 546.9941\n",
            "Epoch [6119/100000], Validation Loss: 546.7462\n",
            "Epoch [6120/100000], Validation Loss: 541.6545\n",
            "Epoch [6121/100000], Validation Loss: 547.4864\n",
            "Epoch [6122/100000], Validation Loss: 549.4083\n",
            "Epoch [6123/100000], Validation Loss: 544.8101\n",
            "Epoch [6124/100000], Validation Loss: 543.5700\n",
            "Epoch [6125/100000], Validation Loss: 548.6319\n",
            "Epoch [6126/100000], Validation Loss: 554.8104\n",
            "Epoch [6127/100000], Validation Loss: 546.4597\n",
            "Epoch [6128/100000], Validation Loss: 551.0532\n",
            "Epoch [6129/100000], Validation Loss: 550.2329\n",
            "Epoch [6130/100000], Validation Loss: 550.2007\n",
            "Epoch [6131/100000], Validation Loss: 543.6841\n",
            "Epoch [6132/100000], Validation Loss: 547.5984\n",
            "Epoch [6133/100000], Validation Loss: 547.0802\n",
            "Epoch [6134/100000], Validation Loss: 547.6788\n",
            "Epoch [6135/100000], Validation Loss: 547.4757\n",
            "Epoch [6136/100000], Validation Loss: 551.5955\n",
            "Epoch [6137/100000], Validation Loss: 544.4362\n",
            "Epoch [6138/100000], Validation Loss: 547.2754\n",
            "Epoch [6139/100000], Validation Loss: 554.4818\n",
            "Epoch [6140/100000], Validation Loss: 550.5449\n",
            "Epoch [6141/100000], Validation Loss: 547.2793\n",
            "Epoch [6142/100000], Validation Loss: 554.2133\n",
            "Epoch [6143/100000], Validation Loss: 553.9228\n",
            "Epoch [6144/100000], Validation Loss: 549.8844\n",
            "Epoch [6145/100000], Validation Loss: 543.5589\n",
            "Epoch [6146/100000], Validation Loss: 553.8294\n",
            "Epoch [6147/100000], Validation Loss: 546.3384\n",
            "Epoch [6148/100000], Validation Loss: 558.3087\n",
            "Epoch [6149/100000], Validation Loss: 548.9619\n",
            "Epoch [6150/100000], Validation Loss: 546.9871\n",
            "Epoch [6151/100000], Validation Loss: 545.8808\n",
            "Epoch [6152/100000], Validation Loss: 548.3770\n",
            "Epoch [6153/100000], Validation Loss: 552.8597\n",
            "Epoch [6154/100000], Validation Loss: 550.3708\n",
            "Epoch [6155/100000], Validation Loss: 549.4325\n",
            "Epoch [6156/100000], Validation Loss: 549.5188\n",
            "Epoch [6157/100000], Validation Loss: 549.5088\n",
            "Epoch [6158/100000], Validation Loss: 548.0519\n",
            "Epoch [6159/100000], Validation Loss: 546.4853\n",
            "Epoch [6160/100000], Validation Loss: 549.8368\n",
            "Epoch [6161/100000], Validation Loss: 547.1087\n",
            "Epoch [6162/100000], Validation Loss: 549.1679\n",
            "Epoch [6163/100000], Validation Loss: 547.7964\n",
            "Epoch [6164/100000], Validation Loss: 546.2383\n",
            "Epoch [6165/100000], Validation Loss: 545.5528\n",
            "Epoch [6166/100000], Validation Loss: 551.9351\n",
            "Epoch [6167/100000], Validation Loss: 551.7030\n",
            "Epoch [6168/100000], Validation Loss: 551.3345\n",
            "Epoch [6169/100000], Validation Loss: 544.0801\n",
            "Epoch [6170/100000], Validation Loss: 550.1810\n",
            "Epoch [6171/100000], Validation Loss: 546.6114\n",
            "Epoch [6172/100000], Validation Loss: 545.6767\n",
            "Epoch [6173/100000], Validation Loss: 549.0975\n",
            "Epoch [6174/100000], Validation Loss: 548.9722\n",
            "Epoch [6175/100000], Validation Loss: 546.3718\n",
            "Epoch [6176/100000], Validation Loss: 547.0244\n",
            "Epoch [6177/100000], Validation Loss: 549.0226\n",
            "Epoch [6178/100000], Validation Loss: 548.2631\n",
            "Epoch [6179/100000], Validation Loss: 554.0258\n",
            "Epoch [6180/100000], Validation Loss: 546.0978\n",
            "Epoch [6181/100000], Validation Loss: 554.8924\n",
            "Epoch [6182/100000], Validation Loss: 551.7096\n",
            "Epoch [6183/100000], Validation Loss: 553.2603\n",
            "Epoch [6184/100000], Validation Loss: 549.0208\n",
            "Epoch [6185/100000], Validation Loss: 554.6255\n",
            "Epoch [6186/100000], Validation Loss: 551.6976\n",
            "Epoch [6187/100000], Validation Loss: 548.9641\n",
            "Epoch [6188/100000], Validation Loss: 554.8205\n",
            "Epoch [6189/100000], Validation Loss: 541.3847\n",
            "Epoch [6190/100000], Validation Loss: 550.3992\n",
            "Epoch [6191/100000], Validation Loss: 541.4100\n",
            "Epoch [6192/100000], Validation Loss: 549.9779\n",
            "Epoch [6193/100000], Validation Loss: 547.8403\n",
            "Epoch [6194/100000], Validation Loss: 549.5184\n",
            "Epoch [6195/100000], Validation Loss: 550.1940\n",
            "Epoch [6196/100000], Validation Loss: 553.9926\n",
            "Epoch [6197/100000], Validation Loss: 544.1360\n",
            "Epoch [6198/100000], Validation Loss: 549.1865\n",
            "Epoch [6199/100000], Validation Loss: 548.0178\n",
            "Epoch [6200/100000], Validation Loss: 545.1288\n",
            "Epoch [6201/100000], Validation Loss: 545.1988\n",
            "Epoch [6202/100000], Validation Loss: 548.9815\n",
            "Epoch [6203/100000], Validation Loss: 548.3626\n",
            "Epoch [6204/100000], Validation Loss: 547.0180\n",
            "Epoch [6205/100000], Validation Loss: 555.8045\n",
            "Epoch [6206/100000], Validation Loss: 548.0422\n",
            "Epoch [6207/100000], Validation Loss: 548.2845\n",
            "Epoch [6208/100000], Validation Loss: 546.9443\n",
            "Epoch [6209/100000], Validation Loss: 545.0161\n",
            "Epoch [6210/100000], Validation Loss: 549.0202\n",
            "Epoch [6211/100000], Validation Loss: 547.3120\n",
            "Epoch [6212/100000], Validation Loss: 548.8566\n",
            "Epoch [6213/100000], Validation Loss: 550.9735\n",
            "Epoch [6214/100000], Validation Loss: 553.9098\n",
            "Epoch [6215/100000], Validation Loss: 546.3938\n",
            "Epoch [6216/100000], Validation Loss: 544.0015\n",
            "Epoch [6217/100000], Validation Loss: 550.2002\n",
            "Epoch [6218/100000], Validation Loss: 546.5240\n",
            "Epoch [6219/100000], Validation Loss: 547.4498\n",
            "Epoch [6220/100000], Validation Loss: 549.3287\n",
            "Epoch [6221/100000], Validation Loss: 546.5887\n",
            "Epoch [6222/100000], Validation Loss: 549.0140\n",
            "Epoch [6223/100000], Validation Loss: 543.0348\n",
            "Epoch [6224/100000], Validation Loss: 545.2690\n",
            "Epoch [6225/100000], Validation Loss: 551.6853\n",
            "Epoch [6226/100000], Validation Loss: 545.7447\n",
            "Epoch [6227/100000], Validation Loss: 553.7438\n",
            "Epoch [6228/100000], Validation Loss: 549.4196\n",
            "Epoch [6229/100000], Validation Loss: 549.0924\n",
            "Epoch [6230/100000], Validation Loss: 547.4266\n",
            "Epoch [6231/100000], Validation Loss: 545.6069\n",
            "Epoch [6232/100000], Validation Loss: 547.5988\n",
            "Epoch [6233/100000], Validation Loss: 547.2604\n",
            "Epoch [6234/100000], Validation Loss: 548.8701\n",
            "Epoch [6235/100000], Validation Loss: 543.4501\n",
            "Epoch [6236/100000], Validation Loss: 542.0340\n",
            "Epoch [6237/100000], Validation Loss: 546.5079\n",
            "Epoch [6238/100000], Validation Loss: 547.1044\n",
            "Epoch [6239/100000], Validation Loss: 549.5152\n",
            "Epoch [6240/100000], Validation Loss: 543.9359\n",
            "Epoch [6241/100000], Validation Loss: 553.1560\n",
            "Epoch [6242/100000], Validation Loss: 549.3284\n",
            "Epoch [6243/100000], Validation Loss: 545.6685\n",
            "Epoch [6244/100000], Validation Loss: 547.5339\n",
            "Epoch [6245/100000], Validation Loss: 549.4240\n",
            "Epoch [6246/100000], Validation Loss: 550.9389\n",
            "Epoch [6247/100000], Validation Loss: 544.0036\n",
            "Epoch [6248/100000], Validation Loss: 548.7591\n",
            "Epoch [6249/100000], Validation Loss: 546.7175\n",
            "Epoch [6250/100000], Validation Loss: 547.6099\n",
            "Epoch [6251/100000], Validation Loss: 548.6138\n",
            "Epoch [6252/100000], Validation Loss: 548.1055\n",
            "Epoch [6253/100000], Validation Loss: 547.4930\n",
            "Epoch [6254/100000], Validation Loss: 553.6250\n",
            "Epoch [6255/100000], Validation Loss: 543.9967\n",
            "Epoch [6256/100000], Validation Loss: 550.8624\n",
            "Epoch [6257/100000], Validation Loss: 547.1835\n",
            "Epoch [6258/100000], Validation Loss: 544.6109\n",
            "Epoch [6259/100000], Validation Loss: 545.1746\n",
            "Epoch [6260/100000], Validation Loss: 552.4750\n",
            "Epoch [6261/100000], Validation Loss: 546.4493\n",
            "Epoch [6262/100000], Validation Loss: 548.5737\n",
            "Epoch [6263/100000], Validation Loss: 543.2357\n",
            "Epoch [6264/100000], Validation Loss: 546.5906\n",
            "Epoch [6265/100000], Validation Loss: 546.5327\n",
            "Epoch [6266/100000], Validation Loss: 549.8660\n",
            "Epoch [6267/100000], Validation Loss: 548.0844\n",
            "Epoch [6268/100000], Validation Loss: 553.6145\n",
            "Epoch [6269/100000], Validation Loss: 548.3126\n",
            "Epoch [6270/100000], Validation Loss: 616.7327\n",
            "Epoch [6271/100000], Validation Loss: 550.5163\n",
            "Epoch [6272/100000], Validation Loss: 545.1667\n",
            "Epoch [6273/100000], Validation Loss: 554.6141\n",
            "Epoch [6274/100000], Validation Loss: 548.7418\n",
            "Epoch [6275/100000], Validation Loss: 551.3934\n",
            "Epoch [6276/100000], Validation Loss: 545.8195\n",
            "Epoch [6277/100000], Validation Loss: 552.0991\n",
            "Epoch [6278/100000], Validation Loss: 547.2453\n",
            "Epoch [6279/100000], Validation Loss: 552.0133\n",
            "Epoch [6280/100000], Validation Loss: 545.5242\n",
            "Epoch [6281/100000], Validation Loss: 553.2322\n",
            "Epoch [6282/100000], Validation Loss: 550.4113\n",
            "Epoch [6283/100000], Validation Loss: 550.1054\n",
            "Epoch [6284/100000], Validation Loss: 549.7800\n",
            "Epoch [6285/100000], Validation Loss: 566.0105\n",
            "Epoch [6286/100000], Validation Loss: 549.3043\n",
            "Epoch [6287/100000], Validation Loss: 544.8302\n",
            "Epoch [6288/100000], Validation Loss: 545.5768\n",
            "Epoch [6289/100000], Validation Loss: 560.1637\n",
            "Epoch [6290/100000], Validation Loss: 549.1180\n",
            "Epoch [6291/100000], Validation Loss: 551.9079\n",
            "Epoch [6292/100000], Validation Loss: 555.3295\n",
            "Epoch [6293/100000], Validation Loss: 547.7933\n",
            "Epoch [6294/100000], Validation Loss: 555.9706\n",
            "Epoch [6295/100000], Validation Loss: 548.9138\n",
            "Epoch [6296/100000], Validation Loss: 549.6114\n",
            "Epoch [6297/100000], Validation Loss: 551.6682\n",
            "Epoch [6298/100000], Validation Loss: 554.7316\n",
            "Epoch [6299/100000], Validation Loss: 548.3061\n",
            "Epoch [6300/100000], Validation Loss: 549.5648\n",
            "Epoch [6301/100000], Validation Loss: 547.6889\n",
            "Epoch [6302/100000], Validation Loss: 549.7259\n",
            "Epoch [6303/100000], Validation Loss: 544.4097\n",
            "Epoch [6304/100000], Validation Loss: 551.7526\n",
            "Epoch [6305/100000], Validation Loss: 548.6167\n",
            "Epoch [6306/100000], Validation Loss: 546.6308\n",
            "Epoch [6307/100000], Validation Loss: 547.0436\n",
            "Epoch [6308/100000], Validation Loss: 549.7473\n",
            "Epoch [6309/100000], Validation Loss: 549.4003\n",
            "Epoch [6310/100000], Validation Loss: 556.8046\n",
            "Epoch [6311/100000], Validation Loss: 546.2654\n",
            "Epoch [6312/100000], Validation Loss: 552.0191\n",
            "Epoch [6313/100000], Validation Loss: 549.8503\n",
            "Epoch [6314/100000], Validation Loss: 542.3201\n",
            "Epoch [6315/100000], Validation Loss: 552.2852\n",
            "Epoch [6316/100000], Validation Loss: 550.2931\n",
            "Epoch [6317/100000], Validation Loss: 545.4059\n",
            "Epoch [6318/100000], Validation Loss: 545.9749\n",
            "Epoch [6319/100000], Validation Loss: 551.1862\n",
            "Epoch [6320/100000], Validation Loss: 545.1021\n",
            "Epoch [6321/100000], Validation Loss: 550.4738\n",
            "Epoch [6322/100000], Validation Loss: 548.2647\n",
            "Epoch [6323/100000], Validation Loss: 550.6249\n",
            "Epoch [6324/100000], Validation Loss: 550.0343\n",
            "Epoch [6325/100000], Validation Loss: 550.3532\n",
            "Epoch [6326/100000], Validation Loss: 551.6509\n",
            "Epoch [6327/100000], Validation Loss: 551.1444\n",
            "Epoch [6328/100000], Validation Loss: 551.8308\n",
            "Epoch [6329/100000], Validation Loss: 551.2890\n",
            "Epoch [6330/100000], Validation Loss: 547.1606\n",
            "Epoch [6331/100000], Validation Loss: 552.2445\n",
            "Epoch [6332/100000], Validation Loss: 545.9135\n",
            "Epoch [6333/100000], Validation Loss: 550.1716\n",
            "Epoch [6334/100000], Validation Loss: 548.1946\n",
            "Epoch [6335/100000], Validation Loss: 551.2534\n",
            "Epoch [6336/100000], Validation Loss: 550.9613\n",
            "Epoch [6337/100000], Validation Loss: 548.6585\n",
            "Epoch [6338/100000], Validation Loss: 548.3208\n",
            "Epoch [6339/100000], Validation Loss: 551.5358\n",
            "Epoch [6340/100000], Validation Loss: 546.3140\n",
            "Epoch [6341/100000], Validation Loss: 551.3140\n",
            "Epoch [6342/100000], Validation Loss: 551.8251\n",
            "Epoch [6343/100000], Validation Loss: 550.2083\n",
            "Epoch [6344/100000], Validation Loss: 552.6725\n",
            "Epoch [6345/100000], Validation Loss: 544.0073\n",
            "Epoch [6346/100000], Validation Loss: 547.6148\n",
            "Epoch [6347/100000], Validation Loss: 545.4878\n",
            "Epoch [6348/100000], Validation Loss: 544.2184\n",
            "Epoch [6349/100000], Validation Loss: 545.3967\n",
            "Epoch [6350/100000], Validation Loss: 549.4452\n",
            "Epoch [6351/100000], Validation Loss: 548.9378\n",
            "Epoch [6352/100000], Validation Loss: 558.8243\n",
            "Epoch [6353/100000], Validation Loss: 551.9430\n",
            "Epoch [6354/100000], Validation Loss: 559.8749\n",
            "Epoch [6355/100000], Validation Loss: 549.7146\n",
            "Epoch [6356/100000], Validation Loss: 549.1971\n",
            "Epoch [6357/100000], Validation Loss: 550.2413\n",
            "Epoch [6358/100000], Validation Loss: 551.7665\n",
            "Epoch [6359/100000], Validation Loss: 548.6085\n",
            "Epoch [6360/100000], Validation Loss: 548.9018\n",
            "Epoch [6361/100000], Validation Loss: 549.2741\n",
            "Epoch [6362/100000], Validation Loss: 548.3006\n",
            "Epoch [6363/100000], Validation Loss: 550.3150\n",
            "Epoch [6364/100000], Validation Loss: 550.0941\n",
            "Epoch [6365/100000], Validation Loss: 575.7519\n",
            "Epoch [6366/100000], Validation Loss: 550.6877\n",
            "Epoch [6367/100000], Validation Loss: 546.2112\n",
            "Epoch [6368/100000], Validation Loss: 545.3692\n",
            "Epoch [6369/100000], Validation Loss: 551.2168\n",
            "Epoch [6370/100000], Validation Loss: 545.9517\n",
            "Epoch [6371/100000], Validation Loss: 549.9342\n",
            "Epoch [6372/100000], Validation Loss: 547.8256\n",
            "Epoch [6373/100000], Validation Loss: 548.2393\n",
            "Epoch [6374/100000], Validation Loss: 546.9950\n",
            "Epoch [6375/100000], Validation Loss: 546.6931\n",
            "Epoch [6376/100000], Validation Loss: 549.4325\n",
            "Epoch [6377/100000], Validation Loss: 545.7265\n",
            "Epoch [6378/100000], Validation Loss: 554.3223\n",
            "Epoch [6379/100000], Validation Loss: 549.8449\n",
            "Epoch [6380/100000], Validation Loss: 550.4461\n",
            "Epoch [6381/100000], Validation Loss: 545.7049\n",
            "Epoch [6382/100000], Validation Loss: 546.6964\n",
            "Epoch [6383/100000], Validation Loss: 560.5059\n",
            "Epoch [6384/100000], Validation Loss: 655.0691\n",
            "Epoch [6385/100000], Validation Loss: 555.0367\n",
            "Epoch [6386/100000], Validation Loss: 547.0118\n",
            "Epoch [6387/100000], Validation Loss: 545.8260\n",
            "Epoch [6388/100000], Validation Loss: 548.0042\n",
            "Epoch [6389/100000], Validation Loss: 546.9082\n",
            "Epoch [6390/100000], Validation Loss: 549.0054\n",
            "Epoch [6391/100000], Validation Loss: 557.4557\n",
            "Epoch [6392/100000], Validation Loss: 544.3104\n",
            "Epoch [6393/100000], Validation Loss: 548.8789\n",
            "Epoch [6394/100000], Validation Loss: 549.1200\n",
            "Epoch [6395/100000], Validation Loss: 547.9154\n",
            "Epoch [6396/100000], Validation Loss: 547.1528\n",
            "Epoch [6397/100000], Validation Loss: 554.7092\n",
            "Epoch [6398/100000], Validation Loss: 558.2306\n",
            "Epoch [6399/100000], Validation Loss: 544.0984\n",
            "Epoch [6400/100000], Validation Loss: 546.9138\n",
            "Epoch [6401/100000], Validation Loss: 551.5862\n",
            "Epoch [6402/100000], Validation Loss: 548.7177\n",
            "Epoch [6403/100000], Validation Loss: 550.9942\n",
            "Epoch [6404/100000], Validation Loss: 547.1369\n",
            "Epoch [6405/100000], Validation Loss: 546.6622\n",
            "Epoch [6406/100000], Validation Loss: 550.1149\n",
            "Epoch [6407/100000], Validation Loss: 546.0433\n",
            "Epoch [6408/100000], Validation Loss: 545.2410\n",
            "Epoch [6409/100000], Validation Loss: 547.0030\n",
            "Epoch [6410/100000], Validation Loss: 553.5696\n",
            "Epoch [6411/100000], Validation Loss: 548.9648\n",
            "Epoch [6412/100000], Validation Loss: 550.5334\n",
            "Epoch [6413/100000], Validation Loss: 547.9123\n",
            "Epoch [6414/100000], Validation Loss: 544.7734\n",
            "Epoch [6415/100000], Validation Loss: 546.6409\n",
            "Epoch [6416/100000], Validation Loss: 549.8200\n",
            "Epoch [6417/100000], Validation Loss: 545.8458\n",
            "Epoch [6418/100000], Validation Loss: 555.9807\n",
            "Epoch [6419/100000], Validation Loss: 546.2272\n",
            "Epoch [6420/100000], Validation Loss: 551.4558\n",
            "Epoch [6421/100000], Validation Loss: 551.9151\n",
            "Epoch [6422/100000], Validation Loss: 549.6571\n",
            "Epoch [6423/100000], Validation Loss: 550.2314\n",
            "Epoch [6424/100000], Validation Loss: 550.1422\n",
            "Epoch [6425/100000], Validation Loss: 546.9735\n",
            "Epoch [6426/100000], Validation Loss: 547.2151\n",
            "Epoch [6427/100000], Validation Loss: 546.7751\n",
            "Epoch [6428/100000], Validation Loss: 553.7765\n",
            "Epoch [6429/100000], Validation Loss: 548.9932\n",
            "Epoch [6430/100000], Validation Loss: 545.4730\n",
            "Epoch [6431/100000], Validation Loss: 546.9401\n",
            "Epoch [6432/100000], Validation Loss: 546.7701\n",
            "Epoch [6433/100000], Validation Loss: 548.9693\n",
            "Epoch [6434/100000], Validation Loss: 547.5826\n",
            "Epoch [6435/100000], Validation Loss: 548.3480\n",
            "Epoch [6436/100000], Validation Loss: 541.9343\n",
            "Epoch [6437/100000], Validation Loss: 546.3460\n",
            "Epoch [6438/100000], Validation Loss: 545.8007\n",
            "Epoch [6439/100000], Validation Loss: 547.7617\n",
            "Epoch [6440/100000], Validation Loss: 545.1004\n",
            "Epoch [6441/100000], Validation Loss: 547.2411\n",
            "Epoch [6442/100000], Validation Loss: 549.6332\n",
            "Epoch [6443/100000], Validation Loss: 547.3409\n",
            "Epoch [6444/100000], Validation Loss: 562.6029\n",
            "Epoch [6445/100000], Validation Loss: 549.5659\n",
            "Epoch [6446/100000], Validation Loss: 548.4224\n",
            "Epoch [6447/100000], Validation Loss: 548.5401\n",
            "Epoch [6448/100000], Validation Loss: 552.2859\n",
            "Epoch [6449/100000], Validation Loss: 550.7583\n",
            "Epoch [6450/100000], Validation Loss: 552.2718\n",
            "Epoch [6451/100000], Validation Loss: 550.9197\n",
            "Epoch [6452/100000], Validation Loss: 554.6958\n",
            "Epoch [6453/100000], Validation Loss: 553.8117\n",
            "Epoch [6454/100000], Validation Loss: 546.4415\n",
            "Epoch [6455/100000], Validation Loss: 543.7662\n",
            "Epoch [6456/100000], Validation Loss: 551.0233\n",
            "Epoch [6457/100000], Validation Loss: 546.6103\n",
            "Epoch [6458/100000], Validation Loss: 551.3818\n",
            "Epoch [6459/100000], Validation Loss: 544.8116\n",
            "Epoch [6460/100000], Validation Loss: 551.7644\n",
            "Epoch [6461/100000], Validation Loss: 546.0333\n",
            "Epoch [6462/100000], Validation Loss: 542.6903\n",
            "Epoch [6463/100000], Validation Loss: 548.3801\n",
            "Epoch [6464/100000], Validation Loss: 552.1525\n",
            "Epoch [6465/100000], Validation Loss: 551.0465\n",
            "Epoch [6466/100000], Validation Loss: 546.8573\n",
            "Epoch [6467/100000], Validation Loss: 544.9987\n",
            "Epoch [6468/100000], Validation Loss: 545.5329\n",
            "Epoch [6469/100000], Validation Loss: 545.8424\n",
            "Epoch [6470/100000], Validation Loss: 543.8368\n",
            "Epoch [6471/100000], Validation Loss: 547.7649\n",
            "Epoch [6472/100000], Validation Loss: 550.5233\n",
            "Epoch [6473/100000], Validation Loss: 546.3176\n",
            "Epoch [6474/100000], Validation Loss: 548.5973\n",
            "Epoch [6475/100000], Validation Loss: 547.9227\n",
            "Epoch [6476/100000], Validation Loss: 549.9198\n",
            "Epoch [6477/100000], Validation Loss: 544.1980\n",
            "Epoch [6478/100000], Validation Loss: 550.1025\n",
            "Epoch [6479/100000], Validation Loss: 549.9768\n",
            "Epoch [6480/100000], Validation Loss: 549.0500\n",
            "Epoch [6481/100000], Validation Loss: 547.1468\n",
            "Epoch [6482/100000], Validation Loss: 552.5363\n",
            "Epoch [6483/100000], Validation Loss: 550.6058\n",
            "Epoch [6484/100000], Validation Loss: 548.6711\n",
            "Epoch [6485/100000], Validation Loss: 547.7354\n",
            "Epoch [6486/100000], Validation Loss: 547.7946\n",
            "Epoch [6487/100000], Validation Loss: 551.6091\n",
            "Epoch [6488/100000], Validation Loss: 559.1474\n",
            "Epoch [6489/100000], Validation Loss: 545.3529\n",
            "Epoch [6490/100000], Validation Loss: 547.6774\n",
            "Epoch [6491/100000], Validation Loss: 547.2500\n",
            "Epoch [6492/100000], Validation Loss: 553.2944\n",
            "Epoch [6493/100000], Validation Loss: 549.7134\n",
            "Epoch [6494/100000], Validation Loss: 548.2547\n",
            "Epoch [6495/100000], Validation Loss: 552.0401\n",
            "Epoch [6496/100000], Validation Loss: 543.2702\n",
            "Epoch [6497/100000], Validation Loss: 544.4440\n",
            "Epoch [6498/100000], Validation Loss: 546.0971\n",
            "Epoch [6499/100000], Validation Loss: 547.7694\n",
            "Epoch [6500/100000], Validation Loss: 549.2045\n",
            "Epoch [6501/100000], Validation Loss: 545.7420\n",
            "Epoch [6502/100000], Validation Loss: 543.7664\n",
            "Epoch [6503/100000], Validation Loss: 554.7638\n",
            "Epoch [6504/100000], Validation Loss: 549.1251\n",
            "Epoch [6505/100000], Validation Loss: 549.1369\n",
            "Epoch [6506/100000], Validation Loss: 551.8374\n",
            "Epoch [6507/100000], Validation Loss: 545.6522\n",
            "Epoch [6508/100000], Validation Loss: 552.8702\n",
            "Epoch [6509/100000], Validation Loss: 545.7071\n",
            "Epoch [6510/100000], Validation Loss: 545.5715\n",
            "Epoch [6511/100000], Validation Loss: 549.2334\n",
            "Epoch [6512/100000], Validation Loss: 549.3380\n",
            "Epoch [6513/100000], Validation Loss: 546.7837\n",
            "Epoch [6514/100000], Validation Loss: 548.6965\n",
            "Epoch [6515/100000], Validation Loss: 548.4055\n",
            "Epoch [6516/100000], Validation Loss: 545.9259\n",
            "Epoch [6517/100000], Validation Loss: 544.9584\n",
            "Epoch [6518/100000], Validation Loss: 551.8962\n",
            "Epoch [6519/100000], Validation Loss: 545.6529\n",
            "Epoch [6520/100000], Validation Loss: 547.5726\n",
            "Epoch [6521/100000], Validation Loss: 553.5892\n",
            "Epoch [6522/100000], Validation Loss: 549.0813\n",
            "Epoch [6523/100000], Validation Loss: 572.2076\n",
            "Epoch [6524/100000], Validation Loss: 550.3719\n",
            "Epoch [6525/100000], Validation Loss: 546.9439\n",
            "Epoch [6526/100000], Validation Loss: 543.4387\n",
            "Epoch [6527/100000], Validation Loss: 551.3552\n",
            "Epoch [6528/100000], Validation Loss: 544.2005\n",
            "Epoch [6529/100000], Validation Loss: 545.4402\n",
            "Epoch [6530/100000], Validation Loss: 546.9314\n",
            "Epoch [6531/100000], Validation Loss: 548.3837\n",
            "Epoch [6532/100000], Validation Loss: 548.7995\n",
            "Epoch [6533/100000], Validation Loss: 545.4366\n",
            "Epoch [6534/100000], Validation Loss: 547.6535\n",
            "Epoch [6535/100000], Validation Loss: 547.6582\n",
            "Epoch [6536/100000], Validation Loss: 549.5122\n",
            "Epoch [6537/100000], Validation Loss: 546.7749\n",
            "Epoch [6538/100000], Validation Loss: 547.6545\n",
            "Epoch [6539/100000], Validation Loss: 545.4119\n",
            "Epoch [6540/100000], Validation Loss: 553.0507\n",
            "Epoch [6541/100000], Validation Loss: 547.0436\n",
            "Epoch [6542/100000], Validation Loss: 550.5621\n",
            "Epoch [6543/100000], Validation Loss: 545.6412\n",
            "Epoch [6544/100000], Validation Loss: 545.9087\n",
            "Epoch [6545/100000], Validation Loss: 546.3460\n",
            "Epoch [6546/100000], Validation Loss: 546.2422\n",
            "Epoch [6547/100000], Validation Loss: 541.3479\n",
            "Epoch [6548/100000], Validation Loss: 549.8638\n",
            "Epoch [6549/100000], Validation Loss: 548.4000\n",
            "Epoch [6550/100000], Validation Loss: 544.1928\n",
            "Epoch [6551/100000], Validation Loss: 541.0422\n",
            "Epoch [6552/100000], Validation Loss: 552.7524\n",
            "Epoch [6553/100000], Validation Loss: 548.0501\n",
            "Epoch [6554/100000], Validation Loss: 547.7457\n",
            "Epoch [6555/100000], Validation Loss: 545.9588\n",
            "Epoch [6556/100000], Validation Loss: 547.0153\n",
            "Epoch [6557/100000], Validation Loss: 548.1175\n",
            "Epoch [6558/100000], Validation Loss: 551.5762\n",
            "Epoch [6559/100000], Validation Loss: 546.8019\n",
            "Epoch [6560/100000], Validation Loss: 546.5762\n",
            "Epoch [6561/100000], Validation Loss: 551.2191\n",
            "Epoch [6562/100000], Validation Loss: 548.0643\n",
            "Epoch [6563/100000], Validation Loss: 551.9047\n",
            "Epoch [6564/100000], Validation Loss: 545.9703\n",
            "Epoch [6565/100000], Validation Loss: 549.5857\n",
            "Epoch [6566/100000], Validation Loss: 552.2987\n",
            "Epoch [6567/100000], Validation Loss: 549.2565\n",
            "Epoch [6568/100000], Validation Loss: 549.9206\n",
            "Epoch [6569/100000], Validation Loss: 552.0366\n",
            "Epoch [6570/100000], Validation Loss: 547.1810\n",
            "Epoch [6571/100000], Validation Loss: 551.3557\n",
            "Epoch [6572/100000], Validation Loss: 548.5835\n",
            "Epoch [6573/100000], Validation Loss: 555.7502\n",
            "Epoch [6574/100000], Validation Loss: 554.1398\n",
            "Epoch [6575/100000], Validation Loss: 550.8554\n",
            "Epoch [6576/100000], Validation Loss: 556.7640\n",
            "Epoch [6577/100000], Validation Loss: 545.9270\n",
            "Epoch [6578/100000], Validation Loss: 563.3134\n",
            "Epoch [6579/100000], Validation Loss: 550.6081\n",
            "Epoch [6580/100000], Validation Loss: 551.0481\n",
            "Epoch [6581/100000], Validation Loss: 549.7546\n",
            "Epoch [6582/100000], Validation Loss: 549.6980\n",
            "Epoch [6583/100000], Validation Loss: 548.8498\n",
            "Epoch [6584/100000], Validation Loss: 547.6299\n",
            "Epoch [6585/100000], Validation Loss: 548.3886\n",
            "Epoch [6586/100000], Validation Loss: 548.1857\n",
            "Epoch [6587/100000], Validation Loss: 544.8629\n",
            "Epoch [6588/100000], Validation Loss: 544.4404\n",
            "Epoch [6589/100000], Validation Loss: 548.3600\n",
            "Epoch [6590/100000], Validation Loss: 548.6419\n",
            "Epoch [6591/100000], Validation Loss: 548.5074\n",
            "Epoch [6592/100000], Validation Loss: 545.4587\n",
            "Epoch [6593/100000], Validation Loss: 546.7268\n",
            "Epoch [6594/100000], Validation Loss: 548.0891\n",
            "Epoch [6595/100000], Validation Loss: 546.4437\n",
            "Epoch [6596/100000], Validation Loss: 552.2169\n",
            "Epoch [6597/100000], Validation Loss: 550.4985\n",
            "Epoch [6598/100000], Validation Loss: 546.2227\n",
            "Epoch [6599/100000], Validation Loss: 550.6544\n",
            "Epoch [6600/100000], Validation Loss: 550.8580\n",
            "Epoch [6601/100000], Validation Loss: 547.8792\n",
            "Epoch [6602/100000], Validation Loss: 546.1820\n",
            "Epoch [6603/100000], Validation Loss: 546.0020\n",
            "Epoch [6604/100000], Validation Loss: 548.8235\n",
            "Epoch [6605/100000], Validation Loss: 553.1835\n",
            "Epoch [6606/100000], Validation Loss: 557.6368\n",
            "Epoch [6607/100000], Validation Loss: 546.4092\n",
            "Epoch [6608/100000], Validation Loss: 567.3580\n",
            "Epoch [6609/100000], Validation Loss: 559.7774\n",
            "Epoch [6610/100000], Validation Loss: 550.3241\n",
            "Epoch [6611/100000], Validation Loss: 548.7248\n",
            "Epoch [6612/100000], Validation Loss: 549.6018\n",
            "Epoch [6613/100000], Validation Loss: 547.3124\n",
            "Epoch [6614/100000], Validation Loss: 551.6699\n",
            "Epoch [6615/100000], Validation Loss: 547.9386\n",
            "Epoch [6616/100000], Validation Loss: 547.5980\n",
            "Epoch [6617/100000], Validation Loss: 547.9508\n",
            "Epoch [6618/100000], Validation Loss: 547.6935\n",
            "Epoch [6619/100000], Validation Loss: 544.6543\n",
            "Epoch [6620/100000], Validation Loss: 556.3978\n",
            "Epoch [6621/100000], Validation Loss: 548.2237\n",
            "Epoch [6622/100000], Validation Loss: 550.3808\n",
            "Epoch [6623/100000], Validation Loss: 545.5467\n",
            "Epoch [6624/100000], Validation Loss: 547.7938\n",
            "Epoch [6625/100000], Validation Loss: 547.2689\n",
            "Epoch [6626/100000], Validation Loss: 553.2891\n",
            "Epoch [6627/100000], Validation Loss: 545.1545\n",
            "Epoch [6628/100000], Validation Loss: 546.5889\n",
            "Epoch [6629/100000], Validation Loss: 548.6083\n",
            "Epoch [6630/100000], Validation Loss: 549.7653\n",
            "Epoch [6631/100000], Validation Loss: 547.0055\n",
            "Epoch [6632/100000], Validation Loss: 549.2062\n",
            "Epoch [6633/100000], Validation Loss: 551.5330\n",
            "Epoch [6634/100000], Validation Loss: 551.9638\n",
            "Epoch [6635/100000], Validation Loss: 551.5549\n",
            "Epoch [6636/100000], Validation Loss: 547.4543\n",
            "Epoch [6637/100000], Validation Loss: 550.4813\n",
            "Epoch [6638/100000], Validation Loss: 548.6018\n",
            "Epoch [6639/100000], Validation Loss: 546.1255\n",
            "Epoch [6640/100000], Validation Loss: 547.6400\n",
            "Epoch [6641/100000], Validation Loss: 552.9832\n",
            "Epoch [6642/100000], Validation Loss: 558.2121\n",
            "Epoch [6643/100000], Validation Loss: 548.9478\n",
            "Epoch [6644/100000], Validation Loss: 548.2867\n",
            "Epoch [6645/100000], Validation Loss: 545.2391\n",
            "Epoch [6646/100000], Validation Loss: 547.2413\n",
            "Epoch [6647/100000], Validation Loss: 547.0191\n",
            "Epoch [6648/100000], Validation Loss: 560.1629\n",
            "Epoch [6649/100000], Validation Loss: 545.9436\n",
            "Epoch [6650/100000], Validation Loss: 553.5116\n",
            "Epoch [6651/100000], Validation Loss: 546.6159\n",
            "Epoch [6652/100000], Validation Loss: 547.2611\n",
            "Epoch [6653/100000], Validation Loss: 545.0251\n",
            "Epoch [6654/100000], Validation Loss: 544.2643\n",
            "Epoch [6655/100000], Validation Loss: 548.2371\n",
            "Epoch [6656/100000], Validation Loss: 548.5059\n",
            "Epoch [6657/100000], Validation Loss: 546.4862\n",
            "Epoch [6658/100000], Validation Loss: 551.4283\n",
            "Epoch [6659/100000], Validation Loss: 548.1336\n",
            "Epoch [6660/100000], Validation Loss: 549.3560\n",
            "Epoch [6661/100000], Validation Loss: 549.3065\n",
            "Epoch [6662/100000], Validation Loss: 549.0214\n",
            "Epoch [6663/100000], Validation Loss: 545.2578\n",
            "Epoch [6664/100000], Validation Loss: 551.7587\n",
            "Epoch [6665/100000], Validation Loss: 548.0687\n",
            "Epoch [6666/100000], Validation Loss: 550.0178\n",
            "Epoch [6667/100000], Validation Loss: 547.2837\n",
            "Epoch [6668/100000], Validation Loss: 548.2389\n",
            "Epoch [6669/100000], Validation Loss: 547.1582\n",
            "Epoch [6670/100000], Validation Loss: 546.0424\n",
            "Epoch [6671/100000], Validation Loss: 542.2387\n",
            "Epoch [6672/100000], Validation Loss: 544.7063\n",
            "Epoch [6673/100000], Validation Loss: 576.6709\n",
            "Epoch [6674/100000], Validation Loss: 547.3858\n",
            "Epoch [6675/100000], Validation Loss: 544.2387\n",
            "Epoch [6676/100000], Validation Loss: 546.8759\n",
            "Epoch [6677/100000], Validation Loss: 548.8129\n",
            "Epoch [6678/100000], Validation Loss: 560.6278\n",
            "Epoch [6679/100000], Validation Loss: 552.4820\n",
            "Epoch [6680/100000], Validation Loss: 547.7561\n",
            "Epoch [6681/100000], Validation Loss: 547.6074\n",
            "Epoch [6682/100000], Validation Loss: 555.2683\n",
            "Epoch [6683/100000], Validation Loss: 551.3670\n",
            "Epoch [6684/100000], Validation Loss: 546.5691\n",
            "Epoch [6685/100000], Validation Loss: 558.6232\n",
            "Epoch [6686/100000], Validation Loss: 561.3189\n",
            "Epoch [6687/100000], Validation Loss: 553.8549\n",
            "Epoch [6688/100000], Validation Loss: 544.2175\n",
            "Epoch [6689/100000], Validation Loss: 547.2778\n",
            "Epoch [6690/100000], Validation Loss: 551.9543\n",
            "Epoch [6691/100000], Validation Loss: 550.0087\n",
            "Epoch [6692/100000], Validation Loss: 545.8329\n",
            "Epoch [6693/100000], Validation Loss: 550.3746\n",
            "Epoch [6694/100000], Validation Loss: 548.6882\n",
            "Epoch [6695/100000], Validation Loss: 552.5010\n",
            "Epoch [6696/100000], Validation Loss: 552.0787\n",
            "Epoch [6697/100000], Validation Loss: 546.8931\n",
            "Epoch [6698/100000], Validation Loss: 551.3867\n",
            "Epoch [6699/100000], Validation Loss: 545.0648\n",
            "Epoch [6700/100000], Validation Loss: 551.0587\n",
            "Epoch [6701/100000], Validation Loss: 568.2090\n",
            "Epoch [6702/100000], Validation Loss: 546.3421\n",
            "Epoch [6703/100000], Validation Loss: 543.3988\n",
            "Epoch [6704/100000], Validation Loss: 548.6399\n",
            "Epoch [6705/100000], Validation Loss: 553.4402\n",
            "Epoch [6706/100000], Validation Loss: 549.4077\n",
            "Epoch [6707/100000], Validation Loss: 546.0526\n",
            "Epoch [6708/100000], Validation Loss: 545.6303\n",
            "Epoch [6709/100000], Validation Loss: 551.3993\n",
            "Epoch [6710/100000], Validation Loss: 559.9118\n",
            "Epoch [6711/100000], Validation Loss: 550.3033\n",
            "Epoch [6712/100000], Validation Loss: 547.6275\n",
            "Epoch [6713/100000], Validation Loss: 547.1013\n",
            "Epoch [6714/100000], Validation Loss: 546.8254\n",
            "Epoch [6715/100000], Validation Loss: 552.5480\n",
            "Epoch [6716/100000], Validation Loss: 555.7317\n",
            "Epoch [6717/100000], Validation Loss: 551.8928\n",
            "Epoch [6718/100000], Validation Loss: 543.1389\n",
            "Epoch [6719/100000], Validation Loss: 549.6930\n",
            "Epoch [6720/100000], Validation Loss: 549.7008\n",
            "Epoch [6721/100000], Validation Loss: 550.6412\n",
            "Epoch [6722/100000], Validation Loss: 544.6643\n",
            "Epoch [6723/100000], Validation Loss: 553.1715\n",
            "Epoch [6724/100000], Validation Loss: 545.7469\n",
            "Epoch [6725/100000], Validation Loss: 545.8206\n",
            "Epoch [6726/100000], Validation Loss: 544.5063\n",
            "Epoch [6727/100000], Validation Loss: 554.3920\n",
            "Epoch [6728/100000], Validation Loss: 547.6436\n",
            "Epoch [6729/100000], Validation Loss: 551.4408\n",
            "Epoch [6730/100000], Validation Loss: 546.5395\n",
            "Epoch [6731/100000], Validation Loss: 551.6853\n",
            "Epoch [6732/100000], Validation Loss: 546.7138\n",
            "Epoch [6733/100000], Validation Loss: 541.3339\n",
            "Epoch [6734/100000], Validation Loss: 547.6999\n",
            "Epoch [6735/100000], Validation Loss: 545.2753\n",
            "Epoch [6736/100000], Validation Loss: 546.7306\n",
            "Epoch [6737/100000], Validation Loss: 546.3074\n",
            "Epoch [6738/100000], Validation Loss: 548.2895\n",
            "Epoch [6739/100000], Validation Loss: 544.7889\n",
            "Epoch [6740/100000], Validation Loss: 557.8090\n",
            "Epoch [6741/100000], Validation Loss: 547.4636\n",
            "Epoch [6742/100000], Validation Loss: 549.3781\n",
            "Epoch [6743/100000], Validation Loss: 547.9010\n",
            "Epoch [6744/100000], Validation Loss: 546.6177\n",
            "Epoch [6745/100000], Validation Loss: 547.4696\n",
            "Epoch [6746/100000], Validation Loss: 543.6328\n",
            "Epoch [6747/100000], Validation Loss: 546.5525\n",
            "Epoch [6748/100000], Validation Loss: 550.5265\n",
            "Epoch [6749/100000], Validation Loss: 550.5118\n",
            "Epoch [6750/100000], Validation Loss: 547.3240\n",
            "Epoch [6751/100000], Validation Loss: 545.7703\n",
            "Epoch [6752/100000], Validation Loss: 559.7991\n",
            "Epoch [6753/100000], Validation Loss: 548.1092\n",
            "Epoch [6754/100000], Validation Loss: 548.3931\n",
            "Epoch [6755/100000], Validation Loss: 554.8316\n",
            "Epoch [6756/100000], Validation Loss: 547.2351\n",
            "Epoch [6757/100000], Validation Loss: 550.8848\n",
            "Epoch [6758/100000], Validation Loss: 546.4111\n",
            "Epoch [6759/100000], Validation Loss: 559.9765\n",
            "Epoch [6760/100000], Validation Loss: 551.1548\n",
            "Epoch [6761/100000], Validation Loss: 571.9323\n",
            "Epoch [6762/100000], Validation Loss: 547.2553\n",
            "Epoch [6763/100000], Validation Loss: 547.5927\n",
            "Epoch [6764/100000], Validation Loss: 550.5280\n",
            "Epoch [6765/100000], Validation Loss: 546.1676\n",
            "Epoch [6766/100000], Validation Loss: 551.4899\n",
            "Epoch [6767/100000], Validation Loss: 549.3901\n",
            "Epoch [6768/100000], Validation Loss: 551.1378\n",
            "Epoch [6769/100000], Validation Loss: 549.8863\n",
            "Epoch [6770/100000], Validation Loss: 548.2374\n",
            "Epoch [6771/100000], Validation Loss: 546.3431\n",
            "Epoch [6772/100000], Validation Loss: 554.1334\n",
            "Epoch [6773/100000], Validation Loss: 550.3291\n",
            "Epoch [6774/100000], Validation Loss: 548.7350\n",
            "Epoch [6775/100000], Validation Loss: 548.2623\n",
            "Epoch [6776/100000], Validation Loss: 550.6020\n",
            "Epoch [6777/100000], Validation Loss: 553.5537\n",
            "Epoch [6778/100000], Validation Loss: 551.4476\n",
            "Epoch [6779/100000], Validation Loss: 553.9668\n",
            "Epoch [6780/100000], Validation Loss: 545.3241\n",
            "Epoch [6781/100000], Validation Loss: 550.9239\n",
            "Epoch [6782/100000], Validation Loss: 548.6117\n",
            "Epoch [6783/100000], Validation Loss: 548.3075\n",
            "Epoch [6784/100000], Validation Loss: 546.1009\n",
            "Epoch [6785/100000], Validation Loss: 548.4495\n",
            "Epoch [6786/100000], Validation Loss: 549.1895\n",
            "Epoch [6787/100000], Validation Loss: 553.6792\n",
            "Epoch [6788/100000], Validation Loss: 550.1649\n",
            "Epoch [6789/100000], Validation Loss: 549.0049\n",
            "Epoch [6790/100000], Validation Loss: 552.4097\n",
            "Epoch [6791/100000], Validation Loss: 549.4560\n",
            "Epoch [6792/100000], Validation Loss: 546.7199\n",
            "Epoch [6793/100000], Validation Loss: 544.3902\n",
            "Epoch [6794/100000], Validation Loss: 553.8595\n",
            "Epoch [6795/100000], Validation Loss: 552.6437\n",
            "Epoch [6796/100000], Validation Loss: 556.5457\n",
            "Epoch [6797/100000], Validation Loss: 550.1794\n",
            "Epoch [6798/100000], Validation Loss: 547.1776\n",
            "Epoch [6799/100000], Validation Loss: 548.9963\n",
            "Epoch [6800/100000], Validation Loss: 547.6103\n",
            "Epoch [6801/100000], Validation Loss: 548.6045\n",
            "Epoch [6802/100000], Validation Loss: 549.8635\n",
            "Epoch [6803/100000], Validation Loss: 546.3209\n",
            "Epoch [6804/100000], Validation Loss: 548.7726\n",
            "Epoch [6805/100000], Validation Loss: 553.0234\n",
            "Epoch [6806/100000], Validation Loss: 552.0302\n",
            "Epoch [6807/100000], Validation Loss: 553.5122\n",
            "Epoch [6808/100000], Validation Loss: 551.3243\n",
            "Epoch [6809/100000], Validation Loss: 547.1772\n",
            "Epoch [6810/100000], Validation Loss: 549.6680\n",
            "Epoch [6811/100000], Validation Loss: 550.0016\n",
            "Epoch [6812/100000], Validation Loss: 544.8233\n",
            "Epoch [6813/100000], Validation Loss: 556.3367\n",
            "Epoch [6814/100000], Validation Loss: 550.4879\n",
            "Epoch [6815/100000], Validation Loss: 548.7215\n",
            "Epoch [6816/100000], Validation Loss: 548.2406\n",
            "Epoch [6817/100000], Validation Loss: 545.5248\n",
            "Epoch [6818/100000], Validation Loss: 551.7934\n",
            "Epoch [6819/100000], Validation Loss: 554.6177\n",
            "Epoch [6820/100000], Validation Loss: 552.0582\n",
            "Epoch [6821/100000], Validation Loss: 549.8852\n",
            "Epoch [6822/100000], Validation Loss: 547.5282\n",
            "Epoch [6823/100000], Validation Loss: 550.9087\n",
            "Epoch [6824/100000], Validation Loss: 548.1805\n",
            "Epoch [6825/100000], Validation Loss: 552.2619\n",
            "Epoch [6826/100000], Validation Loss: 555.0285\n",
            "Epoch [6827/100000], Validation Loss: 547.6713\n",
            "Epoch [6828/100000], Validation Loss: 547.2388\n",
            "Epoch [6829/100000], Validation Loss: 547.5427\n",
            "Epoch [6830/100000], Validation Loss: 546.4232\n",
            "Epoch [6831/100000], Validation Loss: 545.3367\n",
            "Epoch [6832/100000], Validation Loss: 548.6470\n",
            "Epoch [6833/100000], Validation Loss: 548.0755\n",
            "Epoch [6834/100000], Validation Loss: 549.6625\n",
            "Epoch [6835/100000], Validation Loss: 548.8809\n",
            "Epoch [6836/100000], Validation Loss: 557.1699\n",
            "Epoch [6837/100000], Validation Loss: 547.8412\n",
            "Epoch [6838/100000], Validation Loss: 549.0716\n",
            "Epoch [6839/100000], Validation Loss: 543.3974\n",
            "Epoch [6840/100000], Validation Loss: 550.1048\n",
            "Epoch [6841/100000], Validation Loss: 547.3929\n",
            "Epoch [6842/100000], Validation Loss: 553.5176\n",
            "Epoch [6843/100000], Validation Loss: 547.4310\n",
            "Epoch [6844/100000], Validation Loss: 543.8936\n",
            "Epoch [6845/100000], Validation Loss: 547.6878\n",
            "Epoch [6846/100000], Validation Loss: 546.6657\n",
            "Epoch [6847/100000], Validation Loss: 551.7879\n",
            "Epoch [6848/100000], Validation Loss: 548.3096\n",
            "Epoch [6849/100000], Validation Loss: 547.6214\n",
            "Epoch [6850/100000], Validation Loss: 545.7525\n",
            "Epoch [6851/100000], Validation Loss: 544.0217\n",
            "Epoch [6852/100000], Validation Loss: 551.8995\n",
            "Epoch [6853/100000], Validation Loss: 546.4553\n",
            "Epoch [6854/100000], Validation Loss: 547.6943\n",
            "Epoch [6855/100000], Validation Loss: 548.6396\n",
            "Epoch [6856/100000], Validation Loss: 550.0438\n",
            "Epoch [6857/100000], Validation Loss: 548.1777\n",
            "Epoch [6858/100000], Validation Loss: 549.1506\n",
            "Epoch [6859/100000], Validation Loss: 550.6670\n",
            "Epoch [6860/100000], Validation Loss: 551.2281\n",
            "Epoch [6861/100000], Validation Loss: 550.2796\n",
            "Epoch [6862/100000], Validation Loss: 558.4763\n",
            "Epoch [6863/100000], Validation Loss: 552.7799\n",
            "Epoch [6864/100000], Validation Loss: 551.5580\n",
            "Epoch [6865/100000], Validation Loss: 549.0295\n",
            "Epoch [6866/100000], Validation Loss: 548.2618\n",
            "Epoch [6867/100000], Validation Loss: 544.8160\n",
            "Epoch [6868/100000], Validation Loss: 555.0887\n",
            "Epoch [6869/100000], Validation Loss: 553.0354\n",
            "Epoch [6870/100000], Validation Loss: 548.8729\n",
            "Epoch [6871/100000], Validation Loss: 546.9209\n",
            "Epoch [6872/100000], Validation Loss: 548.7137\n",
            "Epoch [6873/100000], Validation Loss: 550.1777\n",
            "Epoch [6874/100000], Validation Loss: 554.5266\n",
            "Epoch [6875/100000], Validation Loss: 557.2073\n",
            "Epoch [6876/100000], Validation Loss: 554.9309\n",
            "Epoch [6877/100000], Validation Loss: 559.3216\n",
            "Epoch [6878/100000], Validation Loss: 548.6481\n",
            "Epoch [6879/100000], Validation Loss: 553.2579\n",
            "Epoch [6880/100000], Validation Loss: 553.8952\n",
            "Epoch [6881/100000], Validation Loss: 553.9279\n",
            "Epoch [6882/100000], Validation Loss: 548.8890\n",
            "Epoch [6883/100000], Validation Loss: 548.1136\n",
            "Epoch [6884/100000], Validation Loss: 553.4535\n",
            "Epoch [6885/100000], Validation Loss: 548.4716\n",
            "Epoch [6886/100000], Validation Loss: 557.0401\n",
            "Epoch [6887/100000], Validation Loss: 545.5670\n",
            "Epoch [6888/100000], Validation Loss: 548.4120\n",
            "Epoch [6889/100000], Validation Loss: 549.2856\n",
            "Epoch [6890/100000], Validation Loss: 548.5594\n",
            "Epoch [6891/100000], Validation Loss: 552.7888\n",
            "Epoch [6892/100000], Validation Loss: 601.8685\n",
            "Epoch [6893/100000], Validation Loss: 552.0665\n",
            "Epoch [6894/100000], Validation Loss: 553.7124\n",
            "Epoch [6895/100000], Validation Loss: 548.6885\n",
            "Epoch [6896/100000], Validation Loss: 548.4969\n",
            "Epoch [6897/100000], Validation Loss: 549.1072\n",
            "Epoch [6898/100000], Validation Loss: 550.1209\n",
            "Epoch [6899/100000], Validation Loss: 550.3027\n",
            "Epoch [6900/100000], Validation Loss: 564.0504\n",
            "Epoch [6901/100000], Validation Loss: 548.4893\n",
            "Epoch [6902/100000], Validation Loss: 547.8429\n",
            "Epoch [6903/100000], Validation Loss: 546.8309\n",
            "Epoch [6904/100000], Validation Loss: 550.4703\n",
            "Epoch [6905/100000], Validation Loss: 553.2783\n",
            "Epoch [6906/100000], Validation Loss: 544.1720\n",
            "Epoch [6907/100000], Validation Loss: 545.4591\n",
            "Epoch [6908/100000], Validation Loss: 569.3974\n",
            "Epoch [6909/100000], Validation Loss: 548.8063\n",
            "Epoch [6910/100000], Validation Loss: 544.0238\n",
            "Epoch [6911/100000], Validation Loss: 552.7230\n",
            "Epoch [6912/100000], Validation Loss: 547.7358\n",
            "Epoch [6913/100000], Validation Loss: 553.1947\n",
            "Epoch [6914/100000], Validation Loss: 543.4891\n",
            "Epoch [6915/100000], Validation Loss: 546.7763\n",
            "Epoch [6916/100000], Validation Loss: 547.9727\n",
            "Epoch [6917/100000], Validation Loss: 544.4497\n",
            "Epoch [6918/100000], Validation Loss: 551.9275\n",
            "Epoch [6919/100000], Validation Loss: 549.1050\n",
            "Epoch [6920/100000], Validation Loss: 550.9212\n",
            "Epoch [6921/100000], Validation Loss: 544.3771\n",
            "Epoch [6922/100000], Validation Loss: 547.5392\n",
            "Epoch [6923/100000], Validation Loss: 546.8054\n",
            "Epoch [6924/100000], Validation Loss: 547.6711\n",
            "Epoch [6925/100000], Validation Loss: 551.4164\n",
            "Epoch [6926/100000], Validation Loss: 547.9953\n",
            "Epoch [6927/100000], Validation Loss: 549.1441\n",
            "Epoch [6928/100000], Validation Loss: 554.1415\n",
            "Epoch [6929/100000], Validation Loss: 549.1601\n",
            "Epoch [6930/100000], Validation Loss: 550.8464\n",
            "Epoch [6931/100000], Validation Loss: 545.1462\n",
            "Epoch [6932/100000], Validation Loss: 551.9339\n",
            "Epoch [6933/100000], Validation Loss: 549.3680\n",
            "Epoch [6934/100000], Validation Loss: 552.6290\n",
            "Epoch [6935/100000], Validation Loss: 546.8439\n",
            "Epoch [6936/100000], Validation Loss: 548.2319\n",
            "Epoch [6937/100000], Validation Loss: 560.8572\n",
            "Epoch [6938/100000], Validation Loss: 547.3198\n",
            "Epoch [6939/100000], Validation Loss: 550.2968\n",
            "Epoch [6940/100000], Validation Loss: 554.2381\n",
            "Epoch [6941/100000], Validation Loss: 546.5218\n",
            "Epoch [6942/100000], Validation Loss: 545.5768\n",
            "Epoch [6943/100000], Validation Loss: 547.4812\n",
            "Epoch [6944/100000], Validation Loss: 554.1745\n",
            "Epoch [6945/100000], Validation Loss: 551.7601\n",
            "Epoch [6946/100000], Validation Loss: 553.9834\n",
            "Epoch [6947/100000], Validation Loss: 547.2510\n",
            "Epoch [6948/100000], Validation Loss: 547.7545\n",
            "Epoch [6949/100000], Validation Loss: 549.9276\n",
            "Epoch [6950/100000], Validation Loss: 546.6640\n",
            "Epoch [6951/100000], Validation Loss: 546.1980\n",
            "Epoch [6952/100000], Validation Loss: 545.6394\n",
            "Epoch [6953/100000], Validation Loss: 551.9222\n",
            "Epoch [6954/100000], Validation Loss: 553.0076\n",
            "Epoch [6955/100000], Validation Loss: 551.4130\n",
            "Epoch [6956/100000], Validation Loss: 551.1369\n",
            "Epoch [6957/100000], Validation Loss: 552.9865\n",
            "Epoch [6958/100000], Validation Loss: 556.0997\n",
            "Epoch [6959/100000], Validation Loss: 549.0291\n",
            "Epoch [6960/100000], Validation Loss: 543.1655\n",
            "Epoch [6961/100000], Validation Loss: 551.2029\n",
            "Epoch [6962/100000], Validation Loss: 545.5718\n",
            "Epoch [6963/100000], Validation Loss: 548.1895\n",
            "Epoch [6964/100000], Validation Loss: 547.6551\n",
            "Epoch [6965/100000], Validation Loss: 546.9154\n",
            "Epoch [6966/100000], Validation Loss: 549.5626\n",
            "Epoch [6967/100000], Validation Loss: 548.2607\n",
            "Epoch [6968/100000], Validation Loss: 549.4273\n",
            "Epoch [6969/100000], Validation Loss: 547.5322\n",
            "Epoch [6970/100000], Validation Loss: 547.4396\n",
            "Epoch [6971/100000], Validation Loss: 542.7663\n",
            "Epoch [6972/100000], Validation Loss: 551.3079\n",
            "Epoch [6973/100000], Validation Loss: 554.5415\n",
            "Epoch [6974/100000], Validation Loss: 546.0829\n",
            "Epoch [6975/100000], Validation Loss: 547.5133\n",
            "Epoch [6976/100000], Validation Loss: 543.3891\n",
            "Epoch [6977/100000], Validation Loss: 551.3173\n",
            "Epoch [6978/100000], Validation Loss: 550.2607\n",
            "Epoch [6979/100000], Validation Loss: 552.5692\n",
            "Epoch [6980/100000], Validation Loss: 552.0547\n",
            "Epoch [6981/100000], Validation Loss: 547.9138\n",
            "Epoch [6982/100000], Validation Loss: 544.5257\n",
            "Epoch [6983/100000], Validation Loss: 551.7608\n",
            "Epoch [6984/100000], Validation Loss: 546.6813\n",
            "Epoch [6985/100000], Validation Loss: 546.0589\n",
            "Epoch [6986/100000], Validation Loss: 545.5852\n",
            "Epoch [6987/100000], Validation Loss: 548.8695\n",
            "Epoch [6988/100000], Validation Loss: 548.5568\n",
            "Epoch [6989/100000], Validation Loss: 544.2649\n",
            "Epoch [6990/100000], Validation Loss: 542.0806\n",
            "Epoch [6991/100000], Validation Loss: 546.0043\n",
            "Epoch [6992/100000], Validation Loss: 555.8116\n",
            "Epoch [6993/100000], Validation Loss: 547.5370\n",
            "Epoch [6994/100000], Validation Loss: 547.4160\n",
            "Epoch [6995/100000], Validation Loss: 545.2752\n",
            "Epoch [6996/100000], Validation Loss: 547.7180\n",
            "Epoch [6997/100000], Validation Loss: 550.1810\n",
            "Epoch [6998/100000], Validation Loss: 548.9890\n",
            "Epoch [6999/100000], Validation Loss: 574.5919\n",
            "Epoch [7000/100000], Validation Loss: 543.9750\n",
            "Epoch [7001/100000], Validation Loss: 551.9840\n",
            "Epoch [7002/100000], Validation Loss: 575.4518\n",
            "Epoch [7003/100000], Validation Loss: 546.0121\n",
            "Epoch [7004/100000], Validation Loss: 556.1887\n",
            "Epoch [7005/100000], Validation Loss: 547.4091\n",
            "Epoch [7006/100000], Validation Loss: 554.8698\n",
            "Epoch [7007/100000], Validation Loss: 562.3699\n",
            "Epoch [7008/100000], Validation Loss: 546.5177\n",
            "Epoch [7009/100000], Validation Loss: 552.5143\n",
            "Epoch [7010/100000], Validation Loss: 549.5960\n",
            "Epoch [7011/100000], Validation Loss: 567.8021\n",
            "Epoch [7012/100000], Validation Loss: 566.9949\n",
            "Epoch [7013/100000], Validation Loss: 550.3111\n",
            "Epoch [7014/100000], Validation Loss: 554.5659\n",
            "Epoch [7015/100000], Validation Loss: 552.5709\n",
            "Epoch [7016/100000], Validation Loss: 547.9971\n",
            "Epoch [7017/100000], Validation Loss: 548.5300\n",
            "Epoch [7018/100000], Validation Loss: 550.7045\n",
            "Epoch [7019/100000], Validation Loss: 546.1692\n",
            "Epoch [7020/100000], Validation Loss: 548.2150\n",
            "Epoch [7021/100000], Validation Loss: 546.0687\n",
            "Epoch [7022/100000], Validation Loss: 548.6849\n",
            "Epoch [7023/100000], Validation Loss: 544.9119\n",
            "Epoch [7024/100000], Validation Loss: 548.1469\n",
            "Epoch [7025/100000], Validation Loss: 544.3131\n",
            "Epoch [7026/100000], Validation Loss: 552.3744\n",
            "Epoch [7027/100000], Validation Loss: 549.5925\n",
            "Epoch [7028/100000], Validation Loss: 550.2853\n",
            "Epoch [7029/100000], Validation Loss: 548.6249\n",
            "Epoch [7030/100000], Validation Loss: 548.9922\n",
            "Epoch [7031/100000], Validation Loss: 546.1549\n",
            "Epoch [7032/100000], Validation Loss: 546.7801\n",
            "Epoch [7033/100000], Validation Loss: 544.0505\n",
            "Epoch [7034/100000], Validation Loss: 547.8039\n",
            "Epoch [7035/100000], Validation Loss: 547.1836\n",
            "Epoch [7036/100000], Validation Loss: 548.1174\n",
            "Epoch [7037/100000], Validation Loss: 553.7423\n",
            "Epoch [7038/100000], Validation Loss: 549.8536\n",
            "Epoch [7039/100000], Validation Loss: 546.3467\n",
            "Epoch [7040/100000], Validation Loss: 545.4756\n",
            "Epoch [7041/100000], Validation Loss: 547.2480\n",
            "Epoch [7042/100000], Validation Loss: 549.6644\n",
            "Epoch [7043/100000], Validation Loss: 548.4335\n",
            "Epoch [7044/100000], Validation Loss: 549.5383\n",
            "Epoch [7045/100000], Validation Loss: 547.2808\n",
            "Epoch [7046/100000], Validation Loss: 547.9256\n",
            "Epoch [7047/100000], Validation Loss: 546.7929\n",
            "Epoch [7048/100000], Validation Loss: 543.3837\n",
            "Epoch [7049/100000], Validation Loss: 543.6686\n",
            "Epoch [7050/100000], Validation Loss: 546.7965\n",
            "Epoch [7051/100000], Validation Loss: 547.9307\n",
            "Epoch [7052/100000], Validation Loss: 543.8992\n",
            "Epoch [7053/100000], Validation Loss: 546.6144\n",
            "Epoch [7054/100000], Validation Loss: 547.3977\n",
            "Epoch [7055/100000], Validation Loss: 547.6426\n",
            "Epoch [7056/100000], Validation Loss: 552.3096\n",
            "Epoch [7057/100000], Validation Loss: 544.5061\n",
            "Epoch [7058/100000], Validation Loss: 555.7532\n",
            "Epoch [7059/100000], Validation Loss: 546.2192\n",
            "Epoch [7060/100000], Validation Loss: 553.5575\n",
            "Epoch [7061/100000], Validation Loss: 553.9069\n",
            "Epoch [7062/100000], Validation Loss: 546.9690\n",
            "Epoch [7063/100000], Validation Loss: 546.7612\n",
            "Epoch [7064/100000], Validation Loss: 547.1113\n",
            "Epoch [7065/100000], Validation Loss: 549.8347\n",
            "Epoch [7066/100000], Validation Loss: 553.1786\n",
            "Epoch [7067/100000], Validation Loss: 547.7473\n",
            "Epoch [7068/100000], Validation Loss: 545.3500\n",
            "Epoch [7069/100000], Validation Loss: 549.2321\n",
            "Epoch [7070/100000], Validation Loss: 545.7697\n",
            "Epoch [7071/100000], Validation Loss: 558.4437\n",
            "Epoch [7072/100000], Validation Loss: 551.2353\n",
            "Epoch [7073/100000], Validation Loss: 549.8454\n",
            "Epoch [7074/100000], Validation Loss: 554.0422\n",
            "Epoch [7075/100000], Validation Loss: 548.2569\n",
            "Epoch [7076/100000], Validation Loss: 552.5164\n",
            "Epoch [7077/100000], Validation Loss: 552.6031\n",
            "Epoch [7078/100000], Validation Loss: 547.3786\n",
            "Epoch [7079/100000], Validation Loss: 544.8452\n",
            "Epoch [7080/100000], Validation Loss: 550.5588\n",
            "Epoch [7081/100000], Validation Loss: 544.9766\n",
            "Epoch [7082/100000], Validation Loss: 552.6893\n",
            "Epoch [7083/100000], Validation Loss: 545.1139\n",
            "Epoch [7084/100000], Validation Loss: 554.9579\n",
            "Epoch [7085/100000], Validation Loss: 547.3714\n",
            "Epoch [7086/100000], Validation Loss: 547.6133\n",
            "Epoch [7087/100000], Validation Loss: 553.3688\n",
            "Epoch [7088/100000], Validation Loss: 558.4626\n",
            "Epoch [7089/100000], Validation Loss: 548.2735\n",
            "Epoch [7090/100000], Validation Loss: 550.7483\n",
            "Epoch [7091/100000], Validation Loss: 545.7935\n",
            "Epoch [7092/100000], Validation Loss: 547.0753\n",
            "Epoch [7093/100000], Validation Loss: 553.0790\n",
            "Epoch [7094/100000], Validation Loss: 544.9914\n",
            "Epoch [7095/100000], Validation Loss: 553.4631\n",
            "Epoch [7096/100000], Validation Loss: 546.5189\n",
            "Epoch [7097/100000], Validation Loss: 547.9917\n",
            "Epoch [7098/100000], Validation Loss: 559.5695\n",
            "Epoch [7099/100000], Validation Loss: 551.0668\n",
            "Epoch [7100/100000], Validation Loss: 549.2051\n",
            "Epoch [7101/100000], Validation Loss: 547.9270\n",
            "Epoch [7102/100000], Validation Loss: 547.4615\n",
            "Epoch [7103/100000], Validation Loss: 547.4370\n",
            "Epoch [7104/100000], Validation Loss: 544.4735\n",
            "Epoch [7105/100000], Validation Loss: 546.9762\n",
            "Epoch [7106/100000], Validation Loss: 549.1312\n",
            "Epoch [7107/100000], Validation Loss: 547.5185\n",
            "Epoch [7108/100000], Validation Loss: 546.2760\n",
            "Epoch [7109/100000], Validation Loss: 550.6910\n",
            "Epoch [7110/100000], Validation Loss: 547.4217\n",
            "Epoch [7111/100000], Validation Loss: 546.8402\n",
            "Epoch [7112/100000], Validation Loss: 544.4752\n",
            "Epoch [7113/100000], Validation Loss: 547.7572\n",
            "Epoch [7114/100000], Validation Loss: 548.5837\n",
            "Epoch [7115/100000], Validation Loss: 547.9954\n",
            "Epoch [7116/100000], Validation Loss: 546.5662\n",
            "Epoch [7117/100000], Validation Loss: 545.4941\n",
            "Epoch [7118/100000], Validation Loss: 548.6400\n",
            "Epoch [7119/100000], Validation Loss: 544.3822\n",
            "Epoch [7120/100000], Validation Loss: 548.7632\n",
            "Epoch [7121/100000], Validation Loss: 557.4493\n",
            "Epoch [7122/100000], Validation Loss: 550.0491\n",
            "Epoch [7123/100000], Validation Loss: 549.2310\n",
            "Epoch [7124/100000], Validation Loss: 554.6380\n",
            "Epoch [7125/100000], Validation Loss: 549.0513\n",
            "Epoch [7126/100000], Validation Loss: 549.3404\n",
            "Epoch [7127/100000], Validation Loss: 550.8762\n",
            "Epoch [7128/100000], Validation Loss: 548.2338\n",
            "Epoch [7129/100000], Validation Loss: 551.3374\n",
            "Epoch [7130/100000], Validation Loss: 548.5752\n",
            "Epoch [7131/100000], Validation Loss: 549.8585\n",
            "Epoch [7132/100000], Validation Loss: 548.9197\n",
            "Epoch [7133/100000], Validation Loss: 548.5816\n",
            "Epoch [7134/100000], Validation Loss: 549.4793\n",
            "Epoch [7135/100000], Validation Loss: 548.4349\n",
            "Epoch [7136/100000], Validation Loss: 558.8663\n",
            "Epoch [7137/100000], Validation Loss: 549.5877\n",
            "Epoch [7138/100000], Validation Loss: 556.7901\n",
            "Epoch [7139/100000], Validation Loss: 544.4644\n",
            "Epoch [7140/100000], Validation Loss: 550.9248\n",
            "Epoch [7141/100000], Validation Loss: 546.4634\n",
            "Epoch [7142/100000], Validation Loss: 549.6308\n",
            "Epoch [7143/100000], Validation Loss: 547.1946\n",
            "Epoch [7144/100000], Validation Loss: 547.4682\n",
            "Epoch [7145/100000], Validation Loss: 552.1174\n",
            "Epoch [7146/100000], Validation Loss: 543.0380\n",
            "Epoch [7147/100000], Validation Loss: 544.0604\n",
            "Epoch [7148/100000], Validation Loss: 546.7863\n",
            "Epoch [7149/100000], Validation Loss: 549.5383\n",
            "Epoch [7150/100000], Validation Loss: 553.2265\n",
            "Epoch [7151/100000], Validation Loss: 545.4762\n",
            "Epoch [7152/100000], Validation Loss: 548.5504\n",
            "Epoch [7153/100000], Validation Loss: 546.9372\n",
            "Epoch [7154/100000], Validation Loss: 547.6078\n",
            "Epoch [7155/100000], Validation Loss: 549.7247\n",
            "Epoch [7156/100000], Validation Loss: 547.6749\n",
            "Epoch [7157/100000], Validation Loss: 553.7891\n",
            "Epoch [7158/100000], Validation Loss: 546.5092\n",
            "Epoch [7159/100000], Validation Loss: 554.8388\n",
            "Epoch [7160/100000], Validation Loss: 549.5542\n",
            "Epoch [7161/100000], Validation Loss: 548.9020\n",
            "Epoch [7162/100000], Validation Loss: 554.7013\n",
            "Epoch [7163/100000], Validation Loss: 552.7403\n",
            "Epoch [7164/100000], Validation Loss: 545.6391\n",
            "Epoch [7165/100000], Validation Loss: 551.7889\n",
            "Epoch [7166/100000], Validation Loss: 548.8361\n",
            "Epoch [7167/100000], Validation Loss: 547.3983\n",
            "Epoch [7168/100000], Validation Loss: 548.9603\n",
            "Epoch [7169/100000], Validation Loss: 549.3502\n",
            "Epoch [7170/100000], Validation Loss: 551.9752\n",
            "Epoch [7171/100000], Validation Loss: 558.9541\n",
            "Epoch [7172/100000], Validation Loss: 548.6993\n",
            "Epoch [7173/100000], Validation Loss: 548.7312\n",
            "Epoch [7174/100000], Validation Loss: 549.3752\n",
            "Epoch [7175/100000], Validation Loss: 545.4368\n",
            "Epoch [7176/100000], Validation Loss: 551.4064\n",
            "Epoch [7177/100000], Validation Loss: 546.2549\n",
            "Epoch [7178/100000], Validation Loss: 548.5504\n",
            "Epoch [7179/100000], Validation Loss: 549.3604\n",
            "Epoch [7180/100000], Validation Loss: 547.0014\n",
            "Epoch [7181/100000], Validation Loss: 551.4907\n",
            "Epoch [7182/100000], Validation Loss: 548.4853\n",
            "Epoch [7183/100000], Validation Loss: 552.3342\n",
            "Epoch [7184/100000], Validation Loss: 563.4697\n",
            "Epoch [7185/100000], Validation Loss: 545.2932\n",
            "Epoch [7186/100000], Validation Loss: 552.8518\n",
            "Epoch [7187/100000], Validation Loss: 551.1368\n",
            "Epoch [7188/100000], Validation Loss: 553.7350\n",
            "Epoch [7189/100000], Validation Loss: 556.0116\n",
            "Epoch [7190/100000], Validation Loss: 542.4876\n",
            "Epoch [7191/100000], Validation Loss: 551.1149\n",
            "Epoch [7192/100000], Validation Loss: 548.7883\n",
            "Epoch [7193/100000], Validation Loss: 545.3923\n",
            "Epoch [7194/100000], Validation Loss: 544.9631\n",
            "Epoch [7195/100000], Validation Loss: 554.2985\n",
            "Epoch [7196/100000], Validation Loss: 549.2928\n",
            "Epoch [7197/100000], Validation Loss: 547.5218\n",
            "Epoch [7198/100000], Validation Loss: 552.4480\n",
            "Epoch [7199/100000], Validation Loss: 547.6084\n",
            "Epoch [7200/100000], Validation Loss: 546.8638\n",
            "Epoch [7201/100000], Validation Loss: 549.1426\n",
            "Epoch [7202/100000], Validation Loss: 547.2522\n",
            "Epoch [7203/100000], Validation Loss: 546.8224\n",
            "Epoch [7204/100000], Validation Loss: 544.6351\n",
            "Epoch [7205/100000], Validation Loss: 546.5669\n",
            "Epoch [7206/100000], Validation Loss: 556.8949\n",
            "Epoch [7207/100000], Validation Loss: 547.1622\n",
            "Epoch [7208/100000], Validation Loss: 557.7126\n",
            "Epoch [7209/100000], Validation Loss: 551.1173\n",
            "Epoch [7210/100000], Validation Loss: 547.0012\n",
            "Epoch [7211/100000], Validation Loss: 551.4284\n",
            "Epoch [7212/100000], Validation Loss: 551.6141\n",
            "Epoch [7213/100000], Validation Loss: 599.6042\n",
            "Epoch [7214/100000], Validation Loss: 550.0981\n",
            "Epoch [7215/100000], Validation Loss: 548.5664\n",
            "Epoch [7216/100000], Validation Loss: 557.3893\n",
            "Epoch [7217/100000], Validation Loss: 551.9745\n",
            "Epoch [7218/100000], Validation Loss: 548.4443\n",
            "Epoch [7219/100000], Validation Loss: 550.5629\n",
            "Epoch [7220/100000], Validation Loss: 548.8932\n",
            "Epoch [7221/100000], Validation Loss: 550.6899\n",
            "Epoch [7222/100000], Validation Loss: 549.4801\n",
            "Epoch [7223/100000], Validation Loss: 545.6732\n",
            "Epoch [7224/100000], Validation Loss: 549.2379\n",
            "Epoch [7225/100000], Validation Loss: 549.8899\n",
            "Epoch [7226/100000], Validation Loss: 545.6854\n",
            "Epoch [7227/100000], Validation Loss: 546.9804\n",
            "Epoch [7228/100000], Validation Loss: 546.4939\n",
            "Epoch [7229/100000], Validation Loss: 555.6731\n",
            "Epoch [7230/100000], Validation Loss: 555.2196\n",
            "Epoch [7231/100000], Validation Loss: 547.6919\n",
            "Epoch [7232/100000], Validation Loss: 547.8849\n",
            "Epoch [7233/100000], Validation Loss: 543.4338\n",
            "Epoch [7234/100000], Validation Loss: 551.9387\n",
            "Epoch [7235/100000], Validation Loss: 558.2614\n",
            "Epoch [7236/100000], Validation Loss: 549.1659\n",
            "Epoch [7237/100000], Validation Loss: 550.1745\n",
            "Epoch [7238/100000], Validation Loss: 552.6661\n",
            "Epoch [7239/100000], Validation Loss: 550.8465\n",
            "Epoch [7240/100000], Validation Loss: 554.8463\n",
            "Epoch [7241/100000], Validation Loss: 551.0362\n",
            "Epoch [7242/100000], Validation Loss: 548.3798\n",
            "Epoch [7243/100000], Validation Loss: 548.8176\n",
            "Epoch [7244/100000], Validation Loss: 549.0080\n",
            "Epoch [7245/100000], Validation Loss: 543.7366\n",
            "Epoch [7246/100000], Validation Loss: 551.2896\n",
            "Epoch [7247/100000], Validation Loss: 547.9910\n",
            "Epoch [7248/100000], Validation Loss: 547.7157\n",
            "Epoch [7249/100000], Validation Loss: 547.9053\n",
            "Epoch [7250/100000], Validation Loss: 551.5113\n",
            "Epoch [7251/100000], Validation Loss: 550.5460\n",
            "Epoch [7252/100000], Validation Loss: 548.1902\n",
            "Epoch [7253/100000], Validation Loss: 550.7594\n",
            "Epoch [7254/100000], Validation Loss: 546.5438\n",
            "Epoch [7255/100000], Validation Loss: 548.6366\n",
            "Epoch [7256/100000], Validation Loss: 553.3172\n",
            "Epoch [7257/100000], Validation Loss: 549.4230\n",
            "Epoch [7258/100000], Validation Loss: 549.2030\n",
            "Epoch [7259/100000], Validation Loss: 551.7035\n",
            "Epoch [7260/100000], Validation Loss: 548.5002\n",
            "Epoch [7261/100000], Validation Loss: 549.1206\n",
            "Epoch [7262/100000], Validation Loss: 550.5331\n",
            "Epoch [7263/100000], Validation Loss: 549.9191\n",
            "Epoch [7264/100000], Validation Loss: 563.6597\n",
            "Epoch [7265/100000], Validation Loss: 548.5851\n",
            "Epoch [7266/100000], Validation Loss: 547.4573\n",
            "Epoch [7267/100000], Validation Loss: 547.4663\n",
            "Epoch [7268/100000], Validation Loss: 553.7840\n",
            "Epoch [7269/100000], Validation Loss: 548.6500\n",
            "Epoch [7270/100000], Validation Loss: 548.4998\n",
            "Epoch [7271/100000], Validation Loss: 547.3770\n",
            "Epoch [7272/100000], Validation Loss: 554.2510\n",
            "Epoch [7273/100000], Validation Loss: 551.1180\n",
            "Epoch [7274/100000], Validation Loss: 547.3881\n",
            "Epoch [7275/100000], Validation Loss: 545.7202\n",
            "Epoch [7276/100000], Validation Loss: 551.1234\n",
            "Epoch [7277/100000], Validation Loss: 553.4253\n",
            "Epoch [7278/100000], Validation Loss: 551.9529\n",
            "Epoch [7279/100000], Validation Loss: 551.3499\n",
            "Epoch [7280/100000], Validation Loss: 546.9777\n",
            "Epoch [7281/100000], Validation Loss: 550.5307\n",
            "Epoch [7282/100000], Validation Loss: 550.6249\n",
            "Epoch [7283/100000], Validation Loss: 545.3641\n",
            "Epoch [7284/100000], Validation Loss: 545.4378\n",
            "Epoch [7285/100000], Validation Loss: 544.7217\n",
            "Epoch [7286/100000], Validation Loss: 547.3347\n",
            "Epoch [7287/100000], Validation Loss: 550.4028\n",
            "Epoch [7288/100000], Validation Loss: 550.1839\n",
            "Epoch [7289/100000], Validation Loss: 545.4406\n",
            "Epoch [7290/100000], Validation Loss: 551.4999\n",
            "Epoch [7291/100000], Validation Loss: 545.1878\n",
            "Epoch [7292/100000], Validation Loss: 548.7968\n",
            "Epoch [7293/100000], Validation Loss: 549.9079\n",
            "Epoch [7294/100000], Validation Loss: 548.7993\n",
            "Epoch [7295/100000], Validation Loss: 547.1622\n",
            "Epoch [7296/100000], Validation Loss: 546.6788\n",
            "Epoch [7297/100000], Validation Loss: 548.2551\n",
            "Epoch [7298/100000], Validation Loss: 551.1380\n",
            "Epoch [7299/100000], Validation Loss: 545.1581\n",
            "Epoch [7300/100000], Validation Loss: 553.3664\n",
            "Epoch [7301/100000], Validation Loss: 552.8001\n",
            "Epoch [7302/100000], Validation Loss: 546.4175\n",
            "Epoch [7303/100000], Validation Loss: 547.0930\n",
            "Epoch [7304/100000], Validation Loss: 546.5167\n",
            "Epoch [7305/100000], Validation Loss: 548.1168\n",
            "Epoch [7306/100000], Validation Loss: 547.6441\n",
            "Epoch [7307/100000], Validation Loss: 548.5118\n",
            "Epoch [7308/100000], Validation Loss: 554.3848\n",
            "Epoch [7309/100000], Validation Loss: 548.4772\n",
            "Epoch [7310/100000], Validation Loss: 553.2350\n",
            "Epoch [7311/100000], Validation Loss: 549.3282\n",
            "Epoch [7312/100000], Validation Loss: 545.3642\n",
            "Epoch [7313/100000], Validation Loss: 549.1951\n",
            "Epoch [7314/100000], Validation Loss: 549.3807\n",
            "Epoch [7315/100000], Validation Loss: 546.8158\n",
            "Epoch [7316/100000], Validation Loss: 549.9467\n",
            "Epoch [7317/100000], Validation Loss: 550.4517\n",
            "Epoch [7318/100000], Validation Loss: 546.5827\n",
            "Epoch [7319/100000], Validation Loss: 547.4915\n",
            "Epoch [7320/100000], Validation Loss: 546.1966\n",
            "Epoch [7321/100000], Validation Loss: 550.1073\n",
            "Epoch [7322/100000], Validation Loss: 552.9955\n",
            "Epoch [7323/100000], Validation Loss: 549.7899\n",
            "Epoch [7324/100000], Validation Loss: 549.2381\n",
            "Epoch [7325/100000], Validation Loss: 546.6315\n",
            "Epoch [7326/100000], Validation Loss: 549.3475\n",
            "Epoch [7327/100000], Validation Loss: 544.6278\n",
            "Epoch [7328/100000], Validation Loss: 548.3184\n",
            "Epoch [7329/100000], Validation Loss: 551.1681\n",
            "Epoch [7330/100000], Validation Loss: 547.9301\n",
            "Epoch [7331/100000], Validation Loss: 547.6083\n",
            "Epoch [7332/100000], Validation Loss: 548.2517\n",
            "Epoch [7333/100000], Validation Loss: 552.0627\n",
            "Epoch [7334/100000], Validation Loss: 549.9330\n",
            "Epoch [7335/100000], Validation Loss: 550.3690\n",
            "Epoch [7336/100000], Validation Loss: 555.1069\n",
            "Epoch [7337/100000], Validation Loss: 553.5778\n",
            "Epoch [7338/100000], Validation Loss: 559.7468\n",
            "Epoch [7339/100000], Validation Loss: 552.0946\n",
            "Epoch [7340/100000], Validation Loss: 548.0161\n",
            "Epoch [7341/100000], Validation Loss: 546.9267\n",
            "Epoch [7342/100000], Validation Loss: 546.5580\n",
            "Epoch [7343/100000], Validation Loss: 549.8813\n",
            "Epoch [7344/100000], Validation Loss: 546.8082\n",
            "Epoch [7345/100000], Validation Loss: 551.0962\n",
            "Epoch [7346/100000], Validation Loss: 545.9899\n",
            "Epoch [7347/100000], Validation Loss: 546.5841\n",
            "Epoch [7348/100000], Validation Loss: 544.3970\n",
            "Epoch [7349/100000], Validation Loss: 556.1162\n",
            "Epoch [7350/100000], Validation Loss: 551.7864\n",
            "Epoch [7351/100000], Validation Loss: 549.6242\n",
            "Epoch [7352/100000], Validation Loss: 553.6335\n",
            "Epoch [7353/100000], Validation Loss: 550.2302\n",
            "Epoch [7354/100000], Validation Loss: 544.9719\n",
            "Epoch [7355/100000], Validation Loss: 549.4746\n",
            "Epoch [7356/100000], Validation Loss: 549.3978\n",
            "Epoch [7357/100000], Validation Loss: 545.2501\n",
            "Epoch [7358/100000], Validation Loss: 548.0756\n",
            "Epoch [7359/100000], Validation Loss: 548.9748\n",
            "Epoch [7360/100000], Validation Loss: 553.9069\n",
            "Epoch [7361/100000], Validation Loss: 547.8983\n",
            "Epoch [7362/100000], Validation Loss: 550.8356\n",
            "Epoch [7363/100000], Validation Loss: 550.1217\n",
            "Epoch [7364/100000], Validation Loss: 551.5246\n",
            "Epoch [7365/100000], Validation Loss: 546.5509\n",
            "Epoch [7366/100000], Validation Loss: 552.6768\n",
            "Epoch [7367/100000], Validation Loss: 546.8158\n",
            "Epoch [7368/100000], Validation Loss: 545.8244\n",
            "Epoch [7369/100000], Validation Loss: 549.8228\n",
            "Epoch [7370/100000], Validation Loss: 549.0743\n",
            "Epoch [7371/100000], Validation Loss: 549.8238\n",
            "Epoch [7372/100000], Validation Loss: 552.6960\n",
            "Epoch [7373/100000], Validation Loss: 546.9401\n",
            "Epoch [7374/100000], Validation Loss: 561.7753\n",
            "Epoch [7375/100000], Validation Loss: 544.2340\n",
            "Epoch [7376/100000], Validation Loss: 553.7713\n",
            "Epoch [7377/100000], Validation Loss: 555.4141\n",
            "Epoch [7378/100000], Validation Loss: 552.9859\n",
            "Epoch [7379/100000], Validation Loss: 547.0257\n",
            "Epoch [7380/100000], Validation Loss: 545.5230\n",
            "Epoch [7381/100000], Validation Loss: 544.2551\n",
            "Epoch [7382/100000], Validation Loss: 547.0810\n",
            "Epoch [7383/100000], Validation Loss: 546.7981\n",
            "Epoch [7384/100000], Validation Loss: 544.7101\n",
            "Epoch [7385/100000], Validation Loss: 543.7406\n",
            "Epoch [7386/100000], Validation Loss: 546.9621\n",
            "Epoch [7387/100000], Validation Loss: 549.3711\n",
            "Epoch [7388/100000], Validation Loss: 551.0286\n",
            "Epoch [7389/100000], Validation Loss: 548.7001\n",
            "Epoch [7390/100000], Validation Loss: 550.9530\n",
            "Epoch [7391/100000], Validation Loss: 549.0823\n",
            "Epoch [7392/100000], Validation Loss: 551.5048\n",
            "Epoch [7393/100000], Validation Loss: 549.9058\n",
            "Epoch [7394/100000], Validation Loss: 545.7404\n",
            "Epoch [7395/100000], Validation Loss: 547.8584\n",
            "Epoch [7396/100000], Validation Loss: 544.3357\n",
            "Epoch [7397/100000], Validation Loss: 548.4287\n",
            "Epoch [7398/100000], Validation Loss: 546.6383\n",
            "Epoch [7399/100000], Validation Loss: 551.4732\n",
            "Epoch [7400/100000], Validation Loss: 543.4065\n",
            "Epoch [7401/100000], Validation Loss: 549.2337\n",
            "Epoch [7402/100000], Validation Loss: 549.8014\n",
            "Epoch [7403/100000], Validation Loss: 547.3891\n",
            "Epoch [7404/100000], Validation Loss: 547.9080\n",
            "Epoch [7405/100000], Validation Loss: 548.3413\n",
            "Epoch [7406/100000], Validation Loss: 548.7481\n",
            "Epoch [7407/100000], Validation Loss: 546.9715\n",
            "Epoch [7408/100000], Validation Loss: 547.1060\n",
            "Epoch [7409/100000], Validation Loss: 546.0870\n",
            "Epoch [7410/100000], Validation Loss: 550.3423\n",
            "Epoch [7411/100000], Validation Loss: 549.3394\n",
            "Epoch [7412/100000], Validation Loss: 545.6876\n",
            "Epoch [7413/100000], Validation Loss: 549.8856\n",
            "Epoch [7414/100000], Validation Loss: 551.4184\n",
            "Epoch [7415/100000], Validation Loss: 554.9485\n",
            "Epoch [7416/100000], Validation Loss: 545.1566\n",
            "Epoch [7417/100000], Validation Loss: 546.6034\n",
            "Epoch [7418/100000], Validation Loss: 551.0776\n",
            "Epoch [7419/100000], Validation Loss: 553.1635\n",
            "Epoch [7420/100000], Validation Loss: 550.5012\n",
            "Epoch [7421/100000], Validation Loss: 542.4826\n",
            "Epoch [7422/100000], Validation Loss: 547.1950\n",
            "Epoch [7423/100000], Validation Loss: 546.6875\n",
            "Epoch [7424/100000], Validation Loss: 549.7718\n",
            "Epoch [7425/100000], Validation Loss: 547.5943\n",
            "Epoch [7426/100000], Validation Loss: 553.4096\n",
            "Epoch [7427/100000], Validation Loss: 549.5002\n",
            "Epoch [7428/100000], Validation Loss: 549.1638\n",
            "Epoch [7429/100000], Validation Loss: 545.2494\n",
            "Epoch [7430/100000], Validation Loss: 550.9017\n",
            "Epoch [7431/100000], Validation Loss: 549.6170\n",
            "Epoch [7432/100000], Validation Loss: 550.4167\n",
            "Epoch [7433/100000], Validation Loss: 546.6810\n",
            "Epoch [7434/100000], Validation Loss: 553.1581\n",
            "Epoch [7435/100000], Validation Loss: 554.1125\n",
            "Epoch [7436/100000], Validation Loss: 549.3119\n",
            "Epoch [7437/100000], Validation Loss: 556.2788\n",
            "Epoch [7438/100000], Validation Loss: 552.0759\n",
            "Epoch [7439/100000], Validation Loss: 549.6579\n",
            "Epoch [7440/100000], Validation Loss: 549.1534\n",
            "Epoch [7441/100000], Validation Loss: 547.7441\n",
            "Epoch [7442/100000], Validation Loss: 549.1612\n",
            "Epoch [7443/100000], Validation Loss: 549.5508\n",
            "Epoch [7444/100000], Validation Loss: 547.4693\n",
            "Epoch [7445/100000], Validation Loss: 545.7383\n",
            "Epoch [7446/100000], Validation Loss: 548.1758\n",
            "Epoch [7447/100000], Validation Loss: 550.3924\n",
            "Epoch [7448/100000], Validation Loss: 550.4727\n",
            "Epoch [7449/100000], Validation Loss: 543.5543\n",
            "Epoch [7450/100000], Validation Loss: 544.2810\n",
            "Epoch [7451/100000], Validation Loss: 554.0658\n",
            "Epoch [7452/100000], Validation Loss: 549.1507\n",
            "Epoch [7453/100000], Validation Loss: 548.2490\n",
            "Epoch [7454/100000], Validation Loss: 545.4178\n",
            "Epoch [7455/100000], Validation Loss: 545.0063\n",
            "Epoch [7456/100000], Validation Loss: 552.4289\n",
            "Epoch [7457/100000], Validation Loss: 546.2121\n",
            "Epoch [7458/100000], Validation Loss: 551.8257\n",
            "Epoch [7459/100000], Validation Loss: 546.9707\n",
            "Epoch [7460/100000], Validation Loss: 546.0724\n",
            "Epoch [7461/100000], Validation Loss: 546.1107\n",
            "Epoch [7462/100000], Validation Loss: 547.8900\n",
            "Epoch [7463/100000], Validation Loss: 551.6581\n",
            "Epoch [7464/100000], Validation Loss: 552.2288\n",
            "Epoch [7465/100000], Validation Loss: 549.3795\n",
            "Epoch [7466/100000], Validation Loss: 548.8772\n",
            "Epoch [7467/100000], Validation Loss: 553.6436\n",
            "Epoch [7468/100000], Validation Loss: 546.0952\n",
            "Epoch [7469/100000], Validation Loss: 552.4626\n",
            "Epoch [7470/100000], Validation Loss: 551.3017\n",
            "Epoch [7471/100000], Validation Loss: 547.9505\n",
            "Epoch [7472/100000], Validation Loss: 552.6596\n",
            "Epoch [7473/100000], Validation Loss: 547.0616\n",
            "Epoch [7474/100000], Validation Loss: 553.6986\n",
            "Epoch [7475/100000], Validation Loss: 551.8918\n",
            "Epoch [7476/100000], Validation Loss: 545.1927\n",
            "Epoch [7477/100000], Validation Loss: 552.8104\n",
            "Epoch [7478/100000], Validation Loss: 550.9806\n",
            "Epoch [7479/100000], Validation Loss: 543.7116\n",
            "Epoch [7480/100000], Validation Loss: 547.1972\n",
            "Epoch [7481/100000], Validation Loss: 546.8957\n",
            "Epoch [7482/100000], Validation Loss: 550.3342\n",
            "Epoch [7483/100000], Validation Loss: 553.2718\n",
            "Epoch [7484/100000], Validation Loss: 549.2872\n",
            "Epoch [7485/100000], Validation Loss: 549.5457\n",
            "Epoch [7486/100000], Validation Loss: 548.5435\n",
            "Epoch [7487/100000], Validation Loss: 554.1833\n",
            "Epoch [7488/100000], Validation Loss: 547.2355\n",
            "Epoch [7489/100000], Validation Loss: 553.7819\n",
            "Epoch [7490/100000], Validation Loss: 553.1931\n",
            "Epoch [7491/100000], Validation Loss: 546.9364\n",
            "Epoch [7492/100000], Validation Loss: 549.7342\n",
            "Epoch [7493/100000], Validation Loss: 547.6234\n",
            "Epoch [7494/100000], Validation Loss: 548.8374\n",
            "Epoch [7495/100000], Validation Loss: 549.8623\n",
            "Epoch [7496/100000], Validation Loss: 550.8083\n",
            "Epoch [7497/100000], Validation Loss: 548.8081\n",
            "Epoch [7498/100000], Validation Loss: 543.9990\n",
            "Epoch [7499/100000], Validation Loss: 550.9227\n",
            "Epoch [7500/100000], Validation Loss: 552.9877\n",
            "Epoch [7501/100000], Validation Loss: 545.1914\n",
            "Epoch [7502/100000], Validation Loss: 548.8679\n",
            "Epoch [7503/100000], Validation Loss: 549.7040\n",
            "Epoch [7504/100000], Validation Loss: 546.7151\n",
            "Epoch [7505/100000], Validation Loss: 543.0516\n",
            "Epoch [7506/100000], Validation Loss: 548.7066\n",
            "Epoch [7507/100000], Validation Loss: 556.2310\n",
            "Epoch [7508/100000], Validation Loss: 550.4995\n",
            "Epoch [7509/100000], Validation Loss: 553.0180\n",
            "Epoch [7510/100000], Validation Loss: 549.6023\n",
            "Epoch [7511/100000], Validation Loss: 561.3870\n",
            "Epoch [7512/100000], Validation Loss: 545.8700\n",
            "Epoch [7513/100000], Validation Loss: 547.8559\n",
            "Epoch [7514/100000], Validation Loss: 548.2869\n",
            "Epoch [7515/100000], Validation Loss: 549.2590\n",
            "Epoch [7516/100000], Validation Loss: 545.2326\n",
            "Epoch [7517/100000], Validation Loss: 546.5993\n",
            "Epoch [7518/100000], Validation Loss: 547.5582\n",
            "Epoch [7519/100000], Validation Loss: 556.7380\n",
            "Epoch [7520/100000], Validation Loss: 551.4027\n",
            "Epoch [7521/100000], Validation Loss: 544.3958\n",
            "Epoch [7522/100000], Validation Loss: 548.0282\n",
            "Epoch [7523/100000], Validation Loss: 554.7763\n",
            "Epoch [7524/100000], Validation Loss: 548.1992\n",
            "Epoch [7525/100000], Validation Loss: 552.3091\n",
            "Epoch [7526/100000], Validation Loss: 551.4832\n",
            "Epoch [7527/100000], Validation Loss: 547.3352\n",
            "Epoch [7528/100000], Validation Loss: 552.0615\n",
            "Epoch [7529/100000], Validation Loss: 552.5556\n",
            "Epoch [7530/100000], Validation Loss: 549.7077\n",
            "Epoch [7531/100000], Validation Loss: 559.4713\n",
            "Epoch [7532/100000], Validation Loss: 551.5075\n",
            "Epoch [7533/100000], Validation Loss: 544.4854\n",
            "Epoch [7534/100000], Validation Loss: 548.9652\n",
            "Epoch [7535/100000], Validation Loss: 545.9599\n",
            "Epoch [7536/100000], Validation Loss: 558.6048\n",
            "Epoch [7537/100000], Validation Loss: 550.9013\n",
            "Epoch [7538/100000], Validation Loss: 547.6734\n",
            "Epoch [7539/100000], Validation Loss: 548.9983\n",
            "Epoch [7540/100000], Validation Loss: 548.4078\n",
            "Epoch [7541/100000], Validation Loss: 547.0892\n",
            "Epoch [7542/100000], Validation Loss: 547.2767\n",
            "Epoch [7543/100000], Validation Loss: 547.2248\n",
            "Epoch [7544/100000], Validation Loss: 547.6244\n",
            "Epoch [7545/100000], Validation Loss: 550.8856\n",
            "Epoch [7546/100000], Validation Loss: 547.9991\n",
            "Epoch [7547/100000], Validation Loss: 556.3862\n",
            "Epoch [7548/100000], Validation Loss: 551.3501\n",
            "Epoch [7549/100000], Validation Loss: 546.9294\n",
            "Epoch [7550/100000], Validation Loss: 551.0261\n",
            "Epoch [7551/100000], Validation Loss: 544.1070\n",
            "Epoch [7552/100000], Validation Loss: 543.8103\n",
            "Epoch [7553/100000], Validation Loss: 543.6988\n",
            "Epoch [7554/100000], Validation Loss: 543.1032\n",
            "Epoch [7555/100000], Validation Loss: 547.1005\n",
            "Epoch [7556/100000], Validation Loss: 550.2748\n",
            "Epoch [7557/100000], Validation Loss: 548.1334\n",
            "Epoch [7558/100000], Validation Loss: 545.9105\n",
            "Epoch [7559/100000], Validation Loss: 547.7161\n",
            "Epoch [7560/100000], Validation Loss: 554.2028\n",
            "Epoch [7561/100000], Validation Loss: 546.5495\n",
            "Epoch [7562/100000], Validation Loss: 551.7937\n",
            "Epoch [7563/100000], Validation Loss: 552.8170\n",
            "Epoch [7564/100000], Validation Loss: 551.9758\n",
            "Epoch [7565/100000], Validation Loss: 548.9373\n",
            "Epoch [7566/100000], Validation Loss: 547.2485\n",
            "Epoch [7567/100000], Validation Loss: 547.8282\n",
            "Epoch [7568/100000], Validation Loss: 548.1107\n",
            "Epoch [7569/100000], Validation Loss: 553.2452\n",
            "Epoch [7570/100000], Validation Loss: 546.1508\n",
            "Epoch [7571/100000], Validation Loss: 551.5499\n",
            "Epoch [7572/100000], Validation Loss: 554.9536\n",
            "Epoch [7573/100000], Validation Loss: 548.7977\n",
            "Epoch [7574/100000], Validation Loss: 546.8068\n",
            "Epoch [7575/100000], Validation Loss: 551.2853\n",
            "Epoch [7576/100000], Validation Loss: 551.7574\n",
            "Epoch [7577/100000], Validation Loss: 551.2501\n",
            "Epoch [7578/100000], Validation Loss: 563.6523\n",
            "Epoch [7579/100000], Validation Loss: 549.0381\n",
            "Epoch [7580/100000], Validation Loss: 552.1058\n",
            "Epoch [7581/100000], Validation Loss: 541.2531\n",
            "Epoch [7582/100000], Validation Loss: 546.8765\n",
            "Epoch [7583/100000], Validation Loss: 551.1876\n",
            "Epoch [7584/100000], Validation Loss: 551.6991\n",
            "Epoch [7585/100000], Validation Loss: 552.3614\n",
            "Epoch [7586/100000], Validation Loss: 550.6093\n",
            "Epoch [7587/100000], Validation Loss: 549.2247\n",
            "Epoch [7588/100000], Validation Loss: 550.9160\n",
            "Epoch [7589/100000], Validation Loss: 549.3012\n",
            "Epoch [7590/100000], Validation Loss: 557.3209\n",
            "Epoch [7591/100000], Validation Loss: 549.2775\n",
            "Epoch [7592/100000], Validation Loss: 552.9899\n",
            "Epoch [7593/100000], Validation Loss: 550.1105\n",
            "Epoch [7594/100000], Validation Loss: 541.0134\n",
            "Epoch [7595/100000], Validation Loss: 553.1696\n",
            "Epoch [7596/100000], Validation Loss: 549.4012\n",
            "Epoch [7597/100000], Validation Loss: 547.9569\n",
            "Epoch [7598/100000], Validation Loss: 571.3193\n",
            "Epoch [7599/100000], Validation Loss: 549.5675\n",
            "Epoch [7600/100000], Validation Loss: 551.3649\n",
            "Epoch [7601/100000], Validation Loss: 549.1106\n",
            "Epoch [7602/100000], Validation Loss: 545.0234\n",
            "Epoch [7603/100000], Validation Loss: 548.8893\n",
            "Epoch [7604/100000], Validation Loss: 548.7270\n",
            "Epoch [7605/100000], Validation Loss: 550.1637\n",
            "Epoch [7606/100000], Validation Loss: 551.1804\n",
            "Epoch [7607/100000], Validation Loss: 548.8841\n",
            "Epoch [7608/100000], Validation Loss: 556.0112\n",
            "Epoch [7609/100000], Validation Loss: 555.2017\n",
            "Epoch [7610/100000], Validation Loss: 545.9404\n",
            "Epoch [7611/100000], Validation Loss: 550.1467\n",
            "Epoch [7612/100000], Validation Loss: 548.8567\n",
            "Epoch [7613/100000], Validation Loss: 545.1381\n",
            "Epoch [7614/100000], Validation Loss: 549.1030\n",
            "Epoch [7615/100000], Validation Loss: 548.7104\n",
            "Epoch [7616/100000], Validation Loss: 546.5920\n",
            "Epoch [7617/100000], Validation Loss: 542.8872\n",
            "Epoch [7618/100000], Validation Loss: 549.7905\n",
            "Epoch [7619/100000], Validation Loss: 549.4754\n",
            "Epoch [7620/100000], Validation Loss: 552.3859\n",
            "Epoch [7621/100000], Validation Loss: 550.0466\n",
            "Epoch [7622/100000], Validation Loss: 550.8963\n",
            "Epoch [7623/100000], Validation Loss: 547.6060\n",
            "Epoch [7624/100000], Validation Loss: 545.1122\n",
            "Epoch [7625/100000], Validation Loss: 548.3343\n",
            "Epoch [7626/100000], Validation Loss: 546.8483\n",
            "Epoch [7627/100000], Validation Loss: 549.6912\n",
            "Epoch [7628/100000], Validation Loss: 544.9641\n",
            "Epoch [7629/100000], Validation Loss: 549.6069\n",
            "Epoch [7630/100000], Validation Loss: 547.4371\n",
            "Epoch [7631/100000], Validation Loss: 551.3885\n",
            "Epoch [7632/100000], Validation Loss: 547.2283\n",
            "Epoch [7633/100000], Validation Loss: 547.9682\n",
            "Epoch [7634/100000], Validation Loss: 550.4022\n",
            "Epoch [7635/100000], Validation Loss: 556.2554\n",
            "Epoch [7636/100000], Validation Loss: 549.6586\n",
            "Epoch [7637/100000], Validation Loss: 557.6539\n",
            "Epoch [7638/100000], Validation Loss: 548.0872\n",
            "Epoch [7639/100000], Validation Loss: 549.9990\n",
            "Epoch [7640/100000], Validation Loss: 547.7497\n",
            "Epoch [7641/100000], Validation Loss: 551.4329\n",
            "Epoch [7642/100000], Validation Loss: 550.7633\n",
            "Epoch [7643/100000], Validation Loss: 547.3360\n",
            "Epoch [7644/100000], Validation Loss: 549.6278\n",
            "Epoch [7645/100000], Validation Loss: 548.9517\n",
            "Epoch [7646/100000], Validation Loss: 549.4557\n",
            "Epoch [7647/100000], Validation Loss: 551.3166\n",
            "Epoch [7648/100000], Validation Loss: 547.9814\n",
            "Epoch [7649/100000], Validation Loss: 554.0267\n",
            "Epoch [7650/100000], Validation Loss: 549.3588\n",
            "Epoch [7651/100000], Validation Loss: 546.8712\n",
            "Epoch [7652/100000], Validation Loss: 549.1886\n",
            "Epoch [7653/100000], Validation Loss: 547.4592\n",
            "Epoch [7654/100000], Validation Loss: 547.5130\n",
            "Epoch [7655/100000], Validation Loss: 548.9942\n",
            "Epoch [7656/100000], Validation Loss: 543.5178\n",
            "Epoch [7657/100000], Validation Loss: 544.4776\n",
            "Epoch [7658/100000], Validation Loss: 549.4568\n",
            "Epoch [7659/100000], Validation Loss: 549.1418\n",
            "Epoch [7660/100000], Validation Loss: 545.7292\n",
            "Epoch [7661/100000], Validation Loss: 555.2303\n",
            "Epoch [7662/100000], Validation Loss: 547.7746\n",
            "Epoch [7663/100000], Validation Loss: 548.2208\n",
            "Epoch [7664/100000], Validation Loss: 556.0746\n",
            "Epoch [7665/100000], Validation Loss: 549.8108\n",
            "Epoch [7666/100000], Validation Loss: 545.0568\n",
            "Epoch [7667/100000], Validation Loss: 552.0703\n",
            "Epoch [7668/100000], Validation Loss: 551.4314\n",
            "Epoch [7669/100000], Validation Loss: 551.8054\n",
            "Epoch [7670/100000], Validation Loss: 545.5402\n",
            "Epoch [7671/100000], Validation Loss: 549.9463\n",
            "Epoch [7672/100000], Validation Loss: 550.2196\n",
            "Epoch [7673/100000], Validation Loss: 546.4321\n",
            "Epoch [7674/100000], Validation Loss: 546.1992\n",
            "Epoch [7675/100000], Validation Loss: 551.2859\n",
            "Epoch [7676/100000], Validation Loss: 551.8084\n",
            "Epoch [7677/100000], Validation Loss: 548.2026\n",
            "Epoch [7678/100000], Validation Loss: 549.3544\n",
            "Epoch [7679/100000], Validation Loss: 553.6991\n",
            "Epoch [7680/100000], Validation Loss: 553.2619\n",
            "Epoch [7681/100000], Validation Loss: 548.7667\n",
            "Epoch [7682/100000], Validation Loss: 567.1681\n",
            "Epoch [7683/100000], Validation Loss: 546.2236\n",
            "Epoch [7684/100000], Validation Loss: 547.4702\n",
            "Epoch [7685/100000], Validation Loss: 548.2321\n",
            "Epoch [7686/100000], Validation Loss: 549.4393\n",
            "Epoch [7687/100000], Validation Loss: 548.0109\n",
            "Epoch [7688/100000], Validation Loss: 546.8526\n",
            "Epoch [7689/100000], Validation Loss: 548.4222\n",
            "Epoch [7690/100000], Validation Loss: 546.7403\n",
            "Epoch [7691/100000], Validation Loss: 547.4387\n",
            "Epoch [7692/100000], Validation Loss: 552.4556\n",
            "Epoch [7693/100000], Validation Loss: 546.2007\n",
            "Epoch [7694/100000], Validation Loss: 551.2672\n",
            "Epoch [7695/100000], Validation Loss: 545.6687\n",
            "Epoch [7696/100000], Validation Loss: 545.0873\n",
            "Epoch [7697/100000], Validation Loss: 547.8825\n",
            "Epoch [7698/100000], Validation Loss: 545.2580\n",
            "Epoch [7699/100000], Validation Loss: 552.3838\n",
            "Epoch [7700/100000], Validation Loss: 545.8106\n",
            "Epoch [7701/100000], Validation Loss: 551.1924\n",
            "Epoch [7702/100000], Validation Loss: 552.0273\n",
            "Epoch [7703/100000], Validation Loss: 549.1475\n",
            "Epoch [7704/100000], Validation Loss: 546.9974\n",
            "Epoch [7705/100000], Validation Loss: 551.8048\n",
            "Epoch [7706/100000], Validation Loss: 547.6787\n",
            "Epoch [7707/100000], Validation Loss: 548.2837\n",
            "Epoch [7708/100000], Validation Loss: 552.1734\n",
            "Epoch [7709/100000], Validation Loss: 549.0323\n",
            "Epoch [7710/100000], Validation Loss: 552.4557\n",
            "Epoch [7711/100000], Validation Loss: 548.5131\n",
            "Epoch [7712/100000], Validation Loss: 547.3169\n",
            "Epoch [7713/100000], Validation Loss: 550.8656\n",
            "Epoch [7714/100000], Validation Loss: 548.2940\n",
            "Epoch [7715/100000], Validation Loss: 550.6594\n",
            "Epoch [7716/100000], Validation Loss: 551.8690\n",
            "Epoch [7717/100000], Validation Loss: 548.7859\n",
            "Epoch [7718/100000], Validation Loss: 548.6360\n",
            "Epoch [7719/100000], Validation Loss: 548.5588\n",
            "Epoch [7720/100000], Validation Loss: 550.8690\n",
            "Epoch [7721/100000], Validation Loss: 547.5567\n",
            "Epoch [7722/100000], Validation Loss: 547.7261\n",
            "Epoch [7723/100000], Validation Loss: 549.1336\n",
            "Epoch [7724/100000], Validation Loss: 546.8840\n",
            "Epoch [7725/100000], Validation Loss: 554.6266\n",
            "Epoch [7726/100000], Validation Loss: 553.8679\n",
            "Epoch [7727/100000], Validation Loss: 545.9477\n",
            "Epoch [7728/100000], Validation Loss: 549.1926\n",
            "Epoch [7729/100000], Validation Loss: 552.4577\n",
            "Epoch [7730/100000], Validation Loss: 545.1591\n",
            "Epoch [7731/100000], Validation Loss: 544.1714\n",
            "Epoch [7732/100000], Validation Loss: 545.0120\n",
            "Epoch [7733/100000], Validation Loss: 557.2112\n",
            "Epoch [7734/100000], Validation Loss: 549.0225\n",
            "Epoch [7735/100000], Validation Loss: 543.3012\n",
            "Epoch [7736/100000], Validation Loss: 544.0825\n",
            "Epoch [7737/100000], Validation Loss: 550.5203\n",
            "Epoch [7738/100000], Validation Loss: 544.1832\n",
            "Epoch [7739/100000], Validation Loss: 547.8293\n",
            "Epoch [7740/100000], Validation Loss: 552.8690\n",
            "Epoch [7741/100000], Validation Loss: 554.4134\n",
            "Epoch [7742/100000], Validation Loss: 551.0123\n",
            "Epoch [7743/100000], Validation Loss: 547.3021\n",
            "Epoch [7744/100000], Validation Loss: 547.6707\n",
            "Epoch [7745/100000], Validation Loss: 547.7464\n",
            "Epoch [7746/100000], Validation Loss: 547.1528\n",
            "Epoch [7747/100000], Validation Loss: 557.2482\n",
            "Epoch [7748/100000], Validation Loss: 552.6283\n",
            "Epoch [7749/100000], Validation Loss: 560.5204\n",
            "Epoch [7750/100000], Validation Loss: 546.8998\n",
            "Epoch [7751/100000], Validation Loss: 549.0709\n",
            "Epoch [7752/100000], Validation Loss: 545.4353\n",
            "Epoch [7753/100000], Validation Loss: 548.9721\n",
            "Epoch [7754/100000], Validation Loss: 551.7661\n",
            "Epoch [7755/100000], Validation Loss: 551.7468\n",
            "Epoch [7756/100000], Validation Loss: 547.4800\n",
            "Epoch [7757/100000], Validation Loss: 549.9210\n",
            "Epoch [7758/100000], Validation Loss: 550.6317\n",
            "Epoch [7759/100000], Validation Loss: 550.0681\n",
            "Epoch [7760/100000], Validation Loss: 557.7983\n",
            "Epoch [7761/100000], Validation Loss: 550.4648\n",
            "Epoch [7762/100000], Validation Loss: 547.4598\n",
            "Epoch [7763/100000], Validation Loss: 550.5608\n",
            "Epoch [7764/100000], Validation Loss: 550.2874\n",
            "Epoch [7765/100000], Validation Loss: 551.7005\n",
            "Epoch [7766/100000], Validation Loss: 546.4723\n",
            "Epoch [7767/100000], Validation Loss: 556.0009\n",
            "Epoch [7768/100000], Validation Loss: 551.6648\n",
            "Epoch [7769/100000], Validation Loss: 549.6496\n",
            "Epoch [7770/100000], Validation Loss: 549.7685\n",
            "Epoch [7771/100000], Validation Loss: 546.9302\n",
            "Epoch [7772/100000], Validation Loss: 546.2481\n",
            "Epoch [7773/100000], Validation Loss: 550.5792\n",
            "Epoch [7774/100000], Validation Loss: 550.2826\n",
            "Epoch [7775/100000], Validation Loss: 549.3948\n",
            "Epoch [7776/100000], Validation Loss: 550.5081\n",
            "Epoch [7777/100000], Validation Loss: 548.4815\n",
            "Epoch [7778/100000], Validation Loss: 546.8032\n",
            "Epoch [7779/100000], Validation Loss: 547.4359\n",
            "Epoch [7780/100000], Validation Loss: 549.6103\n",
            "Epoch [7781/100000], Validation Loss: 554.3077\n",
            "Epoch [7782/100000], Validation Loss: 551.0515\n",
            "Epoch [7783/100000], Validation Loss: 550.7781\n",
            "Epoch [7784/100000], Validation Loss: 551.4433\n",
            "Epoch [7785/100000], Validation Loss: 548.6483\n",
            "Epoch [7786/100000], Validation Loss: 545.9259\n",
            "Epoch [7787/100000], Validation Loss: 549.3075\n",
            "Epoch [7788/100000], Validation Loss: 551.4020\n",
            "Epoch [7789/100000], Validation Loss: 557.3447\n",
            "Epoch [7790/100000], Validation Loss: 549.8615\n",
            "Epoch [7791/100000], Validation Loss: 550.8864\n",
            "Epoch [7792/100000], Validation Loss: 549.8308\n",
            "Epoch [7793/100000], Validation Loss: 548.4407\n",
            "Epoch [7794/100000], Validation Loss: 553.7975\n",
            "Epoch [7795/100000], Validation Loss: 546.6776\n",
            "Epoch [7796/100000], Validation Loss: 550.0160\n",
            "Epoch [7797/100000], Validation Loss: 549.3542\n",
            "Epoch [7798/100000], Validation Loss: 551.4824\n",
            "Epoch [7799/100000], Validation Loss: 547.6980\n",
            "Epoch [7800/100000], Validation Loss: 548.0830\n",
            "Epoch [7801/100000], Validation Loss: 551.2136\n",
            "Epoch [7802/100000], Validation Loss: 546.6471\n",
            "Epoch [7803/100000], Validation Loss: 548.7757\n",
            "Epoch [7804/100000], Validation Loss: 552.9603\n",
            "Epoch [7805/100000], Validation Loss: 549.0291\n",
            "Epoch [7806/100000], Validation Loss: 550.3532\n",
            "Epoch [7807/100000], Validation Loss: 552.8715\n",
            "Epoch [7808/100000], Validation Loss: 546.0977\n",
            "Epoch [7809/100000], Validation Loss: 545.9018\n",
            "Epoch [7810/100000], Validation Loss: 548.9167\n",
            "Epoch [7811/100000], Validation Loss: 547.7449\n",
            "Epoch [7812/100000], Validation Loss: 553.9287\n",
            "Epoch [7813/100000], Validation Loss: 552.2165\n",
            "Epoch [7814/100000], Validation Loss: 553.2994\n",
            "Epoch [7815/100000], Validation Loss: 551.5644\n",
            "Epoch [7816/100000], Validation Loss: 545.8714\n",
            "Epoch [7817/100000], Validation Loss: 587.7531\n",
            "Epoch [7818/100000], Validation Loss: 551.5470\n",
            "Epoch [7819/100000], Validation Loss: 543.4920\n",
            "Epoch [7820/100000], Validation Loss: 553.1156\n",
            "Epoch [7821/100000], Validation Loss: 551.3662\n",
            "Epoch [7822/100000], Validation Loss: 548.6298\n",
            "Epoch [7823/100000], Validation Loss: 555.4620\n",
            "Epoch [7824/100000], Validation Loss: 550.2741\n",
            "Epoch [7825/100000], Validation Loss: 551.9284\n",
            "Epoch [7826/100000], Validation Loss: 545.3450\n",
            "Epoch [7827/100000], Validation Loss: 546.3900\n",
            "Epoch [7828/100000], Validation Loss: 550.9840\n",
            "Epoch [7829/100000], Validation Loss: 551.1362\n",
            "Epoch [7830/100000], Validation Loss: 546.8233\n",
            "Epoch [7831/100000], Validation Loss: 547.4022\n",
            "Epoch [7832/100000], Validation Loss: 558.9053\n",
            "Epoch [7833/100000], Validation Loss: 554.5464\n",
            "Epoch [7834/100000], Validation Loss: 549.2408\n",
            "Epoch [7835/100000], Validation Loss: 549.9870\n",
            "Epoch [7836/100000], Validation Loss: 543.8867\n",
            "Epoch [7837/100000], Validation Loss: 546.7065\n",
            "Epoch [7838/100000], Validation Loss: 555.6751\n",
            "Epoch [7839/100000], Validation Loss: 547.6926\n",
            "Epoch [7840/100000], Validation Loss: 547.6344\n",
            "Epoch [7841/100000], Validation Loss: 549.5171\n",
            "Epoch [7842/100000], Validation Loss: 549.6681\n",
            "Epoch [7843/100000], Validation Loss: 551.9222\n",
            "Epoch [7844/100000], Validation Loss: 552.5344\n",
            "Epoch [7845/100000], Validation Loss: 542.9214\n",
            "Epoch [7846/100000], Validation Loss: 546.5144\n",
            "Epoch [7847/100000], Validation Loss: 549.5642\n",
            "Epoch [7848/100000], Validation Loss: 552.0235\n",
            "Epoch [7849/100000], Validation Loss: 548.6183\n",
            "Epoch [7850/100000], Validation Loss: 547.3698\n",
            "Epoch [7851/100000], Validation Loss: 543.8429\n",
            "Epoch [7852/100000], Validation Loss: 545.5889\n",
            "Epoch [7853/100000], Validation Loss: 548.3660\n",
            "Epoch [7854/100000], Validation Loss: 549.9581\n",
            "Epoch [7855/100000], Validation Loss: 546.6193\n",
            "Epoch [7856/100000], Validation Loss: 548.3355\n",
            "Epoch [7857/100000], Validation Loss: 549.7494\n",
            "Epoch [7858/100000], Validation Loss: 546.4714\n",
            "Epoch [7859/100000], Validation Loss: 551.8183\n",
            "Epoch [7860/100000], Validation Loss: 549.7099\n",
            "Epoch [7861/100000], Validation Loss: 546.9215\n",
            "Epoch [7862/100000], Validation Loss: 547.6798\n",
            "Epoch [7863/100000], Validation Loss: 556.7161\n",
            "Epoch [7864/100000], Validation Loss: 549.5489\n",
            "Epoch [7865/100000], Validation Loss: 547.1756\n",
            "Epoch [7866/100000], Validation Loss: 545.7335\n",
            "Epoch [7867/100000], Validation Loss: 544.9405\n",
            "Epoch [7868/100000], Validation Loss: 549.6809\n",
            "Epoch [7869/100000], Validation Loss: 551.8781\n",
            "Epoch [7870/100000], Validation Loss: 550.4342\n",
            "Epoch [7871/100000], Validation Loss: 547.5501\n",
            "Epoch [7872/100000], Validation Loss: 555.0960\n",
            "Epoch [7873/100000], Validation Loss: 551.4517\n",
            "Epoch [7874/100000], Validation Loss: 546.7777\n",
            "Epoch [7875/100000], Validation Loss: 548.3107\n",
            "Epoch [7876/100000], Validation Loss: 549.1977\n",
            "Epoch [7877/100000], Validation Loss: 554.3258\n",
            "Epoch [7878/100000], Validation Loss: 545.7016\n",
            "Epoch [7879/100000], Validation Loss: 555.4617\n",
            "Epoch [7880/100000], Validation Loss: 556.5071\n",
            "Epoch [7881/100000], Validation Loss: 548.6797\n",
            "Epoch [7882/100000], Validation Loss: 545.9218\n",
            "Epoch [7883/100000], Validation Loss: 545.4126\n",
            "Epoch [7884/100000], Validation Loss: 547.4591\n",
            "Epoch [7885/100000], Validation Loss: 549.4538\n",
            "Epoch [7886/100000], Validation Loss: 549.6294\n",
            "Epoch [7887/100000], Validation Loss: 546.5120\n",
            "Epoch [7888/100000], Validation Loss: 548.0273\n",
            "Epoch [7889/100000], Validation Loss: 550.7861\n",
            "Epoch [7890/100000], Validation Loss: 544.3298\n",
            "Epoch [7891/100000], Validation Loss: 544.2187\n",
            "Epoch [7892/100000], Validation Loss: 547.2387\n",
            "Epoch [7893/100000], Validation Loss: 552.4786\n",
            "Epoch [7894/100000], Validation Loss: 553.6959\n",
            "Epoch [7895/100000], Validation Loss: 554.6829\n",
            "Epoch [7896/100000], Validation Loss: 548.2253\n",
            "Epoch [7897/100000], Validation Loss: 546.5513\n",
            "Epoch [7898/100000], Validation Loss: 554.1734\n",
            "Epoch [7899/100000], Validation Loss: 545.2206\n",
            "Epoch [7900/100000], Validation Loss: 556.2365\n",
            "Epoch [7901/100000], Validation Loss: 547.4558\n",
            "Epoch [7902/100000], Validation Loss: 549.8861\n",
            "Epoch [7903/100000], Validation Loss: 550.1440\n",
            "Epoch [7904/100000], Validation Loss: 550.6728\n",
            "Epoch [7905/100000], Validation Loss: 550.6995\n",
            "Epoch [7906/100000], Validation Loss: 553.0065\n",
            "Epoch [7907/100000], Validation Loss: 547.1705\n",
            "Epoch [7908/100000], Validation Loss: 549.8776\n",
            "Epoch [7909/100000], Validation Loss: 547.0483\n",
            "Epoch [7910/100000], Validation Loss: 548.8005\n",
            "Epoch [7911/100000], Validation Loss: 552.7387\n",
            "Epoch [7912/100000], Validation Loss: 548.8349\n",
            "Epoch [7913/100000], Validation Loss: 548.7355\n",
            "Epoch [7914/100000], Validation Loss: 548.0128\n",
            "Epoch [7915/100000], Validation Loss: 553.4243\n",
            "Epoch [7916/100000], Validation Loss: 545.1039\n",
            "Epoch [7917/100000], Validation Loss: 551.4930\n",
            "Epoch [7918/100000], Validation Loss: 550.9803\n",
            "Epoch [7919/100000], Validation Loss: 547.0419\n",
            "Epoch [7920/100000], Validation Loss: 548.5029\n",
            "Epoch [7921/100000], Validation Loss: 546.1511\n",
            "Epoch [7922/100000], Validation Loss: 548.6137\n",
            "Epoch [7923/100000], Validation Loss: 550.5828\n",
            "Epoch [7924/100000], Validation Loss: 544.8233\n",
            "Epoch [7925/100000], Validation Loss: 548.7468\n",
            "Epoch [7926/100000], Validation Loss: 546.3569\n",
            "Epoch [7927/100000], Validation Loss: 546.6009\n",
            "Epoch [7928/100000], Validation Loss: 546.9978\n",
            "Epoch [7929/100000], Validation Loss: 548.0734\n",
            "Epoch [7930/100000], Validation Loss: 551.9852\n",
            "Epoch [7931/100000], Validation Loss: 550.4694\n",
            "Epoch [7932/100000], Validation Loss: 546.3488\n",
            "Epoch [7933/100000], Validation Loss: 548.2773\n",
            "Epoch [7934/100000], Validation Loss: 550.0457\n",
            "Epoch [7935/100000], Validation Loss: 547.0747\n",
            "Epoch [7936/100000], Validation Loss: 547.7465\n",
            "Epoch [7937/100000], Validation Loss: 550.8604\n",
            "Epoch [7938/100000], Validation Loss: 551.6530\n",
            "Epoch [7939/100000], Validation Loss: 549.2105\n",
            "Epoch [7940/100000], Validation Loss: 552.8188\n",
            "Epoch [7941/100000], Validation Loss: 552.0387\n",
            "Epoch [7942/100000], Validation Loss: 550.2981\n",
            "Epoch [7943/100000], Validation Loss: 552.7278\n",
            "Epoch [7944/100000], Validation Loss: 549.2592\n",
            "Epoch [7945/100000], Validation Loss: 546.9375\n",
            "Epoch [7946/100000], Validation Loss: 551.2507\n",
            "Epoch [7947/100000], Validation Loss: 553.5805\n",
            "Epoch [7948/100000], Validation Loss: 548.4621\n",
            "Epoch [7949/100000], Validation Loss: 553.0257\n",
            "Epoch [7950/100000], Validation Loss: 549.7238\n",
            "Epoch [7951/100000], Validation Loss: 553.4295\n",
            "Epoch [7952/100000], Validation Loss: 549.5639\n",
            "Epoch [7953/100000], Validation Loss: 549.0992\n",
            "Epoch [7954/100000], Validation Loss: 547.9258\n",
            "Epoch [7955/100000], Validation Loss: 553.0041\n",
            "Epoch [7956/100000], Validation Loss: 551.3596\n",
            "Epoch [7957/100000], Validation Loss: 547.7172\n",
            "Epoch [7958/100000], Validation Loss: 550.2038\n",
            "Epoch [7959/100000], Validation Loss: 548.7246\n",
            "Epoch [7960/100000], Validation Loss: 548.5823\n",
            "Epoch [7961/100000], Validation Loss: 551.2814\n",
            "Epoch [7962/100000], Validation Loss: 546.7629\n",
            "Epoch [7963/100000], Validation Loss: 550.5348\n",
            "Epoch [7964/100000], Validation Loss: 546.9360\n",
            "Epoch [7965/100000], Validation Loss: 547.8975\n",
            "Epoch [7966/100000], Validation Loss: 550.6156\n",
            "Epoch [7967/100000], Validation Loss: 548.4645\n",
            "Epoch [7968/100000], Validation Loss: 550.9763\n",
            "Epoch [7969/100000], Validation Loss: 547.7679\n",
            "Epoch [7970/100000], Validation Loss: 549.5190\n",
            "Epoch [7971/100000], Validation Loss: 544.4679\n",
            "Epoch [7972/100000], Validation Loss: 578.8833\n",
            "Epoch [7973/100000], Validation Loss: 555.1075\n",
            "Epoch [7974/100000], Validation Loss: 553.3931\n",
            "Epoch [7975/100000], Validation Loss: 551.8273\n",
            "Epoch [7976/100000], Validation Loss: 546.2504\n",
            "Epoch [7977/100000], Validation Loss: 550.8396\n",
            "Epoch [7978/100000], Validation Loss: 551.2665\n",
            "Epoch [7979/100000], Validation Loss: 547.0092\n",
            "Epoch [7980/100000], Validation Loss: 554.0839\n",
            "Epoch [7981/100000], Validation Loss: 549.6293\n",
            "Epoch [7982/100000], Validation Loss: 551.3869\n",
            "Epoch [7983/100000], Validation Loss: 546.8944\n",
            "Epoch [7984/100000], Validation Loss: 550.3009\n",
            "Epoch [7985/100000], Validation Loss: 548.5171\n",
            "Epoch [7986/100000], Validation Loss: 544.9278\n",
            "Epoch [7987/100000], Validation Loss: 549.3562\n",
            "Epoch [7988/100000], Validation Loss: 550.5831\n",
            "Epoch [7989/100000], Validation Loss: 548.6610\n",
            "Epoch [7990/100000], Validation Loss: 549.7452\n",
            "Epoch [7991/100000], Validation Loss: 547.7397\n",
            "Epoch [7992/100000], Validation Loss: 548.1371\n",
            "Epoch [7993/100000], Validation Loss: 554.6756\n",
            "Epoch [7994/100000], Validation Loss: 551.7555\n",
            "Epoch [7995/100000], Validation Loss: 547.1090\n",
            "Epoch [7996/100000], Validation Loss: 546.1802\n",
            "Epoch [7997/100000], Validation Loss: 550.4912\n",
            "Epoch [7998/100000], Validation Loss: 551.4451\n",
            "Epoch [7999/100000], Validation Loss: 552.0049\n",
            "Epoch [8000/100000], Validation Loss: 550.7758\n",
            "Epoch [8001/100000], Validation Loss: 548.4578\n",
            "Epoch [8002/100000], Validation Loss: 549.7624\n",
            "Epoch [8003/100000], Validation Loss: 549.9286\n",
            "Epoch [8004/100000], Validation Loss: 547.3022\n",
            "Epoch [8005/100000], Validation Loss: 551.6583\n",
            "Epoch [8006/100000], Validation Loss: 549.2371\n",
            "Epoch [8007/100000], Validation Loss: 550.3964\n",
            "Epoch [8008/100000], Validation Loss: 543.8878\n",
            "Epoch [8009/100000], Validation Loss: 554.8927\n",
            "Epoch [8010/100000], Validation Loss: 547.4408\n",
            "Epoch [8011/100000], Validation Loss: 552.7201\n",
            "Epoch [8012/100000], Validation Loss: 549.6602\n",
            "Epoch [8013/100000], Validation Loss: 550.7816\n",
            "Epoch [8014/100000], Validation Loss: 552.9259\n",
            "Epoch [8015/100000], Validation Loss: 552.4789\n",
            "Epoch [8016/100000], Validation Loss: 549.5711\n",
            "Epoch [8017/100000], Validation Loss: 565.8889\n",
            "Epoch [8018/100000], Validation Loss: 548.1330\n",
            "Epoch [8019/100000], Validation Loss: 546.6670\n",
            "Epoch [8020/100000], Validation Loss: 549.9829\n",
            "Epoch [8021/100000], Validation Loss: 549.5795\n",
            "Epoch [8022/100000], Validation Loss: 551.1836\n",
            "Epoch [8023/100000], Validation Loss: 550.9584\n",
            "Epoch [8024/100000], Validation Loss: 549.9625\n",
            "Epoch [8025/100000], Validation Loss: 553.8616\n",
            "Epoch [8026/100000], Validation Loss: 550.7887\n",
            "Epoch [8027/100000], Validation Loss: 551.2740\n",
            "Epoch [8028/100000], Validation Loss: 555.6979\n",
            "Epoch [8029/100000], Validation Loss: 547.7238\n",
            "Epoch [8030/100000], Validation Loss: 549.7890\n",
            "Epoch [8031/100000], Validation Loss: 547.9228\n",
            "Epoch [8032/100000], Validation Loss: 547.5939\n",
            "Epoch [8033/100000], Validation Loss: 549.5693\n",
            "Epoch [8034/100000], Validation Loss: 551.8306\n",
            "Epoch [8035/100000], Validation Loss: 550.7100\n",
            "Epoch [8036/100000], Validation Loss: 545.9289\n",
            "Epoch [8037/100000], Validation Loss: 547.7379\n",
            "Epoch [8038/100000], Validation Loss: 549.9108\n",
            "Epoch [8039/100000], Validation Loss: 549.7755\n",
            "Epoch [8040/100000], Validation Loss: 551.8669\n",
            "Epoch [8041/100000], Validation Loss: 554.9986\n",
            "Epoch [8042/100000], Validation Loss: 545.4175\n",
            "Epoch [8043/100000], Validation Loss: 547.9316\n",
            "Epoch [8044/100000], Validation Loss: 545.3473\n",
            "Epoch [8045/100000], Validation Loss: 547.1386\n",
            "Epoch [8046/100000], Validation Loss: 550.3136\n",
            "Epoch [8047/100000], Validation Loss: 546.6254\n",
            "Epoch [8048/100000], Validation Loss: 546.3410\n",
            "Epoch [8049/100000], Validation Loss: 553.0285\n",
            "Epoch [8050/100000], Validation Loss: 555.0730\n",
            "Epoch [8051/100000], Validation Loss: 548.1402\n",
            "Epoch [8052/100000], Validation Loss: 553.4373\n",
            "Epoch [8053/100000], Validation Loss: 550.5479\n",
            "Epoch [8054/100000], Validation Loss: 553.1979\n",
            "Epoch [8055/100000], Validation Loss: 553.1169\n",
            "Epoch [8056/100000], Validation Loss: 549.4338\n",
            "Epoch [8057/100000], Validation Loss: 550.1014\n",
            "Epoch [8058/100000], Validation Loss: 551.6472\n",
            "Epoch [8059/100000], Validation Loss: 551.3022\n",
            "Epoch [8060/100000], Validation Loss: 551.7929\n",
            "Epoch [8061/100000], Validation Loss: 548.1749\n",
            "Epoch [8062/100000], Validation Loss: 555.0055\n",
            "Epoch [8063/100000], Validation Loss: 546.7158\n",
            "Epoch [8064/100000], Validation Loss: 545.8796\n",
            "Epoch [8065/100000], Validation Loss: 544.0170\n",
            "Epoch [8066/100000], Validation Loss: 549.5908\n",
            "Epoch [8067/100000], Validation Loss: 548.7350\n",
            "Epoch [8068/100000], Validation Loss: 553.4264\n",
            "Epoch [8069/100000], Validation Loss: 551.1136\n",
            "Epoch [8070/100000], Validation Loss: 549.2188\n",
            "Epoch [8071/100000], Validation Loss: 553.3741\n",
            "Epoch [8072/100000], Validation Loss: 548.8066\n",
            "Epoch [8073/100000], Validation Loss: 544.0574\n",
            "Epoch [8074/100000], Validation Loss: 544.2757\n",
            "Epoch [8075/100000], Validation Loss: 554.9880\n",
            "Epoch [8076/100000], Validation Loss: 548.0096\n",
            "Epoch [8077/100000], Validation Loss: 547.2614\n",
            "Epoch [8078/100000], Validation Loss: 550.8716\n",
            "Epoch [8079/100000], Validation Loss: 549.5510\n",
            "Epoch [8080/100000], Validation Loss: 551.6754\n",
            "Epoch [8081/100000], Validation Loss: 548.4238\n",
            "Epoch [8082/100000], Validation Loss: 582.1869\n",
            "Epoch [8083/100000], Validation Loss: 554.8556\n",
            "Epoch [8084/100000], Validation Loss: 545.9249\n",
            "Epoch [8085/100000], Validation Loss: 544.5775\n",
            "Epoch [8086/100000], Validation Loss: 551.0220\n",
            "Epoch [8087/100000], Validation Loss: 549.8793\n",
            "Epoch [8088/100000], Validation Loss: 549.1622\n",
            "Epoch [8089/100000], Validation Loss: 548.2533\n",
            "Epoch [8090/100000], Validation Loss: 546.8517\n",
            "Epoch [8091/100000], Validation Loss: 545.6757\n",
            "Epoch [8092/100000], Validation Loss: 546.9386\n",
            "Epoch [8093/100000], Validation Loss: 546.2352\n",
            "Epoch [8094/100000], Validation Loss: 546.5561\n",
            "Epoch [8095/100000], Validation Loss: 550.9690\n",
            "Epoch [8096/100000], Validation Loss: 548.6762\n",
            "Epoch [8097/100000], Validation Loss: 544.9238\n",
            "Epoch [8098/100000], Validation Loss: 553.8302\n",
            "Epoch [8099/100000], Validation Loss: 549.5423\n",
            "Epoch [8100/100000], Validation Loss: 547.7068\n",
            "Epoch [8101/100000], Validation Loss: 548.4852\n",
            "Epoch [8102/100000], Validation Loss: 546.4535\n",
            "Epoch [8103/100000], Validation Loss: 544.4908\n",
            "Epoch [8104/100000], Validation Loss: 545.7909\n",
            "Epoch [8105/100000], Validation Loss: 553.3017\n",
            "Epoch [8106/100000], Validation Loss: 547.3951\n",
            "Epoch [8107/100000], Validation Loss: 548.9238\n",
            "Epoch [8108/100000], Validation Loss: 545.7347\n",
            "Epoch [8109/100000], Validation Loss: 548.7814\n",
            "Epoch [8110/100000], Validation Loss: 546.5564\n",
            "Epoch [8111/100000], Validation Loss: 547.3707\n",
            "Epoch [8112/100000], Validation Loss: 545.2617\n",
            "Epoch [8113/100000], Validation Loss: 544.7848\n",
            "Epoch [8114/100000], Validation Loss: 554.4785\n",
            "Epoch [8115/100000], Validation Loss: 546.2359\n",
            "Epoch [8116/100000], Validation Loss: 545.4482\n",
            "Epoch [8117/100000], Validation Loss: 546.2148\n",
            "Epoch [8118/100000], Validation Loss: 548.4508\n",
            "Epoch [8119/100000], Validation Loss: 549.8103\n",
            "Epoch [8120/100000], Validation Loss: 553.9313\n",
            "Epoch [8121/100000], Validation Loss: 565.7651\n",
            "Epoch [8122/100000], Validation Loss: 553.0908\n",
            "Epoch [8123/100000], Validation Loss: 551.1710\n",
            "Epoch [8124/100000], Validation Loss: 548.7198\n",
            "Epoch [8125/100000], Validation Loss: 547.1145\n",
            "Epoch [8126/100000], Validation Loss: 549.2133\n",
            "Epoch [8127/100000], Validation Loss: 555.4546\n",
            "Epoch [8128/100000], Validation Loss: 545.9420\n",
            "Epoch [8129/100000], Validation Loss: 546.9548\n",
            "Epoch [8130/100000], Validation Loss: 553.1140\n",
            "Epoch [8131/100000], Validation Loss: 548.2674\n",
            "Epoch [8132/100000], Validation Loss: 549.7032\n",
            "Epoch [8133/100000], Validation Loss: 548.3348\n",
            "Epoch [8134/100000], Validation Loss: 550.1218\n",
            "Epoch [8135/100000], Validation Loss: 553.9262\n",
            "Epoch [8136/100000], Validation Loss: 569.2726\n",
            "Epoch [8137/100000], Validation Loss: 553.4202\n",
            "Epoch [8138/100000], Validation Loss: 557.6107\n",
            "Epoch [8139/100000], Validation Loss: 573.6718\n",
            "Epoch [8140/100000], Validation Loss: 551.8477\n",
            "Epoch [8141/100000], Validation Loss: 549.7346\n",
            "Epoch [8142/100000], Validation Loss: 546.8411\n",
            "Epoch [8143/100000], Validation Loss: 552.0354\n",
            "Epoch [8144/100000], Validation Loss: 551.9848\n",
            "Epoch [8145/100000], Validation Loss: 545.2161\n",
            "Epoch [8146/100000], Validation Loss: 544.9046\n",
            "Epoch [8147/100000], Validation Loss: 545.2607\n",
            "Epoch [8148/100000], Validation Loss: 547.2854\n",
            "Epoch [8149/100000], Validation Loss: 548.7180\n",
            "Epoch [8150/100000], Validation Loss: 550.0584\n",
            "Epoch [8151/100000], Validation Loss: 546.8049\n",
            "Epoch [8152/100000], Validation Loss: 553.9395\n",
            "Epoch [8153/100000], Validation Loss: 547.0035\n",
            "Epoch [8154/100000], Validation Loss: 553.6082\n",
            "Epoch [8155/100000], Validation Loss: 546.2878\n",
            "Epoch [8156/100000], Validation Loss: 552.0147\n",
            "Epoch [8157/100000], Validation Loss: 559.6519\n",
            "Epoch [8158/100000], Validation Loss: 548.3689\n",
            "Epoch [8159/100000], Validation Loss: 544.1855\n",
            "Epoch [8160/100000], Validation Loss: 554.5851\n",
            "Epoch [8161/100000], Validation Loss: 549.6256\n",
            "Epoch [8162/100000], Validation Loss: 546.7611\n",
            "Epoch [8163/100000], Validation Loss: 545.9974\n",
            "Epoch [8164/100000], Validation Loss: 548.6617\n",
            "Epoch [8165/100000], Validation Loss: 551.0455\n",
            "Epoch [8166/100000], Validation Loss: 550.8752\n",
            "Epoch [8167/100000], Validation Loss: 548.8016\n",
            "Epoch [8168/100000], Validation Loss: 547.6882\n",
            "Epoch [8169/100000], Validation Loss: 547.1942\n",
            "Epoch [8170/100000], Validation Loss: 547.6335\n",
            "Epoch [8171/100000], Validation Loss: 549.1841\n",
            "Epoch [8172/100000], Validation Loss: 548.7957\n",
            "Epoch [8173/100000], Validation Loss: 550.7717\n",
            "Epoch [8174/100000], Validation Loss: 558.1260\n",
            "Epoch [8175/100000], Validation Loss: 548.3529\n",
            "Epoch [8176/100000], Validation Loss: 552.4381\n",
            "Epoch [8177/100000], Validation Loss: 547.6657\n",
            "Epoch [8178/100000], Validation Loss: 553.9222\n",
            "Epoch [8179/100000], Validation Loss: 572.6464\n",
            "Epoch [8180/100000], Validation Loss: 550.0919\n",
            "Epoch [8181/100000], Validation Loss: 552.3415\n",
            "Epoch [8182/100000], Validation Loss: 549.1746\n",
            "Epoch [8183/100000], Validation Loss: 553.0765\n",
            "Epoch [8184/100000], Validation Loss: 551.6780\n",
            "Epoch [8185/100000], Validation Loss: 548.4821\n",
            "Epoch [8186/100000], Validation Loss: 550.8928\n",
            "Epoch [8187/100000], Validation Loss: 550.5978\n",
            "Epoch [8188/100000], Validation Loss: 548.8774\n",
            "Epoch [8189/100000], Validation Loss: 553.6350\n",
            "Epoch [8190/100000], Validation Loss: 557.4888\n",
            "Epoch [8191/100000], Validation Loss: 547.9179\n",
            "Epoch [8192/100000], Validation Loss: 554.0255\n",
            "Epoch [8193/100000], Validation Loss: 551.4385\n",
            "Epoch [8194/100000], Validation Loss: 554.1586\n",
            "Epoch [8195/100000], Validation Loss: 551.9764\n",
            "Epoch [8196/100000], Validation Loss: 551.2182\n",
            "Epoch [8197/100000], Validation Loss: 551.5614\n",
            "Epoch [8198/100000], Validation Loss: 550.3451\n",
            "Epoch [8199/100000], Validation Loss: 549.1860\n",
            "Epoch [8200/100000], Validation Loss: 548.4920\n",
            "Epoch [8201/100000], Validation Loss: 545.3050\n",
            "Epoch [8202/100000], Validation Loss: 552.4048\n",
            "Epoch [8203/100000], Validation Loss: 550.4076\n",
            "Epoch [8204/100000], Validation Loss: 550.7071\n",
            "Epoch [8205/100000], Validation Loss: 553.8264\n",
            "Epoch [8206/100000], Validation Loss: 548.5247\n",
            "Epoch [8207/100000], Validation Loss: 550.3980\n",
            "Epoch [8208/100000], Validation Loss: 546.9074\n",
            "Epoch [8209/100000], Validation Loss: 553.1350\n",
            "Epoch [8210/100000], Validation Loss: 553.0369\n",
            "Epoch [8211/100000], Validation Loss: 547.8266\n",
            "Epoch [8212/100000], Validation Loss: 550.1821\n",
            "Epoch [8213/100000], Validation Loss: 547.7186\n",
            "Epoch [8214/100000], Validation Loss: 547.6594\n",
            "Epoch [8215/100000], Validation Loss: 557.5697\n",
            "Epoch [8216/100000], Validation Loss: 553.2582\n",
            "Epoch [8217/100000], Validation Loss: 555.6576\n",
            "Epoch [8218/100000], Validation Loss: 550.4905\n",
            "Epoch [8219/100000], Validation Loss: 549.8576\n",
            "Epoch [8220/100000], Validation Loss: 554.8878\n",
            "Epoch [8221/100000], Validation Loss: 548.2724\n",
            "Epoch [8222/100000], Validation Loss: 546.8218\n",
            "Epoch [8223/100000], Validation Loss: 548.1576\n",
            "Epoch [8224/100000], Validation Loss: 548.1000\n",
            "Epoch [8225/100000], Validation Loss: 547.1992\n",
            "Epoch [8226/100000], Validation Loss: 546.3003\n",
            "Epoch [8227/100000], Validation Loss: 566.6319\n",
            "Epoch [8228/100000], Validation Loss: 545.5072\n",
            "Epoch [8229/100000], Validation Loss: 544.3209\n",
            "Epoch [8230/100000], Validation Loss: 550.5716\n",
            "Epoch [8231/100000], Validation Loss: 550.7880\n",
            "Epoch [8232/100000], Validation Loss: 545.5504\n",
            "Epoch [8233/100000], Validation Loss: 548.8827\n",
            "Epoch [8234/100000], Validation Loss: 557.1901\n",
            "Epoch [8235/100000], Validation Loss: 544.7444\n",
            "Epoch [8236/100000], Validation Loss: 548.5469\n",
            "Epoch [8237/100000], Validation Loss: 551.1257\n",
            "Epoch [8238/100000], Validation Loss: 551.6777\n",
            "Epoch [8239/100000], Validation Loss: 553.1045\n",
            "Epoch [8240/100000], Validation Loss: 553.2027\n",
            "Epoch [8241/100000], Validation Loss: 550.5700\n",
            "Epoch [8242/100000], Validation Loss: 548.5382\n",
            "Epoch [8243/100000], Validation Loss: 549.6203\n",
            "Epoch [8244/100000], Validation Loss: 557.4939\n",
            "Epoch [8245/100000], Validation Loss: 550.7707\n",
            "Epoch [8246/100000], Validation Loss: 548.1919\n",
            "Epoch [8247/100000], Validation Loss: 550.3637\n",
            "Epoch [8248/100000], Validation Loss: 553.2311\n",
            "Epoch [8249/100000], Validation Loss: 548.3161\n",
            "Epoch [8250/100000], Validation Loss: 552.7005\n",
            "Epoch [8251/100000], Validation Loss: 546.8866\n",
            "Epoch [8252/100000], Validation Loss: 558.9786\n",
            "Epoch [8253/100000], Validation Loss: 551.7346\n",
            "Epoch [8254/100000], Validation Loss: 551.1263\n",
            "Epoch [8255/100000], Validation Loss: 549.5371\n",
            "Epoch [8256/100000], Validation Loss: 556.6403\n",
            "Epoch [8257/100000], Validation Loss: 549.1891\n",
            "Epoch [8258/100000], Validation Loss: 550.1410\n",
            "Epoch [8259/100000], Validation Loss: 551.9018\n",
            "Epoch [8260/100000], Validation Loss: 551.1594\n",
            "Epoch [8261/100000], Validation Loss: 546.7932\n",
            "Epoch [8262/100000], Validation Loss: 546.7631\n",
            "Epoch [8263/100000], Validation Loss: 543.9900\n",
            "Epoch [8264/100000], Validation Loss: 546.3522\n",
            "Epoch [8265/100000], Validation Loss: 547.4109\n",
            "Epoch [8266/100000], Validation Loss: 564.5498\n",
            "Epoch [8267/100000], Validation Loss: 547.6140\n",
            "Epoch [8268/100000], Validation Loss: 546.3411\n",
            "Epoch [8269/100000], Validation Loss: 545.1460\n",
            "Epoch [8270/100000], Validation Loss: 550.1831\n",
            "Epoch [8271/100000], Validation Loss: 550.4297\n",
            "Epoch [8272/100000], Validation Loss: 550.0200\n",
            "Epoch [8273/100000], Validation Loss: 552.4482\n",
            "Epoch [8274/100000], Validation Loss: 549.1318\n",
            "Epoch [8275/100000], Validation Loss: 550.5204\n",
            "Epoch [8276/100000], Validation Loss: 548.0828\n",
            "Epoch [8277/100000], Validation Loss: 547.3390\n",
            "Epoch [8278/100000], Validation Loss: 551.0601\n",
            "Epoch [8279/100000], Validation Loss: 550.7607\n",
            "Epoch [8280/100000], Validation Loss: 552.9544\n",
            "Epoch [8281/100000], Validation Loss: 546.3165\n",
            "Epoch [8282/100000], Validation Loss: 550.6745\n",
            "Epoch [8283/100000], Validation Loss: 549.8363\n",
            "Epoch [8284/100000], Validation Loss: 545.7831\n",
            "Epoch [8285/100000], Validation Loss: 547.1855\n",
            "Epoch [8286/100000], Validation Loss: 550.5307\n",
            "Epoch [8287/100000], Validation Loss: 546.5450\n",
            "Epoch [8288/100000], Validation Loss: 548.1075\n",
            "Epoch [8289/100000], Validation Loss: 546.2331\n",
            "Epoch [8290/100000], Validation Loss: 548.4777\n",
            "Epoch [8291/100000], Validation Loss: 545.9256\n",
            "Epoch [8292/100000], Validation Loss: 550.4411\n",
            "Epoch [8293/100000], Validation Loss: 572.5844\n",
            "Epoch [8294/100000], Validation Loss: 569.3950\n",
            "Epoch [8295/100000], Validation Loss: 552.1299\n",
            "Epoch [8296/100000], Validation Loss: 549.6234\n",
            "Epoch [8297/100000], Validation Loss: 546.2184\n",
            "Epoch [8298/100000], Validation Loss: 551.8601\n",
            "Epoch [8299/100000], Validation Loss: 550.7568\n",
            "Epoch [8300/100000], Validation Loss: 546.9603\n",
            "Epoch [8301/100000], Validation Loss: 552.1725\n",
            "Epoch [8302/100000], Validation Loss: 547.7226\n",
            "Epoch [8303/100000], Validation Loss: 551.1145\n",
            "Epoch [8304/100000], Validation Loss: 551.0081\n",
            "Epoch [8305/100000], Validation Loss: 550.5677\n",
            "Epoch [8306/100000], Validation Loss: 555.2800\n",
            "Epoch [8307/100000], Validation Loss: 549.9946\n",
            "Epoch [8308/100000], Validation Loss: 550.3424\n",
            "Epoch [8309/100000], Validation Loss: 554.1689\n",
            "Epoch [8310/100000], Validation Loss: 545.0741\n",
            "Epoch [8311/100000], Validation Loss: 547.8746\n",
            "Epoch [8312/100000], Validation Loss: 560.0183\n",
            "Epoch [8313/100000], Validation Loss: 549.1041\n",
            "Epoch [8314/100000], Validation Loss: 549.9680\n",
            "Epoch [8315/100000], Validation Loss: 558.8748\n",
            "Epoch [8316/100000], Validation Loss: 549.0945\n",
            "Epoch [8317/100000], Validation Loss: 549.5733\n",
            "Epoch [8318/100000], Validation Loss: 551.2913\n",
            "Epoch [8319/100000], Validation Loss: 559.8914\n",
            "Epoch [8320/100000], Validation Loss: 552.6561\n",
            "Epoch [8321/100000], Validation Loss: 563.0765\n",
            "Epoch [8322/100000], Validation Loss: 550.9226\n",
            "Epoch [8323/100000], Validation Loss: 553.9817\n",
            "Epoch [8324/100000], Validation Loss: 548.9203\n",
            "Epoch [8325/100000], Validation Loss: 546.0695\n",
            "Epoch [8326/100000], Validation Loss: 548.5952\n",
            "Epoch [8327/100000], Validation Loss: 550.2601\n",
            "Epoch [8328/100000], Validation Loss: 548.3918\n",
            "Epoch [8329/100000], Validation Loss: 553.9351\n",
            "Epoch [8330/100000], Validation Loss: 546.2588\n",
            "Epoch [8331/100000], Validation Loss: 548.6332\n",
            "Epoch [8332/100000], Validation Loss: 548.8739\n",
            "Epoch [8333/100000], Validation Loss: 548.1187\n",
            "Epoch [8334/100000], Validation Loss: 552.2638\n",
            "Epoch [8335/100000], Validation Loss: 551.1749\n",
            "Epoch [8336/100000], Validation Loss: 548.6402\n",
            "Epoch [8337/100000], Validation Loss: 548.3128\n",
            "Epoch [8338/100000], Validation Loss: 543.0456\n",
            "Epoch [8339/100000], Validation Loss: 552.7168\n",
            "Epoch [8340/100000], Validation Loss: 550.5316\n",
            "Epoch [8341/100000], Validation Loss: 552.1835\n",
            "Epoch [8342/100000], Validation Loss: 551.9196\n",
            "Epoch [8343/100000], Validation Loss: 551.9166\n",
            "Epoch [8344/100000], Validation Loss: 545.2577\n",
            "Epoch [8345/100000], Validation Loss: 549.0935\n",
            "Epoch [8346/100000], Validation Loss: 545.6981\n",
            "Epoch [8347/100000], Validation Loss: 551.0891\n",
            "Epoch [8348/100000], Validation Loss: 556.1704\n",
            "Epoch [8349/100000], Validation Loss: 545.1254\n",
            "Epoch [8350/100000], Validation Loss: 547.3347\n",
            "Epoch [8351/100000], Validation Loss: 547.4848\n",
            "Epoch [8352/100000], Validation Loss: 553.2036\n",
            "Epoch [8353/100000], Validation Loss: 550.2972\n",
            "Epoch [8354/100000], Validation Loss: 547.7033\n",
            "Epoch [8355/100000], Validation Loss: 546.8895\n",
            "Epoch [8356/100000], Validation Loss: 557.9783\n",
            "Epoch [8357/100000], Validation Loss: 549.4252\n",
            "Epoch [8358/100000], Validation Loss: 547.3933\n",
            "Epoch [8359/100000], Validation Loss: 550.4291\n",
            "Epoch [8360/100000], Validation Loss: 549.9852\n",
            "Epoch [8361/100000], Validation Loss: 560.5633\n",
            "Epoch [8362/100000], Validation Loss: 543.9642\n",
            "Epoch [8363/100000], Validation Loss: 545.3462\n",
            "Epoch [8364/100000], Validation Loss: 549.4987\n",
            "Epoch [8365/100000], Validation Loss: 551.5648\n",
            "Epoch [8366/100000], Validation Loss: 548.2422\n",
            "Epoch [8367/100000], Validation Loss: 547.5068\n",
            "Epoch [8368/100000], Validation Loss: 549.4559\n",
            "Epoch [8369/100000], Validation Loss: 552.9864\n",
            "Epoch [8370/100000], Validation Loss: 548.0176\n",
            "Epoch [8371/100000], Validation Loss: 544.3710\n",
            "Epoch [8372/100000], Validation Loss: 555.1807\n",
            "Epoch [8373/100000], Validation Loss: 548.5462\n",
            "Epoch [8374/100000], Validation Loss: 546.4173\n",
            "Epoch [8375/100000], Validation Loss: 551.7153\n",
            "Epoch [8376/100000], Validation Loss: 547.2881\n",
            "Epoch [8377/100000], Validation Loss: 549.2709\n",
            "Epoch [8378/100000], Validation Loss: 551.3126\n",
            "Epoch [8379/100000], Validation Loss: 548.9052\n",
            "Epoch [8380/100000], Validation Loss: 550.4905\n",
            "Epoch [8381/100000], Validation Loss: 554.9346\n",
            "Epoch [8382/100000], Validation Loss: 549.9741\n",
            "Epoch [8383/100000], Validation Loss: 547.8923\n",
            "Epoch [8384/100000], Validation Loss: 548.6913\n",
            "Epoch [8385/100000], Validation Loss: 550.5291\n",
            "Epoch [8386/100000], Validation Loss: 547.8144\n",
            "Epoch [8387/100000], Validation Loss: 557.3775\n",
            "Epoch [8388/100000], Validation Loss: 550.5149\n",
            "Epoch [8389/100000], Validation Loss: 546.5887\n",
            "Epoch [8390/100000], Validation Loss: 550.7700\n",
            "Epoch [8391/100000], Validation Loss: 545.7535\n",
            "Epoch [8392/100000], Validation Loss: 550.3793\n",
            "Epoch [8393/100000], Validation Loss: 556.1366\n",
            "Epoch [8394/100000], Validation Loss: 556.5438\n",
            "Epoch [8395/100000], Validation Loss: 549.6904\n",
            "Epoch [8396/100000], Validation Loss: 551.3889\n",
            "Epoch [8397/100000], Validation Loss: 550.2833\n",
            "Epoch [8398/100000], Validation Loss: 550.1732\n",
            "Epoch [8399/100000], Validation Loss: 555.3348\n",
            "Epoch [8400/100000], Validation Loss: 548.4999\n",
            "Epoch [8401/100000], Validation Loss: 552.6964\n",
            "Epoch [8402/100000], Validation Loss: 549.3152\n",
            "Epoch [8403/100000], Validation Loss: 547.5451\n",
            "Epoch [8404/100000], Validation Loss: 543.8253\n",
            "Epoch [8405/100000], Validation Loss: 549.5059\n",
            "Epoch [8406/100000], Validation Loss: 552.2338\n",
            "Epoch [8407/100000], Validation Loss: 553.1132\n",
            "Epoch [8408/100000], Validation Loss: 550.3216\n",
            "Epoch [8409/100000], Validation Loss: 545.8956\n",
            "Epoch [8410/100000], Validation Loss: 550.6180\n",
            "Epoch [8411/100000], Validation Loss: 550.7225\n",
            "Epoch [8412/100000], Validation Loss: 550.8998\n",
            "Epoch [8413/100000], Validation Loss: 548.6128\n",
            "Epoch [8414/100000], Validation Loss: 550.6075\n",
            "Epoch [8415/100000], Validation Loss: 542.1547\n",
            "Epoch [8416/100000], Validation Loss: 548.0130\n",
            "Epoch [8417/100000], Validation Loss: 554.7603\n",
            "Epoch [8418/100000], Validation Loss: 550.7249\n",
            "Epoch [8419/100000], Validation Loss: 548.1084\n",
            "Epoch [8420/100000], Validation Loss: 546.9784\n",
            "Epoch [8421/100000], Validation Loss: 550.4376\n",
            "Epoch [8422/100000], Validation Loss: 553.9583\n",
            "Epoch [8423/100000], Validation Loss: 548.6325\n",
            "Epoch [8424/100000], Validation Loss: 550.2380\n",
            "Epoch [8425/100000], Validation Loss: 551.4978\n",
            "Epoch [8426/100000], Validation Loss: 545.8411\n",
            "Epoch [8427/100000], Validation Loss: 551.1619\n",
            "Epoch [8428/100000], Validation Loss: 556.5574\n",
            "Epoch [8429/100000], Validation Loss: 548.2408\n",
            "Epoch [8430/100000], Validation Loss: 551.2570\n",
            "Epoch [8431/100000], Validation Loss: 556.5337\n",
            "Epoch [8432/100000], Validation Loss: 549.4596\n",
            "Epoch [8433/100000], Validation Loss: 547.3225\n",
            "Epoch [8434/100000], Validation Loss: 545.5564\n",
            "Epoch [8435/100000], Validation Loss: 546.4893\n",
            "Epoch [8436/100000], Validation Loss: 548.3758\n",
            "Epoch [8437/100000], Validation Loss: 544.3088\n",
            "Epoch [8438/100000], Validation Loss: 550.4278\n",
            "Epoch [8439/100000], Validation Loss: 549.0972\n",
            "Epoch [8440/100000], Validation Loss: 548.4936\n",
            "Epoch [8441/100000], Validation Loss: 546.7541\n",
            "Epoch [8442/100000], Validation Loss: 549.9946\n",
            "Epoch [8443/100000], Validation Loss: 550.4231\n",
            "Epoch [8444/100000], Validation Loss: 549.4902\n",
            "Epoch [8445/100000], Validation Loss: 554.9802\n",
            "Epoch [8446/100000], Validation Loss: 555.4702\n",
            "Epoch [8447/100000], Validation Loss: 548.9993\n",
            "Epoch [8448/100000], Validation Loss: 553.2499\n",
            "Epoch [8449/100000], Validation Loss: 552.3170\n",
            "Epoch [8450/100000], Validation Loss: 558.3243\n",
            "Epoch [8451/100000], Validation Loss: 551.0859\n",
            "Epoch [8452/100000], Validation Loss: 549.0786\n",
            "Epoch [8453/100000], Validation Loss: 553.1789\n",
            "Epoch [8454/100000], Validation Loss: 546.8997\n",
            "Epoch [8455/100000], Validation Loss: 548.0494\n",
            "Epoch [8456/100000], Validation Loss: 549.9443\n",
            "Epoch [8457/100000], Validation Loss: 551.1708\n",
            "Epoch [8458/100000], Validation Loss: 548.3299\n",
            "Epoch [8459/100000], Validation Loss: 550.5666\n",
            "Epoch [8460/100000], Validation Loss: 553.7499\n",
            "Epoch [8461/100000], Validation Loss: 547.2974\n",
            "Epoch [8462/100000], Validation Loss: 550.2189\n",
            "Epoch [8463/100000], Validation Loss: 547.5850\n",
            "Epoch [8464/100000], Validation Loss: 553.4815\n",
            "Epoch [8465/100000], Validation Loss: 555.0208\n",
            "Epoch [8466/100000], Validation Loss: 546.6620\n",
            "Epoch [8467/100000], Validation Loss: 550.4247\n",
            "Epoch [8468/100000], Validation Loss: 550.1199\n",
            "Epoch [8469/100000], Validation Loss: 548.8663\n",
            "Epoch [8470/100000], Validation Loss: 549.7614\n",
            "Epoch [8471/100000], Validation Loss: 550.5770\n",
            "Epoch [8472/100000], Validation Loss: 554.0752\n",
            "Epoch [8473/100000], Validation Loss: 551.4197\n",
            "Epoch [8474/100000], Validation Loss: 547.1453\n",
            "Epoch [8475/100000], Validation Loss: 548.6206\n",
            "Epoch [8476/100000], Validation Loss: 550.7012\n",
            "Epoch [8477/100000], Validation Loss: 545.5566\n",
            "Epoch [8478/100000], Validation Loss: 548.4988\n",
            "Epoch [8479/100000], Validation Loss: 544.7678\n",
            "Epoch [8480/100000], Validation Loss: 547.9184\n",
            "Epoch [8481/100000], Validation Loss: 544.0841\n",
            "Epoch [8482/100000], Validation Loss: 548.5273\n",
            "Epoch [8483/100000], Validation Loss: 548.1003\n",
            "Epoch [8484/100000], Validation Loss: 550.0527\n",
            "Epoch [8485/100000], Validation Loss: 553.1458\n",
            "Epoch [8486/100000], Validation Loss: 550.6495\n",
            "Epoch [8487/100000], Validation Loss: 549.3067\n",
            "Epoch [8488/100000], Validation Loss: 548.3470\n",
            "Epoch [8489/100000], Validation Loss: 548.9550\n",
            "Epoch [8490/100000], Validation Loss: 545.9928\n",
            "Epoch [8491/100000], Validation Loss: 557.4631\n",
            "Epoch [8492/100000], Validation Loss: 573.9576\n",
            "Epoch [8493/100000], Validation Loss: 549.7714\n",
            "Epoch [8494/100000], Validation Loss: 546.1573\n",
            "Epoch [8495/100000], Validation Loss: 548.9999\n",
            "Epoch [8496/100000], Validation Loss: 550.7458\n",
            "Epoch [8497/100000], Validation Loss: 552.4499\n",
            "Epoch [8498/100000], Validation Loss: 544.1605\n",
            "Epoch [8499/100000], Validation Loss: 546.2714\n",
            "Epoch [8500/100000], Validation Loss: 549.0300\n",
            "Epoch [8501/100000], Validation Loss: 548.5646\n",
            "Epoch [8502/100000], Validation Loss: 548.4941\n",
            "Epoch [8503/100000], Validation Loss: 548.7280\n",
            "Epoch [8504/100000], Validation Loss: 548.1797\n",
            "Epoch [8505/100000], Validation Loss: 550.1997\n",
            "Epoch [8506/100000], Validation Loss: 550.5230\n",
            "Epoch [8507/100000], Validation Loss: 549.9224\n",
            "Epoch [8508/100000], Validation Loss: 550.7959\n",
            "Epoch [8509/100000], Validation Loss: 551.9908\n",
            "Epoch [8510/100000], Validation Loss: 551.2252\n",
            "Epoch [8511/100000], Validation Loss: 557.0823\n",
            "Epoch [8512/100000], Validation Loss: 550.0044\n",
            "Epoch [8513/100000], Validation Loss: 547.0246\n",
            "Epoch [8514/100000], Validation Loss: 547.5689\n",
            "Epoch [8515/100000], Validation Loss: 549.1034\n",
            "Epoch [8516/100000], Validation Loss: 552.9854\n",
            "Epoch [8517/100000], Validation Loss: 549.3240\n",
            "Epoch [8518/100000], Validation Loss: 547.7914\n",
            "Epoch [8519/100000], Validation Loss: 547.4656\n",
            "Epoch [8520/100000], Validation Loss: 556.5731\n",
            "Epoch [8521/100000], Validation Loss: 552.7182\n",
            "Epoch [8522/100000], Validation Loss: 558.0131\n",
            "Epoch [8523/100000], Validation Loss: 553.0314\n",
            "Epoch [8524/100000], Validation Loss: 548.8364\n",
            "Epoch [8525/100000], Validation Loss: 545.3032\n",
            "Epoch [8526/100000], Validation Loss: 553.9606\n",
            "Epoch [8527/100000], Validation Loss: 548.8807\n",
            "Epoch [8528/100000], Validation Loss: 549.1235\n",
            "Epoch [8529/100000], Validation Loss: 555.7428\n",
            "Epoch [8530/100000], Validation Loss: 551.9607\n",
            "Epoch [8531/100000], Validation Loss: 544.8637\n",
            "Epoch [8532/100000], Validation Loss: 550.4839\n",
            "Epoch [8533/100000], Validation Loss: 546.0420\n",
            "Epoch [8534/100000], Validation Loss: 544.0839\n",
            "Epoch [8535/100000], Validation Loss: 551.4166\n",
            "Epoch [8536/100000], Validation Loss: 552.7102\n",
            "Epoch [8537/100000], Validation Loss: 549.7075\n",
            "Epoch [8538/100000], Validation Loss: 549.1041\n",
            "Epoch [8539/100000], Validation Loss: 549.1583\n",
            "Epoch [8540/100000], Validation Loss: 547.3878\n",
            "Epoch [8541/100000], Validation Loss: 546.7184\n",
            "Epoch [8542/100000], Validation Loss: 547.4136\n",
            "Epoch [8543/100000], Validation Loss: 547.4748\n",
            "Epoch [8544/100000], Validation Loss: 549.2918\n",
            "Epoch [8545/100000], Validation Loss: 547.2371\n",
            "Epoch [8546/100000], Validation Loss: 545.8997\n",
            "Epoch [8547/100000], Validation Loss: 549.7699\n",
            "Epoch [8548/100000], Validation Loss: 550.3602\n",
            "Epoch [8549/100000], Validation Loss: 548.9348\n",
            "Epoch [8550/100000], Validation Loss: 549.7464\n",
            "Epoch [8551/100000], Validation Loss: 546.2120\n",
            "Epoch [8552/100000], Validation Loss: 547.9671\n",
            "Epoch [8553/100000], Validation Loss: 549.8236\n",
            "Epoch [8554/100000], Validation Loss: 561.1435\n",
            "Epoch [8555/100000], Validation Loss: 551.6621\n",
            "Epoch [8556/100000], Validation Loss: 557.8964\n",
            "Epoch [8557/100000], Validation Loss: 547.7357\n",
            "Epoch [8558/100000], Validation Loss: 549.6899\n",
            "Epoch [8559/100000], Validation Loss: 546.6435\n",
            "Epoch [8560/100000], Validation Loss: 545.4345\n",
            "Epoch [8561/100000], Validation Loss: 551.6139\n",
            "Epoch [8562/100000], Validation Loss: 550.4486\n",
            "Epoch [8563/100000], Validation Loss: 552.8844\n",
            "Epoch [8564/100000], Validation Loss: 549.6618\n",
            "Epoch [8565/100000], Validation Loss: 546.3692\n",
            "Epoch [8566/100000], Validation Loss: 553.9668\n",
            "Epoch [8567/100000], Validation Loss: 547.9787\n",
            "Epoch [8568/100000], Validation Loss: 547.8392\n",
            "Epoch [8569/100000], Validation Loss: 552.5697\n",
            "Epoch [8570/100000], Validation Loss: 555.9548\n",
            "Epoch [8571/100000], Validation Loss: 549.9906\n",
            "Epoch [8572/100000], Validation Loss: 548.3385\n",
            "Epoch [8573/100000], Validation Loss: 547.3863\n",
            "Epoch [8574/100000], Validation Loss: 548.8852\n",
            "Epoch [8575/100000], Validation Loss: 550.4085\n",
            "Epoch [8576/100000], Validation Loss: 546.5168\n",
            "Epoch [8577/100000], Validation Loss: 548.7317\n",
            "Epoch [8578/100000], Validation Loss: 548.6068\n",
            "Epoch [8579/100000], Validation Loss: 548.7487\n",
            "Epoch [8580/100000], Validation Loss: 548.9265\n",
            "Epoch [8581/100000], Validation Loss: 545.2520\n",
            "Epoch [8582/100000], Validation Loss: 550.0409\n",
            "Epoch [8583/100000], Validation Loss: 547.5958\n",
            "Epoch [8584/100000], Validation Loss: 552.0883\n",
            "Epoch [8585/100000], Validation Loss: 551.1531\n",
            "Epoch [8586/100000], Validation Loss: 549.0347\n",
            "Epoch [8587/100000], Validation Loss: 559.3175\n",
            "Epoch [8588/100000], Validation Loss: 547.5533\n",
            "Epoch [8589/100000], Validation Loss: 547.7366\n",
            "Epoch [8590/100000], Validation Loss: 545.6207\n",
            "Epoch [8591/100000], Validation Loss: 551.6195\n",
            "Epoch [8592/100000], Validation Loss: 550.1274\n",
            "Epoch [8593/100000], Validation Loss: 548.6449\n",
            "Epoch [8594/100000], Validation Loss: 547.9062\n",
            "Epoch [8595/100000], Validation Loss: 548.1917\n",
            "Epoch [8596/100000], Validation Loss: 552.9908\n",
            "Epoch [8597/100000], Validation Loss: 551.4505\n",
            "Epoch [8598/100000], Validation Loss: 547.6273\n",
            "Epoch [8599/100000], Validation Loss: 546.7387\n",
            "Epoch [8600/100000], Validation Loss: 555.0681\n",
            "Epoch [8601/100000], Validation Loss: 554.3742\n",
            "Epoch [8602/100000], Validation Loss: 559.0351\n",
            "Epoch [8603/100000], Validation Loss: 551.8516\n",
            "Epoch [8604/100000], Validation Loss: 548.0614\n",
            "Epoch [8605/100000], Validation Loss: 551.3467\n",
            "Epoch [8606/100000], Validation Loss: 547.5452\n",
            "Epoch [8607/100000], Validation Loss: 552.2571\n",
            "Epoch [8608/100000], Validation Loss: 553.8733\n",
            "Epoch [8609/100000], Validation Loss: 546.8943\n",
            "Epoch [8610/100000], Validation Loss: 550.3243\n",
            "Epoch [8611/100000], Validation Loss: 557.9126\n",
            "Epoch [8612/100000], Validation Loss: 590.8517\n",
            "Epoch [8613/100000], Validation Loss: 547.4750\n",
            "Epoch [8614/100000], Validation Loss: 553.3550\n",
            "Epoch [8615/100000], Validation Loss: 546.5900\n",
            "Epoch [8616/100000], Validation Loss: 551.1582\n",
            "Epoch [8617/100000], Validation Loss: 552.8102\n",
            "Epoch [8618/100000], Validation Loss: 550.1203\n",
            "Epoch [8619/100000], Validation Loss: 553.2845\n",
            "Epoch [8620/100000], Validation Loss: 553.9866\n",
            "Epoch [8621/100000], Validation Loss: 551.0552\n",
            "Epoch [8622/100000], Validation Loss: 551.5848\n",
            "Epoch [8623/100000], Validation Loss: 546.2440\n",
            "Epoch [8624/100000], Validation Loss: 547.8548\n",
            "Epoch [8625/100000], Validation Loss: 551.9138\n",
            "Epoch [8626/100000], Validation Loss: 554.1655\n",
            "Epoch [8627/100000], Validation Loss: 556.1467\n",
            "Epoch [8628/100000], Validation Loss: 546.3923\n",
            "Epoch [8629/100000], Validation Loss: 555.4680\n",
            "Epoch [8630/100000], Validation Loss: 549.8954\n",
            "Epoch [8631/100000], Validation Loss: 548.6576\n",
            "Epoch [8632/100000], Validation Loss: 547.9810\n",
            "Epoch [8633/100000], Validation Loss: 549.6947\n",
            "Epoch [8634/100000], Validation Loss: 545.5631\n",
            "Epoch [8635/100000], Validation Loss: 549.2563\n",
            "Epoch [8636/100000], Validation Loss: 545.0501\n",
            "Epoch [8637/100000], Validation Loss: 549.2860\n",
            "Epoch [8638/100000], Validation Loss: 546.2941\n",
            "Epoch [8639/100000], Validation Loss: 555.0029\n",
            "Epoch [8640/100000], Validation Loss: 552.1282\n",
            "Epoch [8641/100000], Validation Loss: 553.2526\n",
            "Epoch [8642/100000], Validation Loss: 550.5976\n",
            "Epoch [8643/100000], Validation Loss: 549.7396\n",
            "Epoch [8644/100000], Validation Loss: 545.1845\n",
            "Epoch [8645/100000], Validation Loss: 551.4133\n",
            "Epoch [8646/100000], Validation Loss: 549.9380\n",
            "Epoch [8647/100000], Validation Loss: 547.5756\n",
            "Epoch [8648/100000], Validation Loss: 545.6450\n",
            "Epoch [8649/100000], Validation Loss: 554.4414\n",
            "Epoch [8650/100000], Validation Loss: 548.4095\n",
            "Epoch [8651/100000], Validation Loss: 553.7408\n",
            "Epoch [8652/100000], Validation Loss: 546.4427\n",
            "Epoch [8653/100000], Validation Loss: 552.0618\n",
            "Epoch [8654/100000], Validation Loss: 552.8239\n",
            "Epoch [8655/100000], Validation Loss: 555.5666\n",
            "Epoch [8656/100000], Validation Loss: 553.2536\n",
            "Epoch [8657/100000], Validation Loss: 548.4398\n",
            "Epoch [8658/100000], Validation Loss: 555.4602\n",
            "Epoch [8659/100000], Validation Loss: 550.0138\n",
            "Epoch [8660/100000], Validation Loss: 552.2158\n",
            "Epoch [8661/100000], Validation Loss: 547.3517\n",
            "Epoch [8662/100000], Validation Loss: 547.3458\n",
            "Epoch [8663/100000], Validation Loss: 550.7246\n",
            "Epoch [8664/100000], Validation Loss: 555.2839\n",
            "Epoch [8665/100000], Validation Loss: 551.2909\n",
            "Epoch [8666/100000], Validation Loss: 549.3253\n",
            "Epoch [8667/100000], Validation Loss: 552.6656\n",
            "Epoch [8668/100000], Validation Loss: 545.3032\n",
            "Epoch [8669/100000], Validation Loss: 549.2860\n",
            "Epoch [8670/100000], Validation Loss: 555.0320\n",
            "Epoch [8671/100000], Validation Loss: 549.6298\n",
            "Epoch [8672/100000], Validation Loss: 552.0237\n",
            "Epoch [8673/100000], Validation Loss: 545.6599\n",
            "Epoch [8674/100000], Validation Loss: 550.6919\n",
            "Epoch [8675/100000], Validation Loss: 546.9247\n",
            "Epoch [8676/100000], Validation Loss: 549.4342\n",
            "Epoch [8677/100000], Validation Loss: 552.5612\n",
            "Epoch [8678/100000], Validation Loss: 546.0050\n",
            "Epoch [8679/100000], Validation Loss: 547.2002\n",
            "Epoch [8680/100000], Validation Loss: 551.7529\n",
            "Epoch [8681/100000], Validation Loss: 547.4832\n",
            "Epoch [8682/100000], Validation Loss: 549.6451\n",
            "Epoch [8683/100000], Validation Loss: 554.3059\n",
            "Epoch [8684/100000], Validation Loss: 548.2983\n",
            "Epoch [8685/100000], Validation Loss: 547.2977\n",
            "Epoch [8686/100000], Validation Loss: 548.2992\n",
            "Epoch [8687/100000], Validation Loss: 554.0116\n",
            "Epoch [8688/100000], Validation Loss: 548.6718\n",
            "Epoch [8689/100000], Validation Loss: 549.0966\n",
            "Epoch [8690/100000], Validation Loss: 550.7778\n",
            "Epoch [8691/100000], Validation Loss: 549.2062\n",
            "Epoch [8692/100000], Validation Loss: 549.6708\n",
            "Epoch [8693/100000], Validation Loss: 548.7203\n",
            "Epoch [8694/100000], Validation Loss: 548.7884\n",
            "Epoch [8695/100000], Validation Loss: 547.9374\n",
            "Epoch [8696/100000], Validation Loss: 551.6521\n",
            "Epoch [8697/100000], Validation Loss: 554.8506\n",
            "Epoch [8698/100000], Validation Loss: 551.2417\n",
            "Epoch [8699/100000], Validation Loss: 556.0007\n",
            "Epoch [8700/100000], Validation Loss: 547.0899\n",
            "Epoch [8701/100000], Validation Loss: 549.5837\n",
            "Epoch [8702/100000], Validation Loss: 559.8021\n",
            "Epoch [8703/100000], Validation Loss: 545.3010\n",
            "Epoch [8704/100000], Validation Loss: 552.2996\n",
            "Epoch [8705/100000], Validation Loss: 550.2398\n",
            "Epoch [8706/100000], Validation Loss: 549.2548\n",
            "Epoch [8707/100000], Validation Loss: 544.7999\n",
            "Epoch [8708/100000], Validation Loss: 549.4025\n",
            "Epoch [8709/100000], Validation Loss: 543.6911\n",
            "Epoch [8710/100000], Validation Loss: 557.3356\n",
            "Epoch [8711/100000], Validation Loss: 548.8777\n",
            "Epoch [8712/100000], Validation Loss: 547.6328\n",
            "Epoch [8713/100000], Validation Loss: 557.7172\n",
            "Epoch [8714/100000], Validation Loss: 559.8755\n",
            "Epoch [8715/100000], Validation Loss: 562.9732\n",
            "Epoch [8716/100000], Validation Loss: 549.4480\n",
            "Epoch [8717/100000], Validation Loss: 548.0042\n",
            "Epoch [8718/100000], Validation Loss: 547.9609\n",
            "Epoch [8719/100000], Validation Loss: 549.9989\n",
            "Epoch [8720/100000], Validation Loss: 561.8276\n",
            "Epoch [8721/100000], Validation Loss: 548.3508\n",
            "Epoch [8722/100000], Validation Loss: 544.2801\n",
            "Epoch [8723/100000], Validation Loss: 545.4955\n",
            "Epoch [8724/100000], Validation Loss: 544.7922\n",
            "Epoch [8725/100000], Validation Loss: 546.8608\n",
            "Epoch [8726/100000], Validation Loss: 547.5964\n",
            "Epoch [8727/100000], Validation Loss: 554.2247\n",
            "Epoch [8728/100000], Validation Loss: 548.5252\n",
            "Epoch [8729/100000], Validation Loss: 551.6751\n",
            "Epoch [8730/100000], Validation Loss: 548.2491\n",
            "Epoch [8731/100000], Validation Loss: 549.2430\n",
            "Epoch [8732/100000], Validation Loss: 548.7398\n",
            "Epoch [8733/100000], Validation Loss: 550.4699\n",
            "Epoch [8734/100000], Validation Loss: 553.4445\n",
            "Epoch [8735/100000], Validation Loss: 547.8020\n",
            "Epoch [8736/100000], Validation Loss: 552.1521\n",
            "Epoch [8737/100000], Validation Loss: 549.7790\n",
            "Epoch [8738/100000], Validation Loss: 545.0406\n",
            "Epoch [8739/100000], Validation Loss: 551.8207\n",
            "Epoch [8740/100000], Validation Loss: 552.6366\n",
            "Epoch [8741/100000], Validation Loss: 551.8821\n",
            "Epoch [8742/100000], Validation Loss: 563.4038\n",
            "Epoch [8743/100000], Validation Loss: 551.3151\n",
            "Epoch [8744/100000], Validation Loss: 549.9246\n",
            "Epoch [8745/100000], Validation Loss: 545.4639\n",
            "Epoch [8746/100000], Validation Loss: 545.5742\n",
            "Epoch [8747/100000], Validation Loss: 551.1083\n",
            "Epoch [8748/100000], Validation Loss: 549.3896\n",
            "Epoch [8749/100000], Validation Loss: 546.9536\n",
            "Epoch [8750/100000], Validation Loss: 552.8169\n",
            "Epoch [8751/100000], Validation Loss: 553.2550\n",
            "Epoch [8752/100000], Validation Loss: 548.6210\n",
            "Epoch [8753/100000], Validation Loss: 549.3184\n",
            "Epoch [8754/100000], Validation Loss: 549.7970\n",
            "Epoch [8755/100000], Validation Loss: 548.1125\n",
            "Epoch [8756/100000], Validation Loss: 563.8143\n",
            "Epoch [8757/100000], Validation Loss: 556.8978\n",
            "Epoch [8758/100000], Validation Loss: 549.4370\n",
            "Epoch [8759/100000], Validation Loss: 547.3997\n",
            "Epoch [8760/100000], Validation Loss: 549.6810\n",
            "Epoch [8761/100000], Validation Loss: 552.2523\n",
            "Epoch [8762/100000], Validation Loss: 547.8695\n",
            "Epoch [8763/100000], Validation Loss: 548.0600\n",
            "Epoch [8764/100000], Validation Loss: 548.7464\n",
            "Epoch [8765/100000], Validation Loss: 547.4620\n",
            "Epoch [8766/100000], Validation Loss: 546.8375\n",
            "Epoch [8767/100000], Validation Loss: 555.6861\n",
            "Epoch [8768/100000], Validation Loss: 547.5842\n",
            "Epoch [8769/100000], Validation Loss: 559.4213\n",
            "Epoch [8770/100000], Validation Loss: 552.0390\n",
            "Epoch [8771/100000], Validation Loss: 549.9225\n",
            "Epoch [8772/100000], Validation Loss: 548.3870\n",
            "Epoch [8773/100000], Validation Loss: 549.7654\n",
            "Epoch [8774/100000], Validation Loss: 547.7604\n",
            "Epoch [8775/100000], Validation Loss: 549.0055\n",
            "Epoch [8776/100000], Validation Loss: 550.5466\n",
            "Epoch [8777/100000], Validation Loss: 552.4459\n",
            "Epoch [8778/100000], Validation Loss: 547.2609\n",
            "Epoch [8779/100000], Validation Loss: 547.4798\n",
            "Epoch [8780/100000], Validation Loss: 549.0604\n",
            "Epoch [8781/100000], Validation Loss: 546.7973\n",
            "Epoch [8782/100000], Validation Loss: 548.1889\n",
            "Epoch [8783/100000], Validation Loss: 548.5127\n",
            "Epoch [8784/100000], Validation Loss: 547.8162\n",
            "Epoch [8785/100000], Validation Loss: 554.1324\n",
            "Epoch [8786/100000], Validation Loss: 551.9788\n",
            "Epoch [8787/100000], Validation Loss: 555.6685\n",
            "Epoch [8788/100000], Validation Loss: 550.4423\n",
            "Epoch [8789/100000], Validation Loss: 559.8078\n",
            "Epoch [8790/100000], Validation Loss: 548.1808\n",
            "Epoch [8791/100000], Validation Loss: 546.2940\n",
            "Epoch [8792/100000], Validation Loss: 550.1976\n",
            "Epoch [8793/100000], Validation Loss: 548.5702\n",
            "Epoch [8794/100000], Validation Loss: 544.2180\n",
            "Epoch [8795/100000], Validation Loss: 558.3081\n",
            "Epoch [8796/100000], Validation Loss: 546.0155\n",
            "Epoch [8797/100000], Validation Loss: 549.0973\n",
            "Epoch [8798/100000], Validation Loss: 548.3964\n",
            "Epoch [8799/100000], Validation Loss: 554.6461\n",
            "Epoch [8800/100000], Validation Loss: 552.8624\n",
            "Epoch [8801/100000], Validation Loss: 545.1198\n",
            "Epoch [8802/100000], Validation Loss: 549.0313\n",
            "Epoch [8803/100000], Validation Loss: 545.3519\n",
            "Epoch [8804/100000], Validation Loss: 554.1555\n",
            "Epoch [8805/100000], Validation Loss: 547.2453\n",
            "Epoch [8806/100000], Validation Loss: 550.7634\n",
            "Epoch [8807/100000], Validation Loss: 547.6856\n",
            "Epoch [8808/100000], Validation Loss: 550.1958\n",
            "Epoch [8809/100000], Validation Loss: 554.7274\n",
            "Epoch [8810/100000], Validation Loss: 550.6601\n",
            "Epoch [8811/100000], Validation Loss: 548.9482\n",
            "Epoch [8812/100000], Validation Loss: 549.4174\n",
            "Epoch [8813/100000], Validation Loss: 553.0596\n",
            "Epoch [8814/100000], Validation Loss: 555.0218\n",
            "Epoch [8815/100000], Validation Loss: 553.0124\n",
            "Epoch [8816/100000], Validation Loss: 552.2309\n",
            "Epoch [8817/100000], Validation Loss: 550.8694\n",
            "Epoch [8818/100000], Validation Loss: 550.3494\n",
            "Epoch [8819/100000], Validation Loss: 549.2358\n",
            "Epoch [8820/100000], Validation Loss: 549.4348\n",
            "Epoch [8821/100000], Validation Loss: 557.2902\n",
            "Epoch [8822/100000], Validation Loss: 554.3129\n",
            "Epoch [8823/100000], Validation Loss: 551.2120\n",
            "Epoch [8824/100000], Validation Loss: 545.5658\n",
            "Epoch [8825/100000], Validation Loss: 545.6971\n",
            "Epoch [8826/100000], Validation Loss: 547.1566\n",
            "Epoch [8827/100000], Validation Loss: 547.5209\n",
            "Epoch [8828/100000], Validation Loss: 550.0870\n",
            "Epoch [8829/100000], Validation Loss: 550.1132\n",
            "Epoch [8830/100000], Validation Loss: 550.9787\n",
            "Epoch [8831/100000], Validation Loss: 546.7097\n",
            "Epoch [8832/100000], Validation Loss: 552.3625\n",
            "Epoch [8833/100000], Validation Loss: 550.8566\n",
            "Epoch [8834/100000], Validation Loss: 551.1181\n",
            "Epoch [8835/100000], Validation Loss: 554.1043\n",
            "Epoch [8836/100000], Validation Loss: 549.9165\n",
            "Epoch [8837/100000], Validation Loss: 553.6142\n",
            "Epoch [8838/100000], Validation Loss: 548.4679\n",
            "Epoch [8839/100000], Validation Loss: 552.4816\n",
            "Epoch [8840/100000], Validation Loss: 550.1279\n",
            "Epoch [8841/100000], Validation Loss: 549.4092\n",
            "Epoch [8842/100000], Validation Loss: 548.8361\n",
            "Epoch [8843/100000], Validation Loss: 561.6040\n",
            "Epoch [8844/100000], Validation Loss: 555.2527\n",
            "Epoch [8845/100000], Validation Loss: 551.9691\n",
            "Epoch [8846/100000], Validation Loss: 547.3915\n",
            "Epoch [8847/100000], Validation Loss: 547.0836\n",
            "Epoch [8848/100000], Validation Loss: 558.0405\n",
            "Epoch [8849/100000], Validation Loss: 549.7555\n",
            "Epoch [8850/100000], Validation Loss: 549.3616\n",
            "Epoch [8851/100000], Validation Loss: 547.7924\n",
            "Epoch [8852/100000], Validation Loss: 548.3222\n",
            "Epoch [8853/100000], Validation Loss: 551.7712\n",
            "Epoch [8854/100000], Validation Loss: 554.3614\n",
            "Epoch [8855/100000], Validation Loss: 554.6633\n",
            "Epoch [8856/100000], Validation Loss: 552.4339\n",
            "Epoch [8857/100000], Validation Loss: 552.1095\n",
            "Epoch [8858/100000], Validation Loss: 549.1750\n",
            "Epoch [8859/100000], Validation Loss: 549.4048\n",
            "Epoch [8860/100000], Validation Loss: 550.0879\n",
            "Epoch [8861/100000], Validation Loss: 551.5353\n",
            "Epoch [8862/100000], Validation Loss: 548.4819\n",
            "Epoch [8863/100000], Validation Loss: 548.0223\n",
            "Epoch [8864/100000], Validation Loss: 551.4509\n",
            "Epoch [8865/100000], Validation Loss: 554.6814\n",
            "Epoch [8866/100000], Validation Loss: 549.0322\n",
            "Epoch [8867/100000], Validation Loss: 549.3190\n",
            "Epoch [8868/100000], Validation Loss: 553.1564\n",
            "Epoch [8869/100000], Validation Loss: 549.7199\n",
            "Epoch [8870/100000], Validation Loss: 552.4115\n",
            "Epoch [8871/100000], Validation Loss: 548.9069\n",
            "Epoch [8872/100000], Validation Loss: 550.6585\n",
            "Epoch [8873/100000], Validation Loss: 551.0863\n",
            "Epoch [8874/100000], Validation Loss: 553.7814\n",
            "Epoch [8875/100000], Validation Loss: 550.4434\n",
            "Epoch [8876/100000], Validation Loss: 553.0270\n",
            "Epoch [8877/100000], Validation Loss: 548.0039\n",
            "Epoch [8878/100000], Validation Loss: 548.3592\n",
            "Epoch [8879/100000], Validation Loss: 550.2853\n",
            "Epoch [8880/100000], Validation Loss: 546.9125\n",
            "Epoch [8881/100000], Validation Loss: 548.4100\n",
            "Epoch [8882/100000], Validation Loss: 552.9171\n",
            "Epoch [8883/100000], Validation Loss: 552.7686\n",
            "Epoch [8884/100000], Validation Loss: 548.8782\n",
            "Epoch [8885/100000], Validation Loss: 550.5123\n",
            "Epoch [8886/100000], Validation Loss: 553.1628\n",
            "Epoch [8887/100000], Validation Loss: 553.1974\n",
            "Epoch [8888/100000], Validation Loss: 549.9631\n",
            "Epoch [8889/100000], Validation Loss: 551.1176\n",
            "Epoch [8890/100000], Validation Loss: 548.7140\n",
            "Epoch [8891/100000], Validation Loss: 551.3679\n",
            "Epoch [8892/100000], Validation Loss: 550.8475\n",
            "Epoch [8893/100000], Validation Loss: 550.6153\n",
            "Epoch [8894/100000], Validation Loss: 554.7549\n",
            "Epoch [8895/100000], Validation Loss: 552.4667\n",
            "Epoch [8896/100000], Validation Loss: 551.3194\n",
            "Epoch [8897/100000], Validation Loss: 552.4472\n",
            "Epoch [8898/100000], Validation Loss: 550.4533\n",
            "Epoch [8899/100000], Validation Loss: 551.4127\n",
            "Epoch [8900/100000], Validation Loss: 552.3914\n",
            "Epoch [8901/100000], Validation Loss: 555.3512\n",
            "Epoch [8902/100000], Validation Loss: 562.0872\n",
            "Epoch [8903/100000], Validation Loss: 549.6273\n",
            "Epoch [8904/100000], Validation Loss: 547.4669\n",
            "Epoch [8905/100000], Validation Loss: 551.3648\n",
            "Epoch [8906/100000], Validation Loss: 546.7399\n",
            "Epoch [8907/100000], Validation Loss: 555.2524\n",
            "Epoch [8908/100000], Validation Loss: 551.2348\n",
            "Epoch [8909/100000], Validation Loss: 548.9527\n",
            "Epoch [8910/100000], Validation Loss: 558.0742\n",
            "Epoch [8911/100000], Validation Loss: 554.4008\n",
            "Epoch [8912/100000], Validation Loss: 549.1575\n",
            "Epoch [8913/100000], Validation Loss: 561.3562\n",
            "Epoch [8914/100000], Validation Loss: 547.1404\n",
            "Epoch [8915/100000], Validation Loss: 550.6255\n",
            "Epoch [8916/100000], Validation Loss: 550.8762\n",
            "Epoch [8917/100000], Validation Loss: 550.2240\n",
            "Epoch [8918/100000], Validation Loss: 551.1415\n",
            "Epoch [8919/100000], Validation Loss: 551.9019\n",
            "Epoch [8920/100000], Validation Loss: 552.4028\n",
            "Epoch [8921/100000], Validation Loss: 548.5260\n",
            "Epoch [8922/100000], Validation Loss: 547.5797\n",
            "Epoch [8923/100000], Validation Loss: 552.1470\n",
            "Epoch [8924/100000], Validation Loss: 551.2235\n",
            "Epoch [8925/100000], Validation Loss: 555.3541\n",
            "Epoch [8926/100000], Validation Loss: 550.0041\n",
            "Epoch [8927/100000], Validation Loss: 548.3201\n",
            "Epoch [8928/100000], Validation Loss: 546.6378\n",
            "Epoch [8929/100000], Validation Loss: 551.8555\n",
            "Epoch [8930/100000], Validation Loss: 550.5401\n",
            "Epoch [8931/100000], Validation Loss: 552.9375\n",
            "Epoch [8932/100000], Validation Loss: 548.7004\n",
            "Epoch [8933/100000], Validation Loss: 550.5444\n",
            "Epoch [8934/100000], Validation Loss: 549.1235\n",
            "Epoch [8935/100000], Validation Loss: 550.4817\n",
            "Epoch [8936/100000], Validation Loss: 546.7823\n",
            "Epoch [8937/100000], Validation Loss: 552.0549\n",
            "Epoch [8938/100000], Validation Loss: 547.1161\n",
            "Epoch [8939/100000], Validation Loss: 548.0660\n",
            "Epoch [8940/100000], Validation Loss: 551.4397\n",
            "Epoch [8941/100000], Validation Loss: 554.4170\n",
            "Epoch [8942/100000], Validation Loss: 546.0709\n",
            "Epoch [8943/100000], Validation Loss: 550.7143\n",
            "Epoch [8944/100000], Validation Loss: 549.3936\n",
            "Epoch [8945/100000], Validation Loss: 548.4701\n",
            "Epoch [8946/100000], Validation Loss: 550.7312\n",
            "Epoch [8947/100000], Validation Loss: 547.8949\n",
            "Epoch [8948/100000], Validation Loss: 546.1457\n",
            "Epoch [8949/100000], Validation Loss: 544.9533\n",
            "Epoch [8950/100000], Validation Loss: 547.5748\n",
            "Epoch [8951/100000], Validation Loss: 547.4244\n",
            "Epoch [8952/100000], Validation Loss: 547.7724\n",
            "Epoch [8953/100000], Validation Loss: 549.6629\n",
            "Epoch [8954/100000], Validation Loss: 551.6726\n",
            "Epoch [8955/100000], Validation Loss: 546.6193\n",
            "Epoch [8956/100000], Validation Loss: 546.2452\n",
            "Epoch [8957/100000], Validation Loss: 550.2577\n",
            "Epoch [8958/100000], Validation Loss: 549.1479\n",
            "Epoch [8959/100000], Validation Loss: 553.4375\n",
            "Epoch [8960/100000], Validation Loss: 547.6609\n",
            "Epoch [8961/100000], Validation Loss: 556.6931\n",
            "Epoch [8962/100000], Validation Loss: 550.6371\n",
            "Epoch [8963/100000], Validation Loss: 549.4463\n",
            "Epoch [8964/100000], Validation Loss: 550.5846\n",
            "Epoch [8965/100000], Validation Loss: 550.5203\n",
            "Epoch [8966/100000], Validation Loss: 551.6625\n",
            "Epoch [8967/100000], Validation Loss: 554.7781\n",
            "Epoch [8968/100000], Validation Loss: 554.8187\n",
            "Epoch [8969/100000], Validation Loss: 549.8763\n",
            "Epoch [8970/100000], Validation Loss: 552.7221\n",
            "Epoch [8971/100000], Validation Loss: 549.2531\n",
            "Epoch [8972/100000], Validation Loss: 549.1947\n",
            "Epoch [8973/100000], Validation Loss: 556.8720\n",
            "Epoch [8974/100000], Validation Loss: 549.2856\n",
            "Epoch [8975/100000], Validation Loss: 554.0060\n",
            "Epoch [8976/100000], Validation Loss: 552.2557\n",
            "Epoch [8977/100000], Validation Loss: 552.2273\n",
            "Epoch [8978/100000], Validation Loss: 549.3444\n",
            "Epoch [8979/100000], Validation Loss: 548.0625\n",
            "Epoch [8980/100000], Validation Loss: 556.2777\n",
            "Epoch [8981/100000], Validation Loss: 554.4427\n",
            "Epoch [8982/100000], Validation Loss: 554.2524\n",
            "Epoch [8983/100000], Validation Loss: 554.4112\n",
            "Epoch [8984/100000], Validation Loss: 550.1678\n",
            "Epoch [8985/100000], Validation Loss: 546.0552\n",
            "Epoch [8986/100000], Validation Loss: 551.3346\n",
            "Epoch [8987/100000], Validation Loss: 550.3167\n",
            "Epoch [8988/100000], Validation Loss: 555.9074\n",
            "Epoch [8989/100000], Validation Loss: 548.8732\n",
            "Epoch [8990/100000], Validation Loss: 551.5620\n",
            "Epoch [8991/100000], Validation Loss: 548.7240\n",
            "Epoch [8992/100000], Validation Loss: 546.3440\n",
            "Epoch [8993/100000], Validation Loss: 550.2346\n",
            "Epoch [8994/100000], Validation Loss: 551.1015\n",
            "Epoch [8995/100000], Validation Loss: 552.0012\n",
            "Epoch [8996/100000], Validation Loss: 551.7970\n",
            "Epoch [8997/100000], Validation Loss: 548.7117\n",
            "Epoch [8998/100000], Validation Loss: 552.1618\n",
            "Epoch [8999/100000], Validation Loss: 548.4022\n",
            "Epoch [9000/100000], Validation Loss: 551.9374\n",
            "Epoch [9001/100000], Validation Loss: 560.9589\n",
            "Epoch [9002/100000], Validation Loss: 550.3867\n",
            "Epoch [9003/100000], Validation Loss: 549.6670\n",
            "Epoch [9004/100000], Validation Loss: 546.1239\n",
            "Epoch [9005/100000], Validation Loss: 570.6579\n",
            "Epoch [9006/100000], Validation Loss: 548.7224\n",
            "Epoch [9007/100000], Validation Loss: 553.3522\n",
            "Epoch [9008/100000], Validation Loss: 546.0099\n",
            "Epoch [9009/100000], Validation Loss: 550.1678\n",
            "Epoch [9010/100000], Validation Loss: 550.4550\n",
            "Epoch [9011/100000], Validation Loss: 550.8641\n",
            "Epoch [9012/100000], Validation Loss: 562.3746\n",
            "Epoch [9013/100000], Validation Loss: 550.1063\n",
            "Epoch [9014/100000], Validation Loss: 550.1352\n",
            "Epoch [9015/100000], Validation Loss: 549.6402\n",
            "Epoch [9016/100000], Validation Loss: 553.0344\n",
            "Epoch [9017/100000], Validation Loss: 546.6439\n",
            "Epoch [9018/100000], Validation Loss: 546.3547\n",
            "Epoch [9019/100000], Validation Loss: 552.7677\n",
            "Epoch [9020/100000], Validation Loss: 556.1575\n",
            "Epoch [9021/100000], Validation Loss: 551.6571\n",
            "Epoch [9022/100000], Validation Loss: 548.9954\n",
            "Epoch [9023/100000], Validation Loss: 546.6003\n",
            "Epoch [9024/100000], Validation Loss: 550.8044\n",
            "Epoch [9025/100000], Validation Loss: 543.6962\n",
            "Epoch [9026/100000], Validation Loss: 546.3829\n",
            "Epoch [9027/100000], Validation Loss: 549.4217\n",
            "Epoch [9028/100000], Validation Loss: 548.6439\n",
            "Epoch [9029/100000], Validation Loss: 567.6503\n",
            "Epoch [9030/100000], Validation Loss: 559.0504\n",
            "Epoch [9031/100000], Validation Loss: 552.6036\n",
            "Epoch [9032/100000], Validation Loss: 544.5747\n",
            "Epoch [9033/100000], Validation Loss: 551.7872\n",
            "Epoch [9034/100000], Validation Loss: 550.2280\n",
            "Epoch [9035/100000], Validation Loss: 548.3854\n",
            "Epoch [9036/100000], Validation Loss: 550.7482\n",
            "Epoch [9037/100000], Validation Loss: 550.2991\n",
            "Epoch [9038/100000], Validation Loss: 548.7974\n",
            "Epoch [9039/100000], Validation Loss: 552.1686\n",
            "Epoch [9040/100000], Validation Loss: 551.4212\n",
            "Epoch [9041/100000], Validation Loss: 557.7983\n",
            "Epoch [9042/100000], Validation Loss: 559.6422\n",
            "Epoch [9043/100000], Validation Loss: 554.8428\n",
            "Epoch [9044/100000], Validation Loss: 558.0438\n",
            "Epoch [9045/100000], Validation Loss: 548.7348\n",
            "Epoch [9046/100000], Validation Loss: 555.9685\n",
            "Epoch [9047/100000], Validation Loss: 551.7449\n",
            "Epoch [9048/100000], Validation Loss: 550.0816\n",
            "Epoch [9049/100000], Validation Loss: 547.4832\n",
            "Epoch [9050/100000], Validation Loss: 557.4456\n",
            "Epoch [9051/100000], Validation Loss: 545.3178\n",
            "Epoch [9052/100000], Validation Loss: 548.6005\n",
            "Epoch [9053/100000], Validation Loss: 603.0697\n",
            "Epoch [9054/100000], Validation Loss: 553.6813\n",
            "Epoch [9055/100000], Validation Loss: 550.6546\n",
            "Epoch [9056/100000], Validation Loss: 550.7667\n",
            "Epoch [9057/100000], Validation Loss: 548.8872\n",
            "Epoch [9058/100000], Validation Loss: 550.5017\n",
            "Epoch [9059/100000], Validation Loss: 562.0559\n",
            "Epoch [9060/100000], Validation Loss: 557.2811\n",
            "Epoch [9061/100000], Validation Loss: 554.5839\n",
            "Epoch [9062/100000], Validation Loss: 548.6662\n",
            "Epoch [9063/100000], Validation Loss: 553.1268\n",
            "Epoch [9064/100000], Validation Loss: 549.7145\n",
            "Epoch [9065/100000], Validation Loss: 560.7495\n",
            "Epoch [9066/100000], Validation Loss: 548.7952\n",
            "Epoch [9067/100000], Validation Loss: 549.0682\n",
            "Epoch [9068/100000], Validation Loss: 548.5858\n",
            "Epoch [9069/100000], Validation Loss: 554.1665\n",
            "Epoch [9070/100000], Validation Loss: 550.4263\n",
            "Epoch [9071/100000], Validation Loss: 550.4354\n",
            "Epoch [9072/100000], Validation Loss: 549.4345\n",
            "Epoch [9073/100000], Validation Loss: 551.8557\n",
            "Epoch [9074/100000], Validation Loss: 554.0392\n",
            "Epoch [9075/100000], Validation Loss: 548.9590\n",
            "Epoch [9076/100000], Validation Loss: 552.9208\n",
            "Epoch [9077/100000], Validation Loss: 551.5450\n",
            "Epoch [9078/100000], Validation Loss: 554.2898\n",
            "Epoch [9079/100000], Validation Loss: 551.0896\n",
            "Epoch [9080/100000], Validation Loss: 553.8434\n",
            "Epoch [9081/100000], Validation Loss: 550.6227\n",
            "Epoch [9082/100000], Validation Loss: 549.8147\n",
            "Epoch [9083/100000], Validation Loss: 549.1953\n",
            "Epoch [9084/100000], Validation Loss: 552.3538\n",
            "Epoch [9085/100000], Validation Loss: 550.5242\n",
            "Epoch [9086/100000], Validation Loss: 551.7121\n",
            "Epoch [9087/100000], Validation Loss: 553.5842\n",
            "Epoch [9088/100000], Validation Loss: 552.1913\n",
            "Epoch [9089/100000], Validation Loss: 548.8602\n",
            "Epoch [9090/100000], Validation Loss: 552.4248\n",
            "Epoch [9091/100000], Validation Loss: 554.7723\n",
            "Epoch [9092/100000], Validation Loss: 555.7952\n",
            "Epoch [9093/100000], Validation Loss: 547.1716\n",
            "Epoch [9094/100000], Validation Loss: 551.6850\n",
            "Epoch [9095/100000], Validation Loss: 549.7745\n",
            "Epoch [9096/100000], Validation Loss: 549.5355\n",
            "Epoch [9097/100000], Validation Loss: 548.1116\n",
            "Epoch [9098/100000], Validation Loss: 552.0920\n",
            "Epoch [9099/100000], Validation Loss: 553.4339\n",
            "Epoch [9100/100000], Validation Loss: 549.6320\n",
            "Epoch [9101/100000], Validation Loss: 553.2620\n",
            "Epoch [9102/100000], Validation Loss: 550.0991\n",
            "Epoch [9103/100000], Validation Loss: 549.2245\n",
            "Epoch [9104/100000], Validation Loss: 547.5172\n",
            "Epoch [9105/100000], Validation Loss: 552.7714\n",
            "Epoch [9106/100000], Validation Loss: 569.4238\n",
            "Epoch [9107/100000], Validation Loss: 554.0321\n",
            "Epoch [9108/100000], Validation Loss: 547.8323\n",
            "Epoch [9109/100000], Validation Loss: 550.2870\n",
            "Epoch [9110/100000], Validation Loss: 548.5249\n",
            "Epoch [9111/100000], Validation Loss: 552.6613\n",
            "Epoch [9112/100000], Validation Loss: 550.4584\n",
            "Epoch [9113/100000], Validation Loss: 552.9948\n",
            "Epoch [9114/100000], Validation Loss: 554.3268\n",
            "Epoch [9115/100000], Validation Loss: 552.9074\n",
            "Epoch [9116/100000], Validation Loss: 549.3318\n",
            "Epoch [9117/100000], Validation Loss: 548.5133\n",
            "Epoch [9118/100000], Validation Loss: 546.6409\n",
            "Epoch [9119/100000], Validation Loss: 551.5014\n",
            "Epoch [9120/100000], Validation Loss: 550.8104\n",
            "Epoch [9121/100000], Validation Loss: 547.4651\n",
            "Epoch [9122/100000], Validation Loss: 546.1941\n",
            "Epoch [9123/100000], Validation Loss: 550.7032\n",
            "Epoch [9124/100000], Validation Loss: 557.9924\n",
            "Epoch [9125/100000], Validation Loss: 559.4473\n",
            "Epoch [9126/100000], Validation Loss: 565.7292\n",
            "Epoch [9127/100000], Validation Loss: 546.4125\n",
            "Epoch [9128/100000], Validation Loss: 547.6329\n",
            "Epoch [9129/100000], Validation Loss: 549.1119\n",
            "Epoch [9130/100000], Validation Loss: 552.7666\n",
            "Epoch [9131/100000], Validation Loss: 548.4140\n",
            "Epoch [9132/100000], Validation Loss: 552.3862\n",
            "Epoch [9133/100000], Validation Loss: 547.5455\n",
            "Epoch [9134/100000], Validation Loss: 552.9217\n",
            "Epoch [9135/100000], Validation Loss: 547.2183\n",
            "Epoch [9136/100000], Validation Loss: 544.0664\n",
            "Epoch [9137/100000], Validation Loss: 549.2730\n",
            "Epoch [9138/100000], Validation Loss: 554.3243\n",
            "Epoch [9139/100000], Validation Loss: 551.7274\n",
            "Epoch [9140/100000], Validation Loss: 551.1524\n",
            "Epoch [9141/100000], Validation Loss: 551.5168\n",
            "Epoch [9142/100000], Validation Loss: 545.5229\n",
            "Epoch [9143/100000], Validation Loss: 561.7446\n",
            "Epoch [9144/100000], Validation Loss: 547.1193\n",
            "Epoch [9145/100000], Validation Loss: 560.7640\n",
            "Epoch [9146/100000], Validation Loss: 548.3356\n",
            "Epoch [9147/100000], Validation Loss: 549.0398\n",
            "Epoch [9148/100000], Validation Loss: 554.4979\n",
            "Epoch [9149/100000], Validation Loss: 551.9725\n",
            "Epoch [9150/100000], Validation Loss: 552.2447\n",
            "Epoch [9151/100000], Validation Loss: 556.1869\n",
            "Epoch [9152/100000], Validation Loss: 549.8197\n",
            "Epoch [9153/100000], Validation Loss: 551.4036\n",
            "Epoch [9154/100000], Validation Loss: 549.2548\n",
            "Epoch [9155/100000], Validation Loss: 553.4098\n",
            "Epoch [9156/100000], Validation Loss: 550.0940\n",
            "Epoch [9157/100000], Validation Loss: 552.5003\n",
            "Epoch [9158/100000], Validation Loss: 553.5937\n",
            "Epoch [9159/100000], Validation Loss: 548.3337\n",
            "Epoch [9160/100000], Validation Loss: 546.6307\n",
            "Epoch [9161/100000], Validation Loss: 552.0550\n",
            "Epoch [9162/100000], Validation Loss: 548.8148\n",
            "Epoch [9163/100000], Validation Loss: 552.1891\n",
            "Epoch [9164/100000], Validation Loss: 549.7727\n",
            "Epoch [9165/100000], Validation Loss: 560.7438\n",
            "Epoch [9166/100000], Validation Loss: 547.6811\n",
            "Epoch [9167/100000], Validation Loss: 549.2737\n",
            "Epoch [9168/100000], Validation Loss: 555.6615\n",
            "Epoch [9169/100000], Validation Loss: 556.0819\n",
            "Epoch [9170/100000], Validation Loss: 552.7181\n",
            "Epoch [9171/100000], Validation Loss: 553.1223\n",
            "Epoch [9172/100000], Validation Loss: 561.2319\n",
            "Epoch [9173/100000], Validation Loss: 550.3759\n",
            "Epoch [9174/100000], Validation Loss: 552.9020\n",
            "Epoch [9175/100000], Validation Loss: 551.0272\n",
            "Epoch [9176/100000], Validation Loss: 550.0327\n",
            "Epoch [9177/100000], Validation Loss: 551.9542\n",
            "Epoch [9178/100000], Validation Loss: 552.5688\n",
            "Epoch [9179/100000], Validation Loss: 551.4060\n",
            "Epoch [9180/100000], Validation Loss: 550.3944\n",
            "Epoch [9181/100000], Validation Loss: 550.0239\n",
            "Epoch [9182/100000], Validation Loss: 555.6433\n",
            "Epoch [9183/100000], Validation Loss: 551.7126\n",
            "Epoch [9184/100000], Validation Loss: 553.3793\n",
            "Epoch [9185/100000], Validation Loss: 549.6851\n",
            "Epoch [9186/100000], Validation Loss: 552.4126\n",
            "Epoch [9187/100000], Validation Loss: 554.3405\n",
            "Epoch [9188/100000], Validation Loss: 553.0217\n",
            "Epoch [9189/100000], Validation Loss: 547.9301\n",
            "Epoch [9190/100000], Validation Loss: 551.5859\n",
            "Epoch [9191/100000], Validation Loss: 548.0958\n",
            "Epoch [9192/100000], Validation Loss: 575.8519\n",
            "Epoch [9193/100000], Validation Loss: 560.6052\n",
            "Epoch [9194/100000], Validation Loss: 551.8115\n",
            "Epoch [9195/100000], Validation Loss: 550.7139\n",
            "Epoch [9196/100000], Validation Loss: 553.4219\n",
            "Epoch [9197/100000], Validation Loss: 559.0877\n",
            "Epoch [9198/100000], Validation Loss: 553.0805\n",
            "Epoch [9199/100000], Validation Loss: 549.8667\n",
            "Epoch [9200/100000], Validation Loss: 549.9709\n",
            "Epoch [9201/100000], Validation Loss: 553.7974\n",
            "Epoch [9202/100000], Validation Loss: 550.6342\n",
            "Epoch [9203/100000], Validation Loss: 562.5610\n",
            "Epoch [9204/100000], Validation Loss: 547.7030\n",
            "Epoch [9205/100000], Validation Loss: 551.8480\n",
            "Epoch [9206/100000], Validation Loss: 562.9193\n",
            "Epoch [9207/100000], Validation Loss: 551.3399\n",
            "Epoch [9208/100000], Validation Loss: 549.6072\n",
            "Epoch [9209/100000], Validation Loss: 550.5562\n",
            "Epoch [9210/100000], Validation Loss: 553.1810\n",
            "Epoch [9211/100000], Validation Loss: 552.1895\n",
            "Epoch [9212/100000], Validation Loss: 553.9294\n",
            "Epoch [9213/100000], Validation Loss: 549.6400\n",
            "Epoch [9214/100000], Validation Loss: 550.9627\n",
            "Epoch [9215/100000], Validation Loss: 547.4252\n",
            "Epoch [9216/100000], Validation Loss: 549.9514\n",
            "Epoch [9217/100000], Validation Loss: 544.5205\n",
            "Epoch [9218/100000], Validation Loss: 559.7199\n",
            "Epoch [9219/100000], Validation Loss: 546.3656\n",
            "Epoch [9220/100000], Validation Loss: 549.7180\n",
            "Epoch [9221/100000], Validation Loss: 550.6298\n",
            "Epoch [9222/100000], Validation Loss: 547.2330\n",
            "Epoch [9223/100000], Validation Loss: 545.4195\n",
            "Epoch [9224/100000], Validation Loss: 552.0206\n",
            "Epoch [9225/100000], Validation Loss: 556.4103\n",
            "Epoch [9226/100000], Validation Loss: 551.0470\n",
            "Epoch [9227/100000], Validation Loss: 543.7838\n",
            "Epoch [9228/100000], Validation Loss: 552.9540\n",
            "Epoch [9229/100000], Validation Loss: 555.6906\n",
            "Epoch [9230/100000], Validation Loss: 561.1121\n",
            "Epoch [9231/100000], Validation Loss: 550.5536\n",
            "Epoch [9232/100000], Validation Loss: 550.8410\n",
            "Epoch [9233/100000], Validation Loss: 561.1602\n",
            "Epoch [9234/100000], Validation Loss: 551.0844\n",
            "Epoch [9235/100000], Validation Loss: 556.6892\n",
            "Epoch [9236/100000], Validation Loss: 564.9547\n",
            "Epoch [9237/100000], Validation Loss: 554.3239\n",
            "Epoch [9238/100000], Validation Loss: 548.2106\n",
            "Epoch [9239/100000], Validation Loss: 549.3160\n",
            "Epoch [9240/100000], Validation Loss: 547.0763\n",
            "Epoch [9241/100000], Validation Loss: 551.6095\n",
            "Epoch [9242/100000], Validation Loss: 559.8508\n",
            "Epoch [9243/100000], Validation Loss: 552.0772\n",
            "Epoch [9244/100000], Validation Loss: 552.3447\n",
            "Epoch [9245/100000], Validation Loss: 547.7350\n",
            "Epoch [9246/100000], Validation Loss: 550.1511\n",
            "Epoch [9247/100000], Validation Loss: 555.3373\n",
            "Epoch [9248/100000], Validation Loss: 552.4841\n",
            "Epoch [9249/100000], Validation Loss: 552.1741\n",
            "Epoch [9250/100000], Validation Loss: 553.0536\n",
            "Epoch [9251/100000], Validation Loss: 554.5623\n",
            "Epoch [9252/100000], Validation Loss: 550.5946\n",
            "Epoch [9253/100000], Validation Loss: 546.9505\n",
            "Epoch [9254/100000], Validation Loss: 552.0981\n",
            "Epoch [9255/100000], Validation Loss: 548.5507\n",
            "Epoch [9256/100000], Validation Loss: 545.4108\n",
            "Epoch [9257/100000], Validation Loss: 553.2124\n",
            "Epoch [9258/100000], Validation Loss: 550.1271\n",
            "Epoch [9259/100000], Validation Loss: 552.0953\n",
            "Epoch [9260/100000], Validation Loss: 552.5382\n",
            "Epoch [9261/100000], Validation Loss: 549.5012\n",
            "Epoch [9262/100000], Validation Loss: 550.1258\n",
            "Epoch [9263/100000], Validation Loss: 550.5380\n",
            "Epoch [9264/100000], Validation Loss: 549.8496\n",
            "Epoch [9265/100000], Validation Loss: 550.7207\n",
            "Epoch [9266/100000], Validation Loss: 545.9283\n",
            "Epoch [9267/100000], Validation Loss: 549.0292\n",
            "Epoch [9268/100000], Validation Loss: 550.7258\n",
            "Epoch [9269/100000], Validation Loss: 554.7899\n",
            "Epoch [9270/100000], Validation Loss: 551.8460\n",
            "Epoch [9271/100000], Validation Loss: 551.6797\n",
            "Epoch [9272/100000], Validation Loss: 549.5934\n",
            "Epoch [9273/100000], Validation Loss: 559.0577\n",
            "Epoch [9274/100000], Validation Loss: 550.8067\n",
            "Epoch [9275/100000], Validation Loss: 555.8386\n",
            "Epoch [9276/100000], Validation Loss: 555.2780\n",
            "Epoch [9277/100000], Validation Loss: 553.1695\n",
            "Epoch [9278/100000], Validation Loss: 547.5230\n",
            "Epoch [9279/100000], Validation Loss: 548.6143\n",
            "Epoch [9280/100000], Validation Loss: 551.5390\n",
            "Epoch [9281/100000], Validation Loss: 546.5896\n",
            "Epoch [9282/100000], Validation Loss: 549.8879\n",
            "Epoch [9283/100000], Validation Loss: 548.7845\n",
            "Epoch [9284/100000], Validation Loss: 549.0087\n",
            "Epoch [9285/100000], Validation Loss: 551.8759\n",
            "Epoch [9286/100000], Validation Loss: 546.9723\n",
            "Epoch [9287/100000], Validation Loss: 543.5556\n",
            "Epoch [9288/100000], Validation Loss: 550.3249\n",
            "Epoch [9289/100000], Validation Loss: 563.1594\n",
            "Epoch [9290/100000], Validation Loss: 548.7346\n",
            "Epoch [9291/100000], Validation Loss: 557.4517\n",
            "Epoch [9292/100000], Validation Loss: 549.7170\n",
            "Epoch [9293/100000], Validation Loss: 556.7416\n",
            "Epoch [9294/100000], Validation Loss: 550.3520\n",
            "Epoch [9295/100000], Validation Loss: 555.3138\n",
            "Epoch [9296/100000], Validation Loss: 552.6314\n",
            "Epoch [9297/100000], Validation Loss: 551.9906\n",
            "Epoch [9298/100000], Validation Loss: 549.5812\n",
            "Epoch [9299/100000], Validation Loss: 550.0386\n",
            "Epoch [9300/100000], Validation Loss: 546.7090\n",
            "Epoch [9301/100000], Validation Loss: 548.8445\n",
            "Epoch [9302/100000], Validation Loss: 552.0836\n",
            "Epoch [9303/100000], Validation Loss: 547.9693\n",
            "Epoch [9304/100000], Validation Loss: 548.1445\n",
            "Epoch [9305/100000], Validation Loss: 550.1994\n",
            "Epoch [9306/100000], Validation Loss: 550.7191\n",
            "Epoch [9307/100000], Validation Loss: 550.3255\n",
            "Epoch [9308/100000], Validation Loss: 548.6646\n",
            "Epoch [9309/100000], Validation Loss: 555.0504\n",
            "Epoch [9310/100000], Validation Loss: 550.7165\n",
            "Epoch [9311/100000], Validation Loss: 551.5220\n",
            "Epoch [9312/100000], Validation Loss: 551.1864\n",
            "Epoch [9313/100000], Validation Loss: 554.4859\n",
            "Epoch [9314/100000], Validation Loss: 550.1114\n",
            "Epoch [9315/100000], Validation Loss: 548.5922\n",
            "Epoch [9316/100000], Validation Loss: 553.6596\n",
            "Epoch [9317/100000], Validation Loss: 549.5891\n",
            "Epoch [9318/100000], Validation Loss: 548.3569\n",
            "Epoch [9319/100000], Validation Loss: 551.5328\n",
            "Epoch [9320/100000], Validation Loss: 549.0605\n",
            "Epoch [9321/100000], Validation Loss: 547.2083\n",
            "Epoch [9322/100000], Validation Loss: 548.3075\n",
            "Epoch [9323/100000], Validation Loss: 548.4684\n",
            "Epoch [9324/100000], Validation Loss: 554.1725\n",
            "Epoch [9325/100000], Validation Loss: 550.5939\n",
            "Epoch [9326/100000], Validation Loss: 544.3689\n",
            "Epoch [9327/100000], Validation Loss: 547.7611\n",
            "Epoch [9328/100000], Validation Loss: 553.2497\n",
            "Epoch [9329/100000], Validation Loss: 549.9936\n",
            "Epoch [9330/100000], Validation Loss: 553.6260\n",
            "Epoch [9331/100000], Validation Loss: 548.6019\n",
            "Epoch [9332/100000], Validation Loss: 550.6707\n",
            "Epoch [9333/100000], Validation Loss: 549.4286\n",
            "Epoch [9334/100000], Validation Loss: 551.2117\n",
            "Epoch [9335/100000], Validation Loss: 543.9704\n",
            "Epoch [9336/100000], Validation Loss: 549.8807\n",
            "Epoch [9337/100000], Validation Loss: 552.5889\n",
            "Epoch [9338/100000], Validation Loss: 550.7098\n",
            "Epoch [9339/100000], Validation Loss: 554.9641\n",
            "Epoch [9340/100000], Validation Loss: 551.0928\n",
            "Epoch [9341/100000], Validation Loss: 551.9866\n",
            "Epoch [9342/100000], Validation Loss: 549.5869\n",
            "Epoch [9343/100000], Validation Loss: 554.7935\n",
            "Epoch [9344/100000], Validation Loss: 544.5026\n",
            "Epoch [9345/100000], Validation Loss: 550.8254\n",
            "Epoch [9346/100000], Validation Loss: 552.3556\n",
            "Epoch [9347/100000], Validation Loss: 553.0041\n",
            "Epoch [9348/100000], Validation Loss: 551.9037\n",
            "Epoch [9349/100000], Validation Loss: 547.8924\n",
            "Epoch [9350/100000], Validation Loss: 547.3908\n",
            "Epoch [9351/100000], Validation Loss: 551.0554\n",
            "Epoch [9352/100000], Validation Loss: 555.4157\n",
            "Epoch [9353/100000], Validation Loss: 550.2102\n",
            "Epoch [9354/100000], Validation Loss: 549.6142\n",
            "Epoch [9355/100000], Validation Loss: 556.5085\n",
            "Epoch [9356/100000], Validation Loss: 559.3218\n",
            "Epoch [9357/100000], Validation Loss: 551.5557\n",
            "Epoch [9358/100000], Validation Loss: 548.8420\n",
            "Epoch [9359/100000], Validation Loss: 549.6063\n",
            "Epoch [9360/100000], Validation Loss: 548.3904\n",
            "Epoch [9361/100000], Validation Loss: 555.5359\n",
            "Epoch [9362/100000], Validation Loss: 548.1996\n",
            "Epoch [9363/100000], Validation Loss: 547.3395\n",
            "Epoch [9364/100000], Validation Loss: 563.8642\n",
            "Epoch [9365/100000], Validation Loss: 554.2886\n",
            "Epoch [9366/100000], Validation Loss: 549.5419\n",
            "Epoch [9367/100000], Validation Loss: 562.7804\n",
            "Epoch [9368/100000], Validation Loss: 549.0759\n",
            "Epoch [9369/100000], Validation Loss: 548.9303\n",
            "Epoch [9370/100000], Validation Loss: 551.0228\n",
            "Epoch [9371/100000], Validation Loss: 548.9936\n",
            "Epoch [9372/100000], Validation Loss: 548.4814\n",
            "Epoch [9373/100000], Validation Loss: 552.5263\n",
            "Epoch [9374/100000], Validation Loss: 551.5268\n",
            "Epoch [9375/100000], Validation Loss: 551.4911\n",
            "Epoch [9376/100000], Validation Loss: 551.2051\n",
            "Epoch [9377/100000], Validation Loss: 552.9539\n",
            "Epoch [9378/100000], Validation Loss: 548.1479\n",
            "Epoch [9379/100000], Validation Loss: 553.6479\n",
            "Epoch [9380/100000], Validation Loss: 548.9270\n",
            "Epoch [9381/100000], Validation Loss: 551.4026\n",
            "Epoch [9382/100000], Validation Loss: 550.2221\n",
            "Epoch [9383/100000], Validation Loss: 549.7475\n",
            "Epoch [9384/100000], Validation Loss: 552.6425\n",
            "Epoch [9385/100000], Validation Loss: 551.0445\n",
            "Epoch [9386/100000], Validation Loss: 550.6481\n",
            "Epoch [9387/100000], Validation Loss: 555.0599\n",
            "Epoch [9388/100000], Validation Loss: 553.1882\n",
            "Epoch [9389/100000], Validation Loss: 549.2743\n",
            "Epoch [9390/100000], Validation Loss: 548.9942\n",
            "Epoch [9391/100000], Validation Loss: 551.2980\n",
            "Epoch [9392/100000], Validation Loss: 552.6915\n",
            "Epoch [9393/100000], Validation Loss: 552.7818\n",
            "Epoch [9394/100000], Validation Loss: 549.6285\n",
            "Epoch [9395/100000], Validation Loss: 552.0169\n",
            "Epoch [9396/100000], Validation Loss: 550.2920\n",
            "Epoch [9397/100000], Validation Loss: 551.1550\n",
            "Epoch [9398/100000], Validation Loss: 555.0113\n",
            "Epoch [9399/100000], Validation Loss: 570.1713\n",
            "Epoch [9400/100000], Validation Loss: 551.1200\n",
            "Epoch [9401/100000], Validation Loss: 557.6243\n",
            "Epoch [9402/100000], Validation Loss: 549.3953\n",
            "Epoch [9403/100000], Validation Loss: 549.4890\n",
            "Epoch [9404/100000], Validation Loss: 557.3670\n",
            "Epoch [9405/100000], Validation Loss: 550.7500\n",
            "Epoch [9406/100000], Validation Loss: 551.2930\n",
            "Epoch [9407/100000], Validation Loss: 550.2142\n",
            "Epoch [9408/100000], Validation Loss: 558.4321\n",
            "Epoch [9409/100000], Validation Loss: 547.9347\n",
            "Epoch [9410/100000], Validation Loss: 554.2393\n",
            "Epoch [9411/100000], Validation Loss: 553.2955\n",
            "Epoch [9412/100000], Validation Loss: 556.1705\n",
            "Epoch [9413/100000], Validation Loss: 548.0619\n",
            "Epoch [9414/100000], Validation Loss: 550.1484\n",
            "Epoch [9415/100000], Validation Loss: 548.4044\n",
            "Epoch [9416/100000], Validation Loss: 548.2640\n",
            "Epoch [9417/100000], Validation Loss: 549.7585\n",
            "Epoch [9418/100000], Validation Loss: 559.0582\n",
            "Epoch [9419/100000], Validation Loss: 555.7960\n",
            "Epoch [9420/100000], Validation Loss: 547.4601\n",
            "Epoch [9421/100000], Validation Loss: 553.7937\n",
            "Epoch [9422/100000], Validation Loss: 557.1972\n",
            "Epoch [9423/100000], Validation Loss: 553.7877\n",
            "Epoch [9424/100000], Validation Loss: 552.4929\n",
            "Epoch [9425/100000], Validation Loss: 548.0732\n",
            "Epoch [9426/100000], Validation Loss: 550.6210\n",
            "Epoch [9427/100000], Validation Loss: 549.7097\n",
            "Epoch [9428/100000], Validation Loss: 548.4818\n",
            "Epoch [9429/100000], Validation Loss: 548.9201\n",
            "Epoch [9430/100000], Validation Loss: 549.9072\n",
            "Epoch [9431/100000], Validation Loss: 552.8589\n",
            "Epoch [9432/100000], Validation Loss: 552.3073\n",
            "Epoch [9433/100000], Validation Loss: 558.9023\n",
            "Epoch [9434/100000], Validation Loss: 552.8085\n",
            "Epoch [9435/100000], Validation Loss: 548.2948\n",
            "Epoch [9436/100000], Validation Loss: 583.2905\n",
            "Epoch [9437/100000], Validation Loss: 555.4836\n",
            "Epoch [9438/100000], Validation Loss: 547.7465\n",
            "Epoch [9439/100000], Validation Loss: 549.9626\n",
            "Epoch [9440/100000], Validation Loss: 552.1262\n",
            "Epoch [9441/100000], Validation Loss: 550.0282\n",
            "Epoch [9442/100000], Validation Loss: 553.1093\n",
            "Epoch [9443/100000], Validation Loss: 550.9316\n",
            "Epoch [9444/100000], Validation Loss: 556.2745\n",
            "Epoch [9445/100000], Validation Loss: 553.2599\n",
            "Epoch [9446/100000], Validation Loss: 554.6028\n",
            "Epoch [9447/100000], Validation Loss: 550.7802\n",
            "Epoch [9448/100000], Validation Loss: 555.9728\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
        "\n",
        "# Define k-fold cross-validation\n",
        "k_folds = 5\n",
        "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "# Store results\n",
        "results = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(X_tensor)):\n",
        "    print(f\"Fold {fold + 1}\")\n",
        "\n",
        "    # Split data into training and validation sets\n",
        "    X_train_fold, X_val_fold = X_tensor[train_idx], X_tensor[val_idx]\n",
        "    y_train_fold, y_val_fold = y_tensor[train_idx], y_tensor[val_idx]\n",
        "\n",
        "    # Create DataLoader for batching\n",
        "    train_dataset = TensorDataset(X_train_fold, y_train_fold)\n",
        "    val_dataset = TensorDataset(X_val_fold, y_val_fold)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    # Initialize model, loss, and optimizer\n",
        "    model = SimpleNN(input_size, hidden_size, output_size)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for batch_X, batch_y in train_loader:\n",
        "            outputs = model(batch_X)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_loss = 0\n",
        "            for batch_X, batch_y in val_loader:\n",
        "                outputs = model(batch_X)\n",
        "                val_loss += criterion(outputs, batch_y).item()\n",
        "            val_loss /= len(val_loader)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Validation Loss: {val_loss:.4f}\")\n",
        "\n",
        "    results.append(val_loss)\n",
        "\n",
        "# Print average validation loss across folds\n",
        "print(f\"Average Validation Loss: {np.mean(results):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "eeLh9b2mgwKP"
      },
      "outputs": [],
      "source": [
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 5\n",
        "best_loss = float('inf')\n",
        "counter = 0\n",
        "\n",
        "# Training loop with early stopping\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        outputs = model(batch_X)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_loss = 0\n",
        "        for batch_X, batch_y in val_loader:\n",
        "            outputs = model(batch_X)\n",
        "            val_loss += criterion(outputs, batch_y).item()\n",
        "        val_loss /= len(val_loader)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Validation Loss: {val_loss:.4f}\")\n",
        "\n",
        "    # Early stopping logic\n",
        "    if val_loss < best_loss:\n",
        "        best_loss = val_loss\n",
        "        counter = 0\n",
        "    else:\n",
        "        counter += 1\n",
        "        if counter >= patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_VOIDIwVckv"
      },
      "source": [
        "Concordance(Rank) Loss Custom template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NNwV7xWUVbm-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ConcordanceLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConcordanceLoss, self).__init__()\n",
        "\n",
        "    def forward(self, pred_risk, time, event):\n",
        "        \"\"\"\n",
        "        pred_risk: Predicted risk scores (higher scores mean higher risk of event).\n",
        "        time: True event/censoring times.\n",
        "        event: Event indicator (1 = event occurred, 0 = censored).\n",
        "        \"\"\"\n",
        "        # Create pairs of patients\n",
        "        n = len(time)\n",
        "        concordance = 0\n",
        "        permissible = 0\n",
        "\n",
        "        for i in range(n):\n",
        "            if event[i] == 1:  # Only consider pairs where the first patient had an event\n",
        "                for j in range(n):\n",
        "                    if time[j] > time[i]:  # Second patient must have a longer survival time\n",
        "                        permissible += 1\n",
        "                        if pred_risk[i] > pred_risk[j]:  # Correct ranking\n",
        "                            concordance += 1\n",
        "                        elif pred_risk[i] == pred_risk[j]:  # Tie\n",
        "                            concordance += 0.5\n",
        "\n",
        "        if permissible == 0:\n",
        "            return torch.tensor(0.0, requires_grad=True)  # No permissible pairs\n",
        "\n",
        "        c_index = concordance / permissible\n",
        "        return 1 - c_index  # Minimize 1 - C-Index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_VyBtTvy9uF"
      },
      "source": [
        "### Plts for summary of specific features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "M2ak83A_gzcz"
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "\n",
        "# Explain the model's predictions using SHAP\n",
        "explainer = shap.DeepExplainer(model, X_train_tensor[:100])  # Use a subset of data\n",
        "shap_values = explainer.shap_values(X_train_tensor[:100])\n",
        "\n",
        "# Plot feature importance\n",
        "shap.summary_plot(shap_values, X_train[:100], feature_names=X_train.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qq_ik0kkg1Tf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}